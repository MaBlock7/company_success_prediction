{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-05 10:22:24.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/manuelbolz/Documents/git/for_work/company_success_prediction\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pocketknife.database import connect_database, read_from_database\n",
    "from config import EXTERNAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db queries\n",
    "query_shab = \"\"\"\n",
    "    SELECT\n",
    "        ehraid,\n",
    "        shab_id,\n",
    "        shab_date,\n",
    "        registry_office_canton,\n",
    "        message AS message_raw\n",
    "    FROM zefix.shab\n",
    "\"\"\"\n",
    "\n",
    "query_shab_mutation = \"\"\"\n",
    "    SELECT * \n",
    "    FROM zefix.shab_mutation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    raw_shab = read_from_database(con, query_shab)\n",
    "    raw_shab_mutation = read_from_database(con, query_shab_mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_shab_messages = pd.read_csv(EXTERNAL_DATA_DIR / 'rule_based_parsing_fr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.fillna('{}')\n",
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.apply(ast.literal_eval)\n",
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.fillna({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_shab_messages['text_slice'] = parsed_shab_messages['text_slice'].fillna('[]')\n",
    "parsed_shab_messages['text_slices'] = parsed_shab_messages['text_slice'].apply(ast.literal_eval)\n",
    "parsed_shab_messages = parsed_shab_messages.drop(columns=['text_slice'])\n",
    "parsed_shab_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636019\n",
      "2464457\n"
     ]
    }
   ],
   "source": [
    "print(parsed_shab_messages['shab_id'].nunique())\n",
    "print(raw_shab['shab_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1625920\n",
    "2457792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the companies where the shab entries have been parsed\n",
    "raw_shab = raw_shab[raw_shab.shab_id.isin(parsed_shab_messages['shab_id'].unique())].copy()\n",
    "raw_shab_mutation = raw_shab_mutation[raw_shab_mutation.shab_id.isin(parsed_shab_messages['shab_id'].unique())].copy()\n",
    "raw_shab_mutation_grouped = (\n",
    "    raw_shab_mutation\n",
    "    .groupby('shab_id')\n",
    "    .agg(codes=pd.NamedAgg(column='description', aggfunc=lambda x: [v for v in x]))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_shab_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the dataframes\n",
    "shab_merged = (\n",
    "    raw_shab\n",
    "    .merge(raw_shab_mutation_grouped, on='shab_id', how='left')\n",
    "    .merge(parsed_shab_messages, on='shab_id', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values in the correct temporal order\n",
    "shab_merged = shab_merged.sort_values(['ehraid', 'shab_date', 'shab_id'], ascending=[True, True, True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shab_merged_temp = shab_merged[shab_merged.ehraid.isin([905876, 905843, 905844])]\n",
    "shab_merged_temp = shab_merged_temp.sort_values(['ehraid', 'shab_date', 'shab_id'], ascending=[True, True, True]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_json_history(df: pd.DataFrame) -> dict:\n",
    "    json_structure = defaultdict(lambda: {'history': []})\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the fields to identify a company and its entries\n",
    "        ehraid = row['ehraid']\n",
    "        shab_date = row['shab_date'] if isinstance(row['shab_date'], str) else row['shab_date'].strftime('%Y-%d-%m')\n",
    "        shab_id = row['shab_id']\n",
    "        main_group = row['main_group']\n",
    "        keyword = row['keyword']\n",
    "\n",
    "        # Extract main information\n",
    "        message_info = {\n",
    "            'registry_office_canton': row['registry_office_canton'],\n",
    "            'codes': row['codes'],\n",
    "            'message_raw': row['message_raw'],\n",
    "            'extracted_content': {\n",
    "                main_group: {\n",
    "                    keyword: {\n",
    "                        'text_slices': row['text_slices'],\n",
    "                        'variables': row['parsed_variables']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Search if shab_date already exists in the history\n",
    "        date_entry = next((entry for entry in json_structure[ehraid]['history'] if shab_date in entry), None)\n",
    "\n",
    "        if date_entry is None:\n",
    "            # If the date does not exist, create a new entry\n",
    "            date_entry = {shab_date: {shab_id: message_info}}\n",
    "            json_structure[ehraid]['history'].append(date_entry)\n",
    "        else:\n",
    "            # If the date exists, check if the shab_id already exists\n",
    "            id_entry = date_entry[shab_date].get(shab_id, None)\n",
    "            if id_entry is None:\n",
    "                # If the shab_id does not exists, we can simply add it to the shab_date\n",
    "                date_entry[shab_date][shab_id] = message_info\n",
    "            else:\n",
    "                # If the shab_id exists, we need to check if the main_group already exists\n",
    "                main_group_entry = date_entry[shab_date][shab_id]['extracted_content'].get(main_group, None)\n",
    "                if main_group_entry is None:\n",
    "                    # If it does not exist, we add it to the extracted content\n",
    "                    date_entry[shab_date][shab_id]['extracted_content'][main_group] = message_info['extracted_content'][main_group]\n",
    "                else:\n",
    "                    # If it does, we add the keyword to the main_group, since a keyword can only appear once within the main_group\n",
    "                    date_entry[shab_date][shab_id]['extracted_content'][main_group][keyword] = message_info['extracted_content'][main_group][keyword]      \n",
    "\n",
    "    return dict(json_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_json = create_raw_json_history(shab_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE REGISTERED PEOPLE AND FIRMS TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people = shab_merged[shab_merged.main_group == 'natural persons and legal entities'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure only the expected fields are there\n",
    "def validate_person_and_firms(x: dict):    \n",
    "    schema = {'firms': [], 'people': []}\n",
    "    for firm in x.get('firms', []):\n",
    "        if isinstance(firm, dict):\n",
    "            schema['firms'].append({\n",
    "                'firm_name': firm.get('firm_name'),\n",
    "                'firm_uid': firm.get('id'),\n",
    "                'firm_seat': firm.get('location'),\n",
    "                'firm_type': firm.get('type'),\n",
    "                'firm_shares': firm.get('shares')\n",
    "            })\n",
    "    for person in x.get('people', []):\n",
    "        if isinstance(person, dict):\n",
    "            schema['people'].append({\n",
    "                'first_name': person.get('first_name'),\n",
    "                'last_name': person.get('last_name'),\n",
    "                'hometown': person.get('hometown'),\n",
    "                'place_of_residence': person.get('place_of_residence'),\n",
    "                'nationality': person.get('nationality'),\n",
    "                'job_title': person.get('job_title'),\n",
    "                'authorization': person.get('authorization'),\n",
    "                'shares': person.get('shares')\n",
    "            })\n",
    "    return schema\n",
    "\n",
    "df_people['validated_variables'] = df_people['parsed_variables'].apply(validate_person_and_firms)  \n",
    "df_people['firms'] = df_people['validated_variables'].apply(lambda x: x.get('firms', []))\n",
    "df_people['people'] = df_people['validated_variables'].apply(lambda x: x.get('people', []))\n",
    "\n",
    "# Split individual firm dictionaries into individual rows\n",
    "df_firms_exploded = df_people.explode(column=['firms']).dropna()\n",
    "df_people_exploded = df_people.explode(column=['people']).dropna()\n",
    "\n",
    "# Create individual columns from the dictionary\n",
    "df_firms_norm = pd.json_normalize(\n",
    "    df_firms_exploded['firms'],\n",
    "    errors='raise'\n",
    ")\n",
    "df_firms_concat = pd.concat([df_firms_exploded[['ehraid', 'message_raw', 'shab_date', 'shab_id', 'codes', 'keyword']].reset_index(drop=True), df_firms_norm], axis=1)\n",
    "\n",
    "df_people_norm = pd.json_normalize(\n",
    "    df_people_exploded['people'],\n",
    "    errors='raise'\n",
    ")\n",
    "df_people_concat = pd.concat([df_people_exploded[['ehraid', 'message_raw', 'shab_date', 'shab_id', 'codes', 'keyword']].reset_index(drop=True), df_people_norm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Gender and Nationality to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from unidecode import unidecode\n",
    "from zefix_processing.country_mapping import german2alpha2, french2alpha2, italian2alpha2\n",
    "from zefix_processing.gender_mapping import german2gender, french2gender, italian2gender\n",
    "\n",
    "nlp_models = {\n",
    "    \"de\": spacy.load(\"de_core_news_sm\"),\n",
    "    \"fr\": spacy.load(\"fr_core_news_sm\"),\n",
    "    \"it\": spacy.load(\"it_core_news_sm\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_words(string: str) -> str:\n",
    "    replacements = {\n",
    "        'ä': 'ae',\n",
    "        'ö': 'oe',\n",
    "        'ü': 'ue'\n",
    "    }\n",
    "    for char, replacement in replacements.items():\n",
    "        string = string.replace(char, replacement)\n",
    "    \n",
    "    return unidecode(string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_mapping = {\n",
    "    'de': {normalize_words(k): v for k, v in german2alpha2.items()},\n",
    "    'fr': {normalize_words(k): v for k, v in french2alpha2.items()},\n",
    "    'it': {normalize_words(k): v for k, v in italian2alpha2.items()}\n",
    "}\n",
    "\n",
    "gender_mapping = {\n",
    "    'de': {normalize_words(k): v for k, v in german2gender.items()},\n",
    "    'fr': {normalize_words(k): v for k, v in french2gender.items()},\n",
    "    'it': {normalize_words(k): v for k, v in italian2gender.items()},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the hometown, place of residence, and nationality column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_containing_and = [\n",
    "    'bosnie et herzégovine',\n",
    "    'svalbard et île jan mayen',\n",
    "    'îles turques et caïques',\n",
    "    'géorgie du sud et îles sandwich du sud',\n",
    "    'bonaire, saint eustatius et saba',\n",
    "    'terres australes et antarctiques françaises',\n",
    "    'de são tomé e príncipe',\n",
    "    'serbo e montenegrino',\n",
    "    'serba e montenegrina',\n",
    "    'di saint christopher e nevis',\n",
    "    'de são tomé e príncipe'\n",
    "]\n",
    "\n",
    "# 1. Split multiple nationalities into individual columns\n",
    "def split_locations(df: pd.DataFrame, orig_col: str = 'nationality'):\n",
    "    loc_split = df[orig_col].str.split(r'\\sund\\s|\\set\\s|\\se\\s', regex=True, expand=True)\n",
    "    loc_split.columns = [f'{orig_col}_{i+1}' for i in range(loc_split.shape[1])]\n",
    "    loc_split.fillna('', inplace=True)\n",
    "    df = pd.concat([df, loc_split], axis=1)\n",
    "    return df.drop(columns=[orig_col])\n",
    "\n",
    "# First, split the hometown and place of residence column\n",
    "df_people_concat = split_locations(df_people_concat, 'hometown')\n",
    "df_people_concat = split_locations(df_people_concat, 'place_of_residence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Move nationalities that are in the wrong column\n",
    "def contains_target_word(text):\n",
    "    pattern = r'\\bstaatsangehoerige\\b|\\bcittadina\\b|\\bressortissante\\b|\\bcitoyenne\\b|\\bstaatsangehoeriger\\b|\\bcittadino\\b|\\bressortissant\\b|\\bcitoyen\\b'\n",
    "    return bool(re.search(pattern, text))\n",
    "\n",
    "def move_nationalities(language: str, entries: list[str], nat: str, country_names: set[str]):\n",
    "    \"\"\"\n",
    "    Checks if any of the hometown columns contains a country name\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'de': 'und',\n",
    "        'fr': 'et',\n",
    "        'it': 'e'\n",
    "    }\n",
    "    for i, entry in enumerate(entries):\n",
    "        if entry:\n",
    "            entry_norm = normalize_words(entry)\n",
    "            if entry_norm in country_names or contains_target_word(entry_norm):\n",
    "                nat = f\"{nat} {mapping.get(language, ' und ')} {entry}\" if nat else entry\n",
    "                entries[i] = ''\n",
    "            else:\n",
    "                # If the name is not a country name and the nationality does not include Swiss yet,\n",
    "                # we want to add 'CH' to the nationalities, since the hometown is with high probability a Swiss municipality\n",
    "                if 'CH' not in nat:\n",
    "                    nat = f\"{nat} {mapping.get(language, ' und ')} CH\" if nat else 'CH'\n",
    "    return entries + [nat]\n",
    "\n",
    "countries_norm = french2alpha2.keys()\n",
    "hometown_cols = [col for col in df_people_concat.columns if 'hometown' in col]\n",
    "result_cols = hometown_cols + ['nationality']\n",
    "df_people_concat[result_cols] = df_people_concat.apply(lambda x: pd.Series(move_nationalities('fr', [x[col] for col in hometown_cols], x['nationality'], countries_norm)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, split the nationality column\n",
    "df_people_concat = split_locations(df_people_concat, 'nationality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the authorization and shares column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_auth_and_shares(language: str, auth: str, shares: str):\n",
    "    \"\"\"\n",
    "    Checks if any of the hometown columns contains a country name\n",
    "    \"\"\"\n",
    "    keyword_mapping = {\n",
    "        'de': 'unterschrift',\n",
    "        'fr': 'signature',\n",
    "        'it': 'firma'\n",
    "    }\n",
    "    and_mapping = {\n",
    "        'de': 'und',\n",
    "        'fr': 'et',\n",
    "        'it': 'e'\n",
    "    }\n",
    "\n",
    "    # Base Case: no value in both\n",
    "    if not (auth or shares):\n",
    "        return [auth, shares]  # no switch\n",
    "    \n",
    "    match_auth = re.search(r'\\bchf\\b', auth.lower()) if auth else None\n",
    "    match_shares = re.search(keyword_mapping[language], shares.lower()) if shares else None\n",
    "\n",
    "    # Case 0: no match in both\n",
    "    if not (match_auth or match_shares):\n",
    "        return [auth, shares]  # no switch\n",
    "\n",
    "    # Case 1: match in auth and no match in shares\n",
    "    elif match_auth and not match_shares:\n",
    "        if shares:\n",
    "            return ['', f\"{auth} {and_mapping[language]} {shares}\"]  # Add auth infront of shares\n",
    "        else:\n",
    "            return ['', auth]  # switch columns: auth, share\n",
    "    \n",
    "    # Case 2: match in shares and no value in auth\n",
    "    elif match_shares and not match_auth:\n",
    "        if shares:\n",
    "            return [f\"{auth} {and_mapping[language]} {shares}\", '']  # Add shares after auth\n",
    "        else:\n",
    "            return [shares, '']  # switch columns: auth, share\n",
    "    \n",
    "    # Case 3: match in auth and shares\n",
    "    elif match_auth and not match_shares:\n",
    "        return [shares, auth]  # switch both\n",
    "    \n",
    "    else:\n",
    "        return [auth, shares]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_concat[['authorization', 'shares']] = df_people_concat.apply(lambda x: pd.Series(switch_auth_and_shares('fr', x['authorization'], x['shares'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the nationality and add the iso-3166-1 alpha 2 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_country(nationality: str, language: str, mapping: dict) -> str:\n",
    "    if nationality:\n",
    "        if language == 'de':\n",
    "            nationality = nationality.split()[0].strip()\n",
    "        else:\n",
    "            nationality = nationality.split()[-1].strip()\n",
    "        return mapping[language].get(nationality)\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_cols = [col for col in df_people_concat.columns if re.match(r'^nationality\\_\\d{1}$', col)]\n",
    "for nat_col in nat_cols:\n",
    "    df_people_concat[f\"{nat_col}_norm\"] = df_people_concat[nat_col].fillna('').apply(normalize_words)\n",
    "    df_people_concat[f'{nat_col}_iso_3166_1_alpha_2'] = df_people_concat[f\"{nat_col}_norm\"].apply(lambda x: map_country(x, 'fr', nationality_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>message_raw</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>codes</th>\n",
       "      <th>keyword</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>job_title</th>\n",
       "      <th>authorization</th>\n",
       "      <th>...</th>\n",
       "      <th>hometown_2</th>\n",
       "      <th>hometown_3</th>\n",
       "      <th>place_of_residence_1</th>\n",
       "      <th>place_of_residence_2</th>\n",
       "      <th>nationality_1</th>\n",
       "      <th>nationality_2</th>\n",
       "      <th>nationality_1_norm</th>\n",
       "      <th>nationality_2_norm</th>\n",
       "      <th>nationality_1_iso_3166_1_alpha_2</th>\n",
       "      <th>nationality_2_iso_3166_1_alpha_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>252</td>\n",
       "      <td>BG SA, à Bagnes, CHE-103.527.669, société anon...</td>\n",
       "      <td>2017-05-17</td>\n",
       "      <td>3527803</td>\n",
       "      <td>[aenderungorgane]</td>\n",
       "      <td>personnes et signatures radiées</td>\n",
       "      <td>Jacques</td>\n",
       "      <td>Bruchez</td>\n",
       "      <td>administrateur et secrétaire</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Martigny</td>\n",
       "      <td>None</td>\n",
       "      <td>CH</td>\n",
       "      <td>None</td>\n",
       "      <td>ch</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>252</td>\n",
       "      <td>BG SA, à Bagnes, CHE-103.527.669, société anon...</td>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>3541469</td>\n",
       "      <td>[aenderungorgane]</td>\n",
       "      <td>inscription ou modification de personnes</td>\n",
       "      <td>Eric</td>\n",
       "      <td>Fumeaux</td>\n",
       "      <td>administrateur unique</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Le Châble VS ( Bagnes )</td>\n",
       "      <td>None</td>\n",
       "      <td>CH</td>\n",
       "      <td>None</td>\n",
       "      <td>ch</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>BG SA, à Bagnes, CHE-103.527.669, société anon...</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>1005060112</td>\n",
       "      <td>[aenderungorgane]</td>\n",
       "      <td>personnes et signatures radiées</td>\n",
       "      <td>Eric</td>\n",
       "      <td>Fumeaux</td>\n",
       "      <td>administrateur unique</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Le Châble VS ( Bagnes )</td>\n",
       "      <td>None</td>\n",
       "      <td>CH</td>\n",
       "      <td>None</td>\n",
       "      <td>ch</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>252</td>\n",
       "      <td>BG SA, à Bagnes, CHE-103.527.669, société anon...</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>1005060112</td>\n",
       "      <td>[aenderungorgane]</td>\n",
       "      <td>inscription ou modification de personnes</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>De Vito</td>\n",
       "      <td>directrice</td>\n",
       "      <td>avec signature collective à deux</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Lausanne</td>\n",
       "      <td>None</td>\n",
       "      <td>CH</td>\n",
       "      <td>None</td>\n",
       "      <td>ch</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>252</td>\n",
       "      <td>BG SA, à Bagnes, CHE-103.527.669, société anon...</td>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>1005060112</td>\n",
       "      <td>[aenderungorgane]</td>\n",
       "      <td>inscription ou modification de personnes</td>\n",
       "      <td>Véronique</td>\n",
       "      <td>Billod</td>\n",
       "      <td>directrice adjointe</td>\n",
       "      <td>avec signature collective à deux</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sâles</td>\n",
       "      <td>None</td>\n",
       "      <td>CH</td>\n",
       "      <td>None</td>\n",
       "      <td>ch</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120655</th>\n",
       "      <td>1681999</td>\n",
       "      <td>SalvaTerre Ryan Van Boven, à Cugy (VD), Chemin...</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006273021</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>personnes inscrites special</td>\n",
       "      <td>Boven Ryan</td>\n",
       "      <td>Van</td>\n",
       "      <td>titulaire</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Montanaire</td>\n",
       "      <td>None</td>\n",
       "      <td>Canada</td>\n",
       "      <td>None</td>\n",
       "      <td>canada</td>\n",
       "      <td></td>\n",
       "      <td>CA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120656</th>\n",
       "      <td>1682003</td>\n",
       "      <td>Tserman Vladyslav, Ingénieur logiciel (Tserman...</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006273026</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>personnes inscrites special</td>\n",
       "      <td>Vladyslav</td>\n",
       "      <td>Tserman</td>\n",
       "      <td>titulaire</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Vevey</td>\n",
       "      <td>None</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>None</td>\n",
       "      <td>ukraine</td>\n",
       "      <td></td>\n",
       "      <td>UA</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120657</th>\n",
       "      <td>1682005</td>\n",
       "      <td>Yaffa Nettoyages, Flamur Pllana, à Ecublens (V...</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006273029</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>personnes inscrites special</td>\n",
       "      <td>Flamur</td>\n",
       "      <td>Pllana</td>\n",
       "      <td>titulaire</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ecublens ( VD )</td>\n",
       "      <td>None</td>\n",
       "      <td>CH</td>\n",
       "      <td>None</td>\n",
       "      <td>ch</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120658</th>\n",
       "      <td>1682009</td>\n",
       "      <td>Clément Peyré, Cicatriclem, à Valbirse, CHE-14...</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006272333</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>personnes inscrites</td>\n",
       "      <td>Clément</td>\n",
       "      <td>Peyré</td>\n",
       "      <td>titulaire</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Malleray ( Valbirse )</td>\n",
       "      <td>None</td>\n",
       "      <td>ressortissant français</td>\n",
       "      <td>None</td>\n",
       "      <td>ressortissant francais</td>\n",
       "      <td></td>\n",
       "      <td>FR</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120659</th>\n",
       "      <td>1682024</td>\n",
       "      <td>T consult Tino Cocco, à Saint-Imier, CHE-232.8...</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006272348</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>personnes inscrites</td>\n",
       "      <td>Tino</td>\n",
       "      <td>Cocco</td>\n",
       "      <td>titulaire</td>\n",
       "      <td>avec signature individuelle</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>St-Imier ( Saint-Imier )</td>\n",
       "      <td>None</td>\n",
       "      <td>CH</td>\n",
       "      <td>None</td>\n",
       "      <td>ch</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120660 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ehraid                                        message_raw  \\\n",
       "0           252  BG SA, à Bagnes, CHE-103.527.669, société anon...   \n",
       "1           252  BG SA, à Bagnes, CHE-103.527.669, société anon...   \n",
       "2           252  BG SA, à Bagnes, CHE-103.527.669, société anon...   \n",
       "3           252  BG SA, à Bagnes, CHE-103.527.669, société anon...   \n",
       "4           252  BG SA, à Bagnes, CHE-103.527.669, société anon...   \n",
       "...         ...                                                ...   \n",
       "120655  1681999  SalvaTerre Ryan Van Boven, à Cugy (VD), Chemin...   \n",
       "120656  1682003  Tserman Vladyslav, Ingénieur logiciel (Tserman...   \n",
       "120657  1682005  Yaffa Nettoyages, Flamur Pllana, à Ecublens (V...   \n",
       "120658  1682009  Clément Peyré, Cicatriclem, à Valbirse, CHE-14...   \n",
       "120659  1682024  T consult Tino Cocco, à Saint-Imier, CHE-232.8...   \n",
       "\n",
       "         shab_date     shab_id                 codes  \\\n",
       "0       2017-05-17     3527803     [aenderungorgane]   \n",
       "1       2017-05-24     3541469     [aenderungorgane]   \n",
       "2       2020-12-28  1005060112     [aenderungorgane]   \n",
       "3       2020-12-28  1005060112     [aenderungorgane]   \n",
       "4       2020-12-28  1005060112     [aenderungorgane]   \n",
       "...            ...         ...                   ...   \n",
       "120655  2025-03-04  1006273021  [status, status.neu]   \n",
       "120656  2025-03-04  1006273026  [status, status.neu]   \n",
       "120657  2025-03-04  1006273029  [status, status.neu]   \n",
       "120658  2025-03-04  1006272333  [status, status.neu]   \n",
       "120659  2025-03-04  1006272348  [status, status.neu]   \n",
       "\n",
       "                                         keyword  first_name last_name  \\\n",
       "0                personnes et signatures radiées     Jacques   Bruchez   \n",
       "1       inscription ou modification de personnes        Eric   Fumeaux   \n",
       "2                personnes et signatures radiées        Eric   Fumeaux   \n",
       "3       inscription ou modification de personnes      Sandra   De Vito   \n",
       "4       inscription ou modification de personnes   Véronique    Billod   \n",
       "...                                          ...         ...       ...   \n",
       "120655               personnes inscrites special  Boven Ryan       Van   \n",
       "120656               personnes inscrites special   Vladyslav   Tserman   \n",
       "120657               personnes inscrites special      Flamur    Pllana   \n",
       "120658                       personnes inscrites     Clément     Peyré   \n",
       "120659                       personnes inscrites        Tino     Cocco   \n",
       "\n",
       "                           job_title                     authorization  ...  \\\n",
       "0       administrateur et secrétaire       avec signature individuelle  ...   \n",
       "1              administrateur unique       avec signature individuelle  ...   \n",
       "2              administrateur unique       avec signature individuelle  ...   \n",
       "3                         directrice  avec signature collective à deux  ...   \n",
       "4                directrice adjointe  avec signature collective à deux  ...   \n",
       "...                              ...                               ...  ...   \n",
       "120655                     titulaire       avec signature individuelle  ...   \n",
       "120656                     titulaire       avec signature individuelle  ...   \n",
       "120657                     titulaire       avec signature individuelle  ...   \n",
       "120658                     titulaire       avec signature individuelle  ...   \n",
       "120659                     titulaire       avec signature individuelle  ...   \n",
       "\n",
       "       hometown_2 hometown_3      place_of_residence_1 place_of_residence_2  \\\n",
       "0            None       None                  Martigny                 None   \n",
       "1            None       None   Le Châble VS ( Bagnes )                 None   \n",
       "2            None       None   Le Châble VS ( Bagnes )                 None   \n",
       "3            None       None                  Lausanne                 None   \n",
       "4            None       None                     Sâles                 None   \n",
       "...           ...        ...                       ...                  ...   \n",
       "120655       None       None                Montanaire                 None   \n",
       "120656       None       None                     Vevey                 None   \n",
       "120657       None       None           Ecublens ( VD )                 None   \n",
       "120658       None       None     Malleray ( Valbirse )                 None   \n",
       "120659       None       None  St-Imier ( Saint-Imier )                 None   \n",
       "\n",
       "                 nationality_1 nationality_2      nationality_1_norm  \\\n",
       "0                           CH          None                      ch   \n",
       "1                           CH          None                      ch   \n",
       "2                           CH          None                      ch   \n",
       "3                           CH          None                      ch   \n",
       "4                           CH          None                      ch   \n",
       "...                        ...           ...                     ...   \n",
       "120655                  Canada          None                  canada   \n",
       "120656                 Ukraine          None                 ukraine   \n",
       "120657                      CH          None                      ch   \n",
       "120658  ressortissant français          None  ressortissant francais   \n",
       "120659                      CH          None                      ch   \n",
       "\n",
       "       nationality_2_norm nationality_1_iso_3166_1_alpha_2  \\\n",
       "0                                                     None   \n",
       "1                                                     None   \n",
       "2                                                     None   \n",
       "3                                                     None   \n",
       "4                                                     None   \n",
       "...                   ...                              ...   \n",
       "120655                                                  CA   \n",
       "120656                                                  UA   \n",
       "120657                                                None   \n",
       "120658                                                  FR   \n",
       "120659                                                None   \n",
       "\n",
       "       nationality_2_iso_3166_1_alpha_2  \n",
       "0                                        \n",
       "1                                        \n",
       "2                                        \n",
       "3                                        \n",
       "4                                        \n",
       "...                                 ...  \n",
       "120655                                   \n",
       "120656                                   \n",
       "120657                                   \n",
       "120658                                   \n",
       "120659                                   \n",
       "\n",
       "[120660 rows x 22 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find gendered job titles and/or determine gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_endings = {\n",
    "    'de': {\n",
    "        'female': ['in'],\n",
    "        'male': [],  # no specific ending for male words in German\n",
    "    },\n",
    "    'fr': {\n",
    "        'female': ['euse', 'ienne', 'onne', 'ane', 'trice', 'esse'],\n",
    "        'male': ['eur', 'ien', 'on', 'an'],\n",
    "    },\n",
    "    'it': {\n",
    "        'female': ['a', 'trice', 'essa'],\n",
    "        'male': ['o', 'ore']\n",
    "    }   \n",
    "}\n",
    "\n",
    "\n",
    "def extract_nouns(text: str, language: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts nouns from a given text using spaCy for German, French, and Italian.\n",
    "    \"\"\"\n",
    "    nlp = nlp_models[language]\n",
    "    return [token.text for token in nlp(text) if token.pos_ == 'NOUN']\n",
    "\n",
    "\n",
    "def create_gendered_job_names(language: str, df: pd.DataFrame, col: str = 'job_title_norm') -> tuple[list]:\n",
    "    nouns = set([title.lower() for title in df[col].unique() for title in extract_nouns(title, language)])\n",
    "       \n",
    "    female_words = []\n",
    "    male_words = []\n",
    "    undetermined = []\n",
    "\n",
    "    for word in nouns:\n",
    "        if any(word.endswith(ending) for ending in gendered_endings[language]['female']):\n",
    "            female_words.append(word)\n",
    "        elif any(word.endswith(ending) for ending in gendered_endings[language]['male']):\n",
    "            male_words.append(word)\n",
    "        else:\n",
    "            undetermined.append(word)\n",
    "\n",
    "    if language == 'de':\n",
    "        for word in female_words:\n",
    "            male_version = word.removesuffix('in')\n",
    "            if male_version in undetermined:\n",
    "                male_words.append(male_version)\n",
    "        \n",
    "    undetermined = [w for w in undetermined if w not in male_words]\n",
    "\n",
    "    return female_words, male_words, undetermined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize job title\n",
    "df_people_concat['job_title_norm'] = df_people_concat['job_title'].apply(normalize_words)\n",
    "df_people_concat['job_title_norm'] = df_people_concat['job_title_norm'].str.replace(r'[^a-zA-Z]', ' ', regex=True).apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_GENDERED_WORDS = False\n",
    "\n",
    "if CREATE_GENDERED_WORDS:\n",
    "    female_words, male_words, undetermined = create_gendered_job_names('fr', df_people_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_gender(mapping: dict, nationalities: list[str], job_title: str, name: str, country_code):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Try to infer gender via nationality\n",
    "    for nationality in nationalities:\n",
    "        if re.search(r'\\bstaatsangehoeriger\\b|\\bcittadino\\b|\\bressortissant\\b|\\bcitoyen\\b', nationality):\n",
    "            return 'm'\n",
    "        elif re.search(r'\\bstaatsangehoerige\\b|\\bcittadina\\b|\\bressortissante\\b|\\bcitoyenne\\b', nationality):\n",
    "            return 'f'\n",
    "\n",
    "    # Try to infer gender via job title\n",
    "    genders = [mapping[w] for w in mapping.keys() if re.match(rf'\\b{w}\\b', job_title)]\n",
    "    if genders:\n",
    "        # Check if list only contains one gender\n",
    "        if genders.count(genders[0]) == len(genders):\n",
    "            return genders[0]\n",
    "\n",
    "    # Try to infer gender via first name and country_code\n",
    "    pass\n",
    "\n",
    "    return 'u'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_norm_cols = [col for col in df_people_concat.columns if re.match(r'\\bnationality_\\d{1}_norm\\b', col)]\n",
    "df_people_concat['gender'] = df_people_concat.apply(lambda x: determine_gender(gender_mapping['fr'], [x[col] for col in nat_norm_cols], x['job_title_norm'], x['first_name'], x['nationality_1_iso_3166_1_alpha_2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS CAPITAL CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_changes = parsed_shab_messages[parsed_shab_messages.main_group == 'kapital- und rechtsformänderungen'].copy()\n",
    "mergers_and_acquisitions = parsed_shab_messages[parsed_shab_messages.main_group == 'fusionen und ab- und aufspaltungen'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct wrong values like '446.001.000.00'\n",
    "def correct_number(number_str: str) -> str:\n",
    "    parts = number_str.rsplit('.', 1)\n",
    "    return parts[0].replace('.', '') + '.' + parts[1] if len(parts) > 1 else parts[0].replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shab_ids = [[], []]\n",
    "keywords = [[], []]\n",
    "main_groups = [[], []]\n",
    "\n",
    "capital_new = []\n",
    "capital_until_now = []\n",
    "num_shares_new = []\n",
    "val_shares_new = []\n",
    "typ_shares_new = []\n",
    "\n",
    "for i, row in capital_changes.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    keyword = row['keyword']\n",
    "    main_group = row['main_group']\n",
    "\n",
    "    parsed_variables = row['parsed_variables']\n",
    "    cap_new = parsed_variables.get('capital_new', [])\n",
    "    cap_unt = parsed_variables.get('capital_until_now', [])\n",
    "    srs_new = parsed_variables.get('shares_new', [])\n",
    "\n",
    "    if cap_new or cap_unt:\n",
    "        capital_new.append(cap_new[0] if len(cap_new) > 0 else None)\n",
    "        capital_until_now.append(cap_unt[0] if len(cap_unt) > 0 else None)\n",
    "        shab_ids[0].append(shab_id)\n",
    "        keywords[0].append(keyword)\n",
    "        main_groups[0].append(main_group)\n",
    "    if srs_new:\n",
    "        for s in srs_new:\n",
    "            num_shares_new.append(s.get('number'))\n",
    "            val_shares_new.append(s.get('value'))\n",
    "            typ_shares_new.append(s.get('type'))\n",
    "            shab_ids[1].append(shab_id)\n",
    "            keywords[1].append(keyword)\n",
    "            main_groups[1].append(main_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_new = pd.DataFrame({\n",
    "    'shab_id': shab_ids[0],\n",
    "    'keyword': keywords[0],\n",
    "    'main_group': main_groups[0],\n",
    "    'capital_new': capital_new,\n",
    "    'capital_until_now': capital_until_now})\n",
    "\n",
    "cap_new['capital_new'] = cap_new['capital_new'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Extract Währung\n",
    "cap_new['currency_new'] = cap_new['capital_new'].str.extract(r'^([^\\d\\s]+)')\n",
    "cap_new['currency_new'] = cap_new['currency_new'].fillna('')\n",
    "\n",
    "cap_new['currency_until_now'] = cap_new['capital_until_now'].str.extract(r'^([^\\d\\s]+)')\n",
    "cap_new['currency_until_now'] = cap_new['currency_until_now'].fillna('')\n",
    "\n",
    "# Extract Kapital\n",
    "cap_new['capital_new'] = cap_new['capital_new'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "# Apply correction to remove unneccessary punctuations\n",
    "cap_new['capital_new'] = cap_new['capital_new'].apply(correct_number)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].apply(correct_number)\n",
    "\n",
    "cap_new['capital_new'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in cap_new['capital_new']]\n",
    "cap_new.loc[cap_new['capital_new'] == '.', 'capital_new'] = np.nan\n",
    "\n",
    "cap_new['capital_until_now'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in cap_new['capital_until_now']]\n",
    "cap_new.loc[cap_new['capital_until_now'] == '.', 'capital_until_now'] = np.nan\n",
    "\n",
    "# Ensure correct types\n",
    "cap_new['capital_new'] = cap_new['capital_new'].astype(float)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_new = pd.DataFrame({\n",
    "    'shab_id': shab_ids[1],\n",
    "    'keyword': keywords[1],\n",
    "    'hauptkategorie': main_groups[1],\n",
    "    'num_shares_new': num_shares_new,\n",
    "    'val_shares_new': val_shares_new,\n",
    "    'typ_shares_new': typ_shares_new,})\n",
    "\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Extract Währung\n",
    "stocks_new['currency_shares_new'] = stocks_new['val_shares_new'].str.extract(r'^([^\\d\\s]+)')\n",
    "stocks_new['currency_shares_new'] = stocks_new['currency_shares_new'].fillna('')\n",
    "\n",
    "# Extract number of Stocks, etc.\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "# Apply correction to remove unneccessary punctuations\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].apply(correct_number)\n",
    "stocks_new['val_shares_new'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in stocks_new['val_shares_new']]\n",
    "stocks_new.loc[stocks_new['val_shares_new'] == '.', 'val_shares_new'] = np.nan\n",
    "\n",
    "# Ensure correct types\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].astype(float)\n",
    "stocks_new['num_shares_new'] = stocks_new['num_shares_new'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total value of the capital by multiplying the number of shares with their individual value\n",
    "stocks_new['value_total'] = stocks_new['num_shares_new'] * stocks_new['val_shares_new']\n",
    "\n",
    "# Calculate new capital for shab ids where kapital_neu variable is not given, but scheine_neu is\n",
    "missing_ids = set(stocks_new.shab_id).difference(set(cap_new.shab_id))\n",
    "stocks_new_missing = stocks_new[stocks_new.shab_id.isin(missing_ids)]\n",
    "\n",
    "cap_new_missing = stocks_new_missing.groupby(['shab_id', 'keyword', 'main_group']).agg(\n",
    "    capital_new=pd.NamedAgg(column='value_total', aggfunc='sum'),\n",
    "    currency_new=pd.NamedAgg(column='currency_shares_new', aggfunc=lambda x: list(set([currency for currency in x if currency != ''])))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are mixed currencies\n",
    "assert len([cur_set for cur_set in cap_new_missing['currency_new'] if len(cur_set) > 1]) == 0\n",
    "\n",
    "cap_new_missing['currency_new'] = [v[0] if len(v) > 0 else '' for v in cap_new_missing['currency_new']]\n",
    "cap_new_missing['currency_until_now'] = ''\n",
    "cap_new_missing['capital_new'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes to get all capital changes\n",
    "cap_new_concat = pd.concat([cap_new, cap_new_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_new_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_new_concat.to_csv(EXTERNAL_DATA_DIR / 'capital_changes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS MERGERS AND ACQUISIITONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers_and_acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shab_ids = []\n",
    "keywords = []\n",
    "main_groups = []\n",
    "text_passages = []\n",
    "\n",
    "contract_dates = []\n",
    "bilanzdaten = []\n",
    "\n",
    "firm_taken_over = []\n",
    "location_taken_over = []\n",
    "id_taken_over = []\n",
    "assets_taken_over = []\n",
    "liabilities_taken_over = []\n",
    "\n",
    "for i, row in mergers_and_acquisitions.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    keyword = row['keyword']\n",
    "    main_group = row['main_group']\n",
    "    text_passage = row['text_slice']\n",
    "\n",
    "    parsed_variables = row['parsed_variables']\n",
    "\n",
    "    if parsed_variables:\n",
    "        contract_date = parsed_variables.get('contract_date', [])\n",
    "        bilanzdatum = parsed_variables.get('bilanzdatum', [])\n",
    "        firms_taken_over = parsed_variables.get('firms_taken_over', [])\n",
    "        for firm in firms_taken_over:\n",
    "            firm_name = firm.get('firm_name', '')\n",
    "            location = firm.get('location', '')\n",
    "            id = firm.get('id', '')\n",
    "            capital_taken_over = firm.get('capital_taken_over', {})\n",
    "            assets = capital_taken_over.get('aktiven', '') if capital_taken_over else ''\n",
    "            liabilities = capital_taken_over.get('passiven', '') if capital_taken_over else ''\n",
    "\n",
    "            contract_dates.append(contract_date[0] if len(contract_date) > 0 else '')\n",
    "            bilanzdaten.append(bilanzdatum[0] if len(bilanzdatum) > 0 else '')\n",
    "            firm_taken_over.append(firm_name)\n",
    "            location_taken_over.append(location)\n",
    "            id_taken_over.append(id)\n",
    "            assets_taken_over.append(assets)\n",
    "            liabilities_taken_over.append(liabilities)\n",
    "            shab_ids.append(shab_id)\n",
    "            keywords.append(keyword)\n",
    "            main_groups.append(main_group)\n",
    "            text_passages.append(text_passage)\n",
    "    else:\n",
    "        firm_taken_over.append('')\n",
    "        location_taken_over.append('')\n",
    "        id_taken_over.append('')\n",
    "        assets_taken_over.append('')\n",
    "        liabilities_taken_over.append('')\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        main_groups.append(main_group)\n",
    "        text_passages.append(text_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(shab_ids) == len(keywords) == len(main_groups) == len(firm_taken_over) == len(location_taken_over) == len(id_taken_over) == len(assets_taken_over) == len(liabilities_taken_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mergers = pd.DataFrame({\n",
    "    'shab_id': shab_ids,\n",
    "    'keyword': keywords,\n",
    "    'hauptkategorie': main_groups,\n",
    "    'texte': text_passages,\n",
    "    'firm_taken_over': firm_taken_over,\n",
    "    'location_taken_over': location_taken_over,\n",
    "    'id_taken_over': id_taken_over,\n",
    "    'assets_taken_over': assets_taken_over,\n",
    "    'liabilities_taken_over': liabilities_taken_over\n",
    "})\n",
    "\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Extract Währung of Aktiven/Passiven\n",
    "processed_mergers['currency_assets_taken_over'] = processed_mergers['assets_taken_over'].str.extract(r'^([^\\d\\s]+)')\n",
    "processed_mergers['currency_assets_taken_over'] = processed_mergers['currency_assets_taken_over'].fillna('')\n",
    "\n",
    "processed_mergers['currency_liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].str.extract(r'^([^\\d\\s]+)')\n",
    "processed_mergers['currency_liabilities_taken_over'] = processed_mergers['currency_liabilities_taken_over'].fillna('')\n",
    "\n",
    "# Extract value of Aktiven/Passiven\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "# Apply correction to remove unneccessary punctuations\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].apply(correct_number)\n",
    "processed_mergers['assets_taken_over'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in processed_mergers['assets_taken_over']]\n",
    "\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].apply(correct_number)\n",
    "processed_mergers['liabilities_taken_over'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in processed_mergers['liabilities_taken_over']]\n",
    "\n",
    "# Ensure correct types\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].astype(float)\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mergers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mergers.to_csv(EXTERNAL_DATA_DIR / 'merger_sizes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
