{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manuelbolz/Documents/git/for_work/company_success_prediction/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-06-07 22:55:24.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msuccess_prediction.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/manuelbolz/Documents/git/for_work/company_success_prediction\u001b[0m\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/manuelbolz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/manuelbolz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manuelbolz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ftlangdetect import detect\n",
    "from geopy.geocoders import Nominatim, GoogleV3\n",
    "from shapely.geometry import Point\n",
    "from thefuzz import fuzz, process\n",
    "from transformers import pipeline\n",
    "\n",
    "from pocketknife.database import (\n",
    "    connect_database, read_from_database)\n",
    "\n",
    "from success_prediction.config import (\n",
    "    PROJ_ROOT, RAW_DATA_DIR, EXTERNAL_DATA_DIR, PROCESSED_DATA_DIR)\n",
    "from success_prediction.zefix_processing.clustering import PersonClustering\n",
    "\n",
    "dotenv_path = os.path.join(PROJ_ROOT, '.env')\n",
    "dotenv.load_dotenv(dotenv_path)\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARE INSCRIBED PEOPLE/FIRMS DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_inscribed_people = \"\"\" \n",
    "    SELECT * FROM zefix.history_inscribed_people WHERE founders = TRUE AND shab_date < '2024-01-01';\n",
    "\"\"\"\n",
    "\n",
    "query_inscribed_firms = \"\"\" \n",
    "    SELECT * FROM zefix.history_inscribed_firms WHERE shab_date < '2024-01-01';\n",
    "\"\"\"\n",
    "\n",
    "with connect_database() as con:\n",
    "    df_insc_people = read_from_database(connection=con, query=query_inscribed_people)\n",
    "    df_insc_firms = read_from_database(connection=con, query=query_inscribed_firms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_fids_with_combine_first(df, group_cols=['ehraid', 'fid']):\n",
    "    \"\"\"\n",
    "    Combines instances of the same fid within a company and fills missing\n",
    "    to create a complete representation for each founder\n",
    "    \"\"\"\n",
    "    filled_rows = []\n",
    "    for _, group in df.groupby(group_cols, sort=False):\n",
    "        # Use first entry as base instance of fid\n",
    "        combined = group.iloc[0]\n",
    "        # Iteratively combine_first with the next row(s)\n",
    "        for i in range(1, len(group)):\n",
    "            combined = combined.combine_first(group.iloc[i])\n",
    "        filled_rows.append(combined)\n",
    "    return pd.DataFrame(filled_rows).reset_index(drop=True)\n",
    "\n",
    "def build_founder_dict(df):\n",
    "    founder_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        ehraid = row['ehraid']\n",
    "        fid = row['fid']\n",
    "        \n",
    "        # Handle BFS code lists for hometown (1–5)\n",
    "        hometown_bfs_codes_latest = [\n",
    "            int(code) for code in [\n",
    "                row.get('hometown_1_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_2_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_3_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_4_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_5_bfs_gmde_code_latest'),\n",
    "            ] if pd.notnull(code)\n",
    "        ]\n",
    "        \n",
    "        # Handle BFS code lists for places of residence (1–2)\n",
    "        place_of_residence_bfs_codes_latest = [\n",
    "            int(code) for code in [\n",
    "                row.get('place_of_residence_1_bfs_gmde_code_latest'),\n",
    "                row.get('place_of_residence_2_bfs_gmde_code_latest'),\n",
    "            ] if pd.notnull(code)\n",
    "        ]\n",
    "        \n",
    "        # Handle nationality codes (1–3)\n",
    "        nationality_iso_codes = [\n",
    "            str(code) for code in [\n",
    "                row.get('nationality_1_iso_3166_1_alpha_2'),\n",
    "                row.get('nationality_2_iso_3166_1_alpha_2'),\n",
    "                row.get('nationality_3_iso_3166_1_alpha_2'),\n",
    "            ] if pd.notnull(code) and code != np.nan\n",
    "        ]\n",
    "        \n",
    "        # Prepare entry\n",
    "        founder_data = {\n",
    "            'first_name': row.get('first_name') or '',\n",
    "            'last_name': row.get('last_name') or '',\n",
    "            'first_name_norm': row.get('first_name_norm') or '',\n",
    "            'last_name_norm': row.get('last_name_norm') or '',\n",
    "            'gender': row.get('gender') or '',\n",
    "            'job_title': row.get('job_title') or '',\n",
    "            'dr_title': row.get('founder_with_academic_title'),\n",
    "            'signing_rights': row.get('signing_rights') or '',\n",
    "            'shares': row.get('shares') or '',\n",
    "            'hometown_bfs_codes_latest': hometown_bfs_codes_latest,\n",
    "            'place_of_residence_bfs_codes_latest': place_of_residence_bfs_codes_latest,\n",
    "            'nationality_iso_codes': nationality_iso_codes,\n",
    "        }\n",
    "        \n",
    "        # Insert into nested dictionary\n",
    "        if ehraid not in founder_dict:\n",
    "            founder_dict[ehraid] = {}\n",
    "        founder_dict[ehraid][fid] = founder_data\n",
    "    \n",
    "    return founder_dict\n",
    "\n",
    "def count_founders(founder_dict):\n",
    "    stats = {}\n",
    "    for ehraid, founders in founder_dict.items():\n",
    "        total = 0\n",
    "        male = 0\n",
    "        female = 0\n",
    "        swiss = 0\n",
    "        foreign = 0\n",
    "        dr = 0\n",
    "        for fdata in founders.values():\n",
    "            total += 1\n",
    "            gender = fdata.get('gender', '')\n",
    "            if gender == 'm':\n",
    "                male += 1\n",
    "            elif gender == 'f':\n",
    "                female += 1\n",
    "            nationalities = fdata.get('nationality_iso_codes', [])\n",
    "            if 'CH' in nationalities:\n",
    "                swiss += 1\n",
    "            else:\n",
    "                foreign += 1\n",
    "            dr += fdata.get('dr_title', 0)\n",
    "\n",
    "        stats[ehraid] = {\n",
    "            'n_founders': total,\n",
    "            'n_female_founders': female,\n",
    "            'n_male_founders': male,\n",
    "            'n_swiss_founders': swiss,\n",
    "            'n_foreign_founders': foreign,\n",
    "            'n_dr_titles': dr\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "def get_founder_lists(founder_dict):\n",
    "    stats = {}\n",
    "    for ehraid, founders in founder_dict.items():\n",
    "        names = [(fdata.get('first_name_norm', ''), fdata.get('last_name_norm', '')) for fdata in founders.values()]\n",
    "        fids = [k for k in founders.keys()]\n",
    "        nationalities = [fdata.get('nationality_iso_codes', []) for fdata in founders.values()]\n",
    "        hometowns = [fdata.get('hometown_bfs_codes_latest', []) for fdata in founders.values()]\n",
    "        residencies = [fdata.get('place_of_residence_bfs_codes_latest', []) for fdata in founders.values()]\n",
    "\n",
    "        stats[ehraid] = {\n",
    "            'founder_names': names,\n",
    "            'founder_fids': fids,\n",
    "            'founder_nationalities': nationalities,\n",
    "            'founder_hometowns': hometowns,\n",
    "            'founder_residencies': residencies\n",
    "        }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process and cluster the dataframe\n",
    "bfs_code_cols = [col for col in df_insc_people.columns if 'bfs_gmde_code_' in col]\n",
    "df_insc_people[bfs_code_cols] = df_insc_people[bfs_code_cols].astype(str).replace('0', np.nan)\n",
    "df_insc_people[bfs_code_cols] = df_insc_people[bfs_code_cols].replace('None', np.nan).replace('', np.nan)\n",
    "\n",
    "# Cluster the people\n",
    "clustering = PersonClustering(df_insc_people)\n",
    "clustered_df = clustering.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cluster people within company: 100%|██████████| 357357/357357 [09:51<00:00, 604.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Group fids within companies\n",
    "filled_df = group_fids_with_combine_first(clustered_df)\n",
    "for col in bfs_code_cols:\n",
    "    filled_df[col] = filled_df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor_titles = [\n",
    "    r\"dr\\.?\",\n",
    "    r\"doctor\",\n",
    "    r\"doktor\",\n",
    "    r\"prof\\.?\",\n",
    "    r\"ph\\.?\\s*d\\.?\",         # PhD / Ph.D.\n",
    "    r\"dphil\\.?\",             # DPhil (Oxford)\n",
    "    r\"sc\\.?\\s*d\\.?\",         # ScD / Sc.D.\n",
    "    r\"dsc\\.?\", r\"drsc\\.?\",   # DSc / DrSc\n",
    "    r\"dr\\.?\\s*-?\\s*ing\\.?\",  # Dr-Ing.\n",
    "    r\"dott\\.?\",  # Italian variants\n",
    "    r\"dottore\",\n",
    "    r\"hdr\", # French short forms\n",
    "]\n",
    "pattern = re.compile(rf\"\\b(?:{'|'.join(doctor_titles)})\\b\", flags=re.IGNORECASE)\n",
    "filled_df['founder_with_academic_title'] = filled_df['first_name'].fillna('').str.contains(pattern) | filled_df['last_name'].fillna('').str.contains(pattern)\n",
    "filled_df['founder_with_academic_title'] = filled_df['founder_with_academic_title'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary with founders\n",
    "founder_dict = build_founder_dict(filled_df)\n",
    "\n",
    "# Count the total founders, female foundes, and male founders\n",
    "count_stats = count_founders(founder_dict)\n",
    "\n",
    "# Get the names, nationalities, hometowns, and place of residency of all founders as flat lists\n",
    "founder_lists = get_founder_lists(founder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the number of inscribed firms at founding\n",
    "grouped_insc_firms = df_insc_firms.groupby(['ehraid', 'shab_date']).agg({'shab_id': 'count'}).reset_index().rename(columns={'shab_id': 'n_inscribed_firms', 'shab_date': 'firm_inscription_date'})\n",
    "grouped_insc_firms = grouped_insc_firms.sort_values('firm_inscription_date').drop_duplicates(subset=['ehraid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARE FIRM-LEVEL DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2legalform = {\n",
    "    1: 'Sole proprietorship',  # Einzelunternehmen  ->  EXCLUDE\n",
    "    2: 'General Partnership',  # Kollektivgesellschaft  ->  INCLUDE\n",
    "    3: 'Corporation',  # Aktiengesellschaft  ->  INCLUDE\n",
    "    4: 'Limited Liability Company',  # Gesellschaft mit beschränkter Haftung  ->  INCLUDE\n",
    "    5: 'Cooperative',  # Genossenschaft  ->  EXCLUDE\n",
    "    6: 'Association',  # Verein  ->  EXCLUDE\n",
    "    7: 'Foundation',  # Stiftung  ->  EXCLUDE\n",
    "    8: 'Public sector institution',  # Institut des öffentlichen Rechts  ->  EXCLUDE\n",
    "    9: 'Branch',  # Zweigniederlassung  ->  EXCLUDE\n",
    "    10: 'Limited Partnership',  # Kommanditgesellschaft  ->  INCLUDE\n",
    "    11: 'Foreign branch',  # Zweigniederlassung einer ausl. Gesellschaft  ->  EXCLUDE\n",
    "    12: 'Corporation with unlimited partners',  # Kommanditaktiengesellschaft  ->  INCLUDE\n",
    "    13: 'Special legal form',  # Besondere Rechtsform  ->  EXCLUDE\n",
    "    14: 'Ownership in undivided shares',  # Gemeinderschaft  ->  EXCLUDE\n",
    "    15: 'Limited Partnership for collective investment schemes with a fixed capital',  # Investmentgesellschaft mit festem Kapital  ->  INCLUDE\n",
    "    16: 'Limited Partnership for collective investment schemes with a variable capital',  # Investmentgesellschaft mit variablem Kapital  ->  INCLUDE\n",
    "    17: 'Limited Partnership for collective investment schemes',  # Kommanditgesellschaft für kollektive Kapitalanlagen  ->  INCLUDE\n",
    "    18: 'Non commercial power of attorney',  # Nichtkaufmännische Prokure  ->  EXCLUDE\n",
    "    19: '(unknown)',  # (unbekannt)  ->  EXCLUDE\n",
    "}\n",
    "\n",
    "growth_oriented_legal_forms = [2, 3, 4, 10, 12, 15, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query gets the sample of growth oriented firms that were founded between 2016 and current for the prediction sample\n",
    "\n",
    "query_founded_firms = \"\"\" \n",
    "    SELECT\n",
    "        base.ehraid,\n",
    "        base.uid,\n",
    "\n",
    "        -- Dissolution information\n",
    "        base.delete_date,\n",
    "        dissolution.shab_date as dissolution_date,\n",
    "        dissolution.reason_for_dissolution,\n",
    "        dissolution.liquidation,\n",
    "        dissolution.bankruptcy,\n",
    "        dissolution.other_exit,\n",
    "\n",
    "        -- Names\n",
    "        base.name AS current_name,\n",
    "        founding_name.firm_name AS founding_name,\n",
    "\n",
    "        -- Legal forms\n",
    "        base.legal_form_id AS current_legal_form,\n",
    "        legal_form.legal_form_id AS founding_legal_form,\n",
    "\n",
    "        -- Purpose\n",
    "        base.purpose_raw AS current_purpose,\n",
    "        founding_purpose.purpose_raw AS founding_purpose,\n",
    "        founding_purpose.purpose_clean AS founding_purpose_clean,\n",
    "\n",
    "        -- Founding NOGA code\n",
    "        founding_purpose.section_1_label AS founding_section,\n",
    "        founding_purpose.sec.class_1_label AS founding_class,\n",
    "\n",
    "        -- Current address\n",
    "        COALESCE(address.street, '') || ' ' || COALESCE(address.house_number, '') AS current_street,\n",
    "        address.town AS current_town,\n",
    "        address.swiss_zip_code AS current_zip_code,\n",
    "        address.country AS current_country,\n",
    "\n",
    "        -- Founding address\n",
    "        founding_address.street AS founding_street,\n",
    "        founding_address.town AS founding_town,\n",
    "        founding_address.postal_code AS founding_zip_code,\n",
    "        founding_address.town_bfs_gmde_code_latest AS founding_bfs_code,\n",
    "\n",
    "        -- Founding SHAB entry\n",
    "        shab.shab_id,\n",
    "        shab.shab_date AS founding_date,\n",
    "        shab.message AS founding_message\n",
    "\n",
    "    FROM zefix_release_159.base base\n",
    "\n",
    "    -- Founding SHAB messages\n",
    "    INNER JOIN (\n",
    "        SELECT s.ehraid, s.shab_id, s.shab_date, s.message\n",
    "        FROM zefix_release_159.shab s\n",
    "        INNER JOIN zefix_release_159.shab_mutation sm ON s.shab_id = sm.shab_id\n",
    "        WHERE sm.description = 'status.neu'\n",
    "    ) AS shab ON base.ehraid = shab.ehraid\n",
    "\n",
    "    -- Current address\n",
    "    LEFT JOIN zefix_release_159.address address ON base.ehraid = address.ehraid\n",
    "\n",
    "    -- Founding address\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT hfa.ehraid, hfa.street, hfa.postal_code, hfa.town, hfa.town_bfs_gmde_code_latest\n",
    "        FROM zefix.history_firm_addresses hfa\n",
    "        WHERE founding = TRUE\n",
    "    ) AS founding_address ON base.ehraid = founding_address.ehraid\n",
    "\n",
    "    -- Founding name\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT hfn.ehraid, hfn.firm_name\n",
    "        FROM zefix.history_firm_names hfn\n",
    "        WHERE hfn.founding = TRUE\n",
    "    ) AS founding_name ON base.ehraid = founding_name.ehraid\n",
    "\n",
    "    -- Founding purpose\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT hp.ehraid, hp.purpose_raw, sec.purpose as purpose_clean, sec.section_1_label, sec.class_1_label\n",
    "        FROM zefix.history_purpose hp\n",
    "        LEFT JOIN zefix.history_sector sec\n",
    "        ON hp.ehraid = sec.ehraid AND hp.shab_id = sec.shab_id\n",
    "        WHERE hp.founding_purpose = TRUE\n",
    "    ) AS founding_purpose ON base.ehraid = founding_purpose.ehraid\n",
    "\n",
    "    -- Founding legal form\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT hlf.ehraid, hlf.legal_form_id\n",
    "        FROM zefix.history_founding_legal_form hlf\n",
    "    ) AS legal_form ON base.ehraid = legal_form.ehraid\n",
    "\n",
    "    -- Dissolution information\n",
    "    LEFT JOIN (\n",
    "        -- Only keep the last dissolution message as the final dissolution\n",
    "        SELECT hd.ehraid, hd.shab_date, hd.reason_for_dissolution, hd.liquidation, hd.bankruptcy, hd.other_exit\n",
    "        FROM (\n",
    "            SELECT *,\n",
    "                ROW_NUMBER() OVER (PARTITION BY ehraid ORDER BY shab_date DESC) AS rn\n",
    "            FROM zefix.history_dissolutions\n",
    "        ) hd\n",
    "        WHERE hd.rn = 1\n",
    "    ) AS dissolution ON base.ehraid = dissolution.ehraid\n",
    "\n",
    "    -- Filter out irrelevant records\n",
    "    WHERE\n",
    "        NOT base.is_branch\n",
    "        AND shab.shab_date < '2024-01-01'\n",
    "        AND base.legal_form_id IN (2, 3, 4, 10, 12, 15, 16, 17)\n",
    "        AND LOWER(base.name) NOT LIKE '%zweigniederlassung%'\n",
    "        AND LOWER(base.name) NOT LIKE '%succursale%';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    df_startups = read_from_database(connection=con, query=query_founded_firms)\n",
    "\n",
    "df_startups['founding_date'] = pd.to_datetime(df_startups['founding_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(226559, 25)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observed duplicates stem from entries having multiple new inscriptions in Zefix. -> Remove them from the sample because history seems to contain errors\n",
    "display(df_startups[df_startups.duplicated(subset=['ehraid', 'founding_town'], keep=False)].uid.unique())\n",
    "df_startups = df_startups.drop_duplicates(subset=['ehraid'], keep=False)\n",
    "display(df_startups.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEO ENCODE ADDRESS INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current information if the founding address is missing\n",
    "df_startups['founding_street'] = df_startups['founding_street'].fillna(df_startups['current_street'])\n",
    "df_startups['founding_zip_code'] = df_startups['founding_zip_code'].fillna(df_startups['current_zip_code'])\n",
    "df_startups['founding_town'] = df_startups['founding_town'].fillna(df_startups['current_town'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_startups[df_startups['founding_street'].isna()].empty\n",
    "assert df_startups[df_startups['founding_zip_code'].isna()].empty\n",
    "assert df_startups[df_startups['founding_town'].isna()].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominatim_geolocator = Nominatim(\n",
    "    user_agent=\"local_geocoder\",\n",
    "    domain=\"localhost:8080\",\n",
    "    scheme=\"http\"\n",
    ")\n",
    "google_geolocator = GoogleV3(api_key=os.getenv('GOOGLE_GEOCODE_API_KEY'))\n",
    "\n",
    "\n",
    "def geocode_address(nominatim_geolocator, google_geolocator, row):\n",
    "    try:\n",
    "        location = nominatim_geolocator.geocode({\n",
    "            'street': row['founding_street'],\n",
    "            'city': row['founding_town'],\n",
    "            'postalcode': int(row['founding_zip_code']),\n",
    "            'country': 'Schweiz'\n",
    "        }, timeout=2)\n",
    "        if location:\n",
    "            return pd.Series([location.address, location.latitude, location.longitude])\n",
    "        else:\n",
    "            location = google_geolocator.geocode({\n",
    "                'street': row['founding_street'],\n",
    "                'city': row['founding_town'],\n",
    "                'postalcode': int(row['founding_zip_code']),\n",
    "                'country': 'Schweiz'\n",
    "            }, timeout=1)\n",
    "            if location:\n",
    "                return pd.Series([location.address, location.latitude, location.longitude])\n",
    "            return pd.Series([None, None, None])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.Series([None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[['geocoded_address', 'latitude', 'longitude']] = df_startups.apply(lambda row: geocode_address(nominatim_geolocator, google_geolocator, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['founding_bfs_code'] = df_startups['founding_bfs_code'].astype(int)\n",
    "df_startups['founding_zip_code'] = df_startups['founding_zip_code'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETERMINE BFS MUNICIPALITY CODE BY COORDINATES WHERE MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(EXTERNAL_DATA_DIR / 'geo_data' / 'swissBOUNDARIES3D_1_5_LV95_LN02.gdb', layer=\"TLM_HOHEITSGEBIET\")\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "gdf = gdf[['geometry', 'BFS_NUMMER', 'EINWOHNERZAHL']]\n",
    "\n",
    "df_startups = gpd.GeoDataFrame(\n",
    "    df_startups,\n",
    "    geometry=gpd.points_from_xy(df_startups['longitude'], df_startups['latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "df_startups = gpd.sjoin(df_startups, gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Replace where code is 0 (unmatched) or where it does not match the coordinates\n",
    "df_startups.loc[df_startups['founding_bfs_code'] == 0, 'founding_bfs_code'] = pd.NA\n",
    "df_startups.loc[(df_startups['founding_bfs_code'].astype(float) != df_startups['BFS_NUMMER']) & (~df_startups['BFS_NUMMER'].isna()), 'founding_bfs_code'] = pd.NA\n",
    "df_startups['founding_bfs_code'] = df_startups['founding_bfs_code'].fillna(df_startups['BFS_NUMMER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['founding_bfs_code'] = df_startups['founding_bfs_code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>founding_town</th>\n",
       "      <th>combined_address</th>\n",
       "      <th>founding_bfs_code</th>\n",
       "      <th>BFS_NUMMER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223603</th>\n",
       "      <td>Chiasso</td>\n",
       "      <td>Via Henry Dunant 1, 6830 Chiasso</td>\n",
       "      <td>5250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223613</th>\n",
       "      <td>Morcote</td>\n",
       "      <td>Via Isella 11, 6922 Morcote</td>\n",
       "      <td>5203</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223639</th>\n",
       "      <td>Brusino Arsizio</td>\n",
       "      <td>Via Lungolago 83, 6827 Brusino Arsizio</td>\n",
       "      <td>5160</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224506</th>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>Residenza Mons Avium , appartamento 25, 6565 S...</td>\n",
       "      <td>3822</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224785</th>\n",
       "      <td>La Tène</td>\n",
       "      <td>route de Bellevue 7, 2074 La Tène</td>\n",
       "      <td>6513</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225829</th>\n",
       "      <td>Roggwil TG</td>\n",
       "      <td>Im Pünst 1, 9325 Roggwil TG</td>\n",
       "      <td>4431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225873</th>\n",
       "      <td>Bassins</td>\n",
       "      <td>Chemin de Raulan 24, 1269 Bassins</td>\n",
       "      <td>5703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226046</th>\n",
       "      <td>Warth</td>\n",
       "      <td>Kartause Ittingen, 8532 Warth</td>\n",
       "      <td>4621</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          founding_town                                   combined_address  \\\n",
       "223603          Chiasso                   Via Henry Dunant 1, 6830 Chiasso   \n",
       "223613          Morcote                        Via Isella 11, 6922 Morcote   \n",
       "223639  Brusino Arsizio             Via Lungolago 83, 6827 Brusino Arsizio   \n",
       "224506   San Bernardino  Residenza Mons Avium , appartamento 25, 6565 S...   \n",
       "224785          La Tène                  route de Bellevue 7, 2074 La Tène   \n",
       "225829       Roggwil TG                        Im Pünst 1, 9325 Roggwil TG   \n",
       "225873          Bassins                  Chemin de Raulan 24, 1269 Bassins   \n",
       "226046            Warth                      Kartause Ittingen, 8532 Warth   \n",
       "\n",
       "        founding_bfs_code  BFS_NUMMER  \n",
       "223603               5250         NaN  \n",
       "223613               5203         NaN  \n",
       "223639               5160         NaN  \n",
       "224506               3822         NaN  \n",
       "224785               6513         NaN  \n",
       "225829               4431         NaN  \n",
       "225873               5703         NaN  \n",
       "226046               4621         NaN  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups[df_startups.founding_bfs_code.astype(float) != df_startups.BFS_NUMMER][['founding_town', 'combined_address', 'founding_bfs_code', 'BFS_NUMMER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups.drop(columns=['geometry', 'index_right', 'Unnamed: 0', 'BFS_NUMMER'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD MUNICIPALITY TYPOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typology = pd.read_excel(EXTERNAL_DATA_DIR / 'geo_data' / 'Raumgliederungen.xlsx')\n",
    "df_typology.drop(columns=['Gemeindename', 'Bezirksname', 'Kanton'], inplace=True)\n",
    "df_typology = df_typology.rename(columns={'BFS Gde-nummer': 'founding_bfs_code', 'Bezirks-nummer': 'district_id', 'Kantons-nummer': 'canton_id', 'Stadt/Land-Typologie': 'urban_rural', 'Gemeindetypologie (9 Typen)': 'typology_9c', 'Gemeindetypologie (25 Typen)': 'typology_25c'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups = df_startups.merge(df_typology, on='founding_bfs_code', how='left')\n",
    "df_startups.rename(columns={'EINWOHNERZAHL': 'population'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "      <th>delete_date</th>\n",
       "      <th>reason_for_dissolution</th>\n",
       "      <th>liquidation</th>\n",
       "      <th>bankruptcy</th>\n",
       "      <th>other_exit</th>\n",
       "      <th>current_name</th>\n",
       "      <th>founding_name</th>\n",
       "      <th>current_legal_form</th>\n",
       "      <th>...</th>\n",
       "      <th>combined_address</th>\n",
       "      <th>geocoded_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "      <th>canton_id</th>\n",
       "      <th>Bezirks-nummer</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>typology_9c</th>\n",
       "      <th>typology_25c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1255845</td>\n",
       "      <td>CHE395917849</td>\n",
       "      <td>2020-06-19</td>\n",
       "      <td>['Nachdem kein begründeter Einspruch gegen die...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Same Same GmbH in Liquidation</td>\n",
       "      <td>Same Same GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Seeplatz 1, 8820 Wädenswil</td>\n",
       "      <td>1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...</td>\n",
       "      <td>47.228758</td>\n",
       "      <td>8.676404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80542</th>\n",
       "      <td>1389236</td>\n",
       "      <td>CHE291873431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MS Glärnisch AG</td>\n",
       "      <td>MS Glärnisch AG</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Seeplatz 1, 8820 Wädenswil</td>\n",
       "      <td>1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...</td>\n",
       "      <td>47.228758</td>\n",
       "      <td>8.676404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223147</th>\n",
       "      <td>1310452</td>\n",
       "      <td>CHE338654358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mit Entscheid vom 07.01.2025 hat der Einzelr...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Peter Jegen GmbH in Liquidation</td>\n",
       "      <td>Peter Jegen GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Sagastrasse 3, 7214 Grüsch</td>\n",
       "      <td>Sägastrasse 3, 9495 Triesen, Liechtenstein</td>\n",
       "      <td>47.088149</td>\n",
       "      <td>9.522204</td>\n",
       "      <td>5532.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ehraid           uid delete_date  \\\n",
       "2469    1255845  CHE395917849  2020-06-19   \n",
       "80542   1389236  CHE291873431         NaN   \n",
       "223147  1310452  CHE338654358         NaN   \n",
       "\n",
       "                                   reason_for_dissolution liquidation  \\\n",
       "2469    ['Nachdem kein begründeter Einspruch gegen die...       False   \n",
       "80542                                                 NaN         NaN   \n",
       "223147  ['Mit Entscheid vom 07.01.2025 hat der Einzelr...       False   \n",
       "\n",
       "       bankruptcy other_exit                     current_name  \\\n",
       "2469        False       True    Same Same GmbH in Liquidation   \n",
       "80542         NaN        NaN                  MS Glärnisch AG   \n",
       "223147       True      False  Peter Jegen GmbH in Liquidation   \n",
       "\n",
       "           founding_name  current_legal_form  ...            combined_address  \\\n",
       "2469      Same Same GmbH                   4  ...  Seeplatz 1, 8820 Wädenswil   \n",
       "80542    MS Glärnisch AG                   3  ...  Seeplatz 1, 8820 Wädenswil   \n",
       "223147  Peter Jegen GmbH                   4  ...  Sagastrasse 3, 7214 Grüsch   \n",
       "\n",
       "                                         geocoded_address   latitude  \\\n",
       "2469    1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...  47.228758   \n",
       "80542   1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...  47.228758   \n",
       "223147         Sägastrasse 3, 9495 Triesen, Liechtenstein  47.088149   \n",
       "\n",
       "       longitude population  canton_id Bezirks-nummer urban_rural typology_9c  \\\n",
       "2469    8.676404        0.0        NaN            NaN         NaN         NaN   \n",
       "80542   8.676404        0.0        NaN            NaN         NaN         NaN   \n",
       "223147  9.522204     5532.0        NaN            NaN         NaN         NaN   \n",
       "\n",
       "        typology_25c  \n",
       "2469             NaN  \n",
       "80542            NaN  \n",
       "223147           NaN  \n",
       "\n",
       "[3 rows x 34 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups[df_startups.canton_id.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_startups.to_csv(RAW_DATA_DIR / 'company_sample' / 'geo_coded_company_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_startups = pd.read_csv(RAW_DATA_DIR / 'company_sample' / 'geo_coded_company_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD STARTING CAPITAL TO COMPANY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical exchange rates\n",
    "import yfinance as yf\n",
    "\n",
    "exchange_rate_dfs = []\n",
    "for symbol in ['EUR', 'GBP', 'USD']:\n",
    "    ticker = yf.Ticker(f'{symbol}CHF=X')\n",
    "    df_ticker = ticker.history(start='2016-01-01', end='2024-01-01')\n",
    "    df_ticker['symbol'] = symbol\n",
    "    exchange_rate_dfs.append(df_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exchange_rates = pd.concat(exchange_rate_dfs).reset_index()[['Date', 'symbol', 'Open']]\n",
    "\n",
    "df_exchange_rates = df_exchange_rates.rename(columns={'Date': 'founding_date'})\n",
    "df_exchange_rates['founding_date'] = pd.to_datetime(df_exchange_rates['founding_date']).dt.date\n",
    "df_exchange_rates['founding_date'] = pd.to_datetime(df_exchange_rates['founding_date'])\n",
    "\n",
    "df_temp = pd.DataFrame({'founding_date': pd.date_range(start='2016-01-01', end='2024-01-01').tolist() * 3})\n",
    "df_temp['symbol'] = ['EUR'] * int(len(df_temp) / 3) + ['GBP'] * int(len(df_temp) / 3) + ['USD'] * int(len(df_temp) / 3)\n",
    "\n",
    "df_exchange_rates = df_temp.merge(df_exchange_rates, on=['founding_date', 'symbol'], how='left')\n",
    "df_exchange_rates['Open'] = df_exchange_rates['Open'].ffill()\n",
    "df_exchange_rates['symbol'] = df_exchange_rates['symbol'].ffill()\n",
    "\n",
    "df_exchange_rates.to_csv(EXTERNAL_DATA_DIR / 'exchange_rates' / 'exchange_rates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_capital = \"\"\" \n",
    "    SELECT * FROM zefix.history_registered_capital WHERE shab_date < '2024-01-01';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    df_capital = read_from_database(connection=con, query=query_capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capital = df_capital.rename(columns={'shab_date': 'founding_date', 'currency_new': 'symbol'})\n",
    "df_capital['founding_date'] = pd.to_datetime(df_capital['founding_date'])\n",
    "\n",
    "mapping = {\n",
    "    'Euro': 'EUR',\n",
    "    'Eur': 'EUR',\n",
    "    'EURO': 'EUR',\n",
    "    '€': 'EUR',\n",
    "    'fr': 'CHF',\n",
    "    'Fr.': 'CHF',\n",
    "    'CHE': 'CHF',\n",
    "    '£': 'GBP',\n",
    "    'US': 'USD'\n",
    "}\n",
    "df_capital['symbol'] = df_capital['symbol'].replace(mapping)\n",
    "\n",
    "# Drop duplicate entries where we have libaration information do avoid duplicates before aggregation\n",
    "df_capital = df_capital[~df_capital.duplicated(subset=['ehraid', 'founding_date'], keep=False) | (df_capital.duplicated(subset=['ehraid', 'founding_date'], keep=False) & ~(df_capital['keyword'].str.contains('liberierung|liberato|libéré', regex=True)))]\n",
    "\n",
    "# Drop entries where the currency is not a common currency\n",
    "df_capital = df_capital[df_capital.symbol.isin(['CHF', 'EUR', 'USD', 'GBP'])]\n",
    "\n",
    "# Add exchange rates and convert registered capital\n",
    "df_capital = df_capital.merge(df_exchange_rates, on=['symbol', 'founding_date'], how='left')\n",
    "df_capital['Open'] = df_capital['Open'].fillna(1.0)\n",
    "df_capital['capital_chf'] = df_capital['capital_new'].astype(float) * df_capital['Open'].astype(float)\n",
    "\n",
    "# Aggregate capital into one value for registered capital\n",
    "df_capital_agg = df_capital.groupby(['ehraid', 'founding_date']).agg({'capital_chf': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups = df_startups.merge(df_capital_agg[['ehraid', 'founding_date', 'capital_chf']], on=['ehraid', 'founding_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "      <th>delete_date</th>\n",
       "      <th>reason_for_dissolution</th>\n",
       "      <th>liquidation</th>\n",
       "      <th>bankruptcy</th>\n",
       "      <th>other_exit</th>\n",
       "      <th>current_name</th>\n",
       "      <th>founding_name</th>\n",
       "      <th>current_legal_form</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "      <th>canton_id</th>\n",
       "      <th>district_id</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>typology_9c</th>\n",
       "      <th>typology_25c</th>\n",
       "      <th>capital_chf</th>\n",
       "      <th>company_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1251325</td>\n",
       "      <td>CHE153193257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mit Urteil des Gerichtspräsidenten des Zivil...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Arlez Carrosserie GmbH in Liquidation</td>\n",
       "      <td>Arlez Carrosserie GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.460137</td>\n",
       "      <td>7.861180</td>\n",
       "      <td>6296.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>http://www.arlez-carrosserie.ch/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1251326</td>\n",
       "      <td>CHE392024369</td>\n",
       "      <td>2020-11-11</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Vista Coaching GmbH in Liquidation</td>\n",
       "      <td>Vista Coaching GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.504062</td>\n",
       "      <td>7.724207</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>http://vista-coaching.ch/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1251327</td>\n",
       "      <td>CHE473646370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.523075</td>\n",
       "      <td>7.845789</td>\n",
       "      <td>941.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>no website available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1251328</td>\n",
       "      <td>CHE205344235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wolf Regio GmbH</td>\n",
       "      <td>Wolf Regio GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.518085</td>\n",
       "      <td>7.603328</td>\n",
       "      <td>12304.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>http://wolfregio.ch/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1251329</td>\n",
       "      <td>CHE190527339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mit Entscheid vom 27.09.2022 , 9.15 Uhr , ha...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>AHAS GmbH in Liquidation</td>\n",
       "      <td>AHAS GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.036300</td>\n",
       "      <td>8.177812</td>\n",
       "      <td>7771.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>no website available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ehraid           uid delete_date  \\\n",
       "0  1251325  CHE153193257         NaN   \n",
       "1  1251326  CHE392024369  2020-11-11   \n",
       "2  1251327  CHE473646370         NaN   \n",
       "3  1251328  CHE205344235         NaN   \n",
       "4  1251329  CHE190527339         NaN   \n",
       "\n",
       "                              reason_for_dissolution liquidation bankruptcy  \\\n",
       "0  ['Mit Urteil des Gerichtspräsidenten des Zivil...        True      False   \n",
       "1                                                 []       False      False   \n",
       "2                                                NaN         NaN        NaN   \n",
       "3                                                NaN         NaN        NaN   \n",
       "4  ['Mit Entscheid vom 27.09.2022 , 9.15 Uhr , ha...       False       True   \n",
       "\n",
       "  other_exit                           current_name            founding_name  \\\n",
       "0      False  Arlez Carrosserie GmbH in Liquidation   Arlez Carrosserie GmbH   \n",
       "1       True     Vista Coaching GmbH in Liquidation      Vista Coaching GmbH   \n",
       "2        NaN                Wissler Consulting GmbH  Wissler Consulting GmbH   \n",
       "3        NaN                        Wolf Regio GmbH          Wolf Regio GmbH   \n",
       "4      False               AHAS GmbH in Liquidation                AHAS GmbH   \n",
       "\n",
       "   current_legal_form  ...   latitude longitude population canton_id  \\\n",
       "0                   4  ...  47.460137  7.861180     6296.0      13.0   \n",
       "1                   4  ...  47.504062  7.724207     4700.0      13.0   \n",
       "2                   4  ...  47.523075  7.845789      941.0      13.0   \n",
       "3                   4  ...  47.518085  7.603328    12304.0      13.0   \n",
       "4                   4  ...  47.036300  8.177812     7771.0       3.0   \n",
       "\n",
       "  district_id  urban_rural typology_9c typology_25c capital_chf  \\\n",
       "0      1304.0          2.0        21.0        217.0     20000.0   \n",
       "1      1303.0          1.0        11.0        113.0     20000.0   \n",
       "2      1304.0          3.0        23.0        236.0     20000.0   \n",
       "3      1301.0          1.0        11.0        112.0     20000.0   \n",
       "4       312.0          2.0        21.0        216.0     20000.0   \n",
       "\n",
       "                        company_url  \n",
       "0  http://www.arlez-carrosserie.ch/  \n",
       "1         http://vista-coaching.ch/  \n",
       "2              no website available  \n",
       "3              http://wolfregio.ch/  \n",
       "4              no website available  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD WEBSITE URLS TO COMPANY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv(RAW_DATA_DIR / 'company_urls' / 'urls.csv')\n",
    "websites['ehraid'] = websites['ehraid'].astype(int)\n",
    "\n",
    "assert websites[websites.duplicated(subset='ehraid', keep=False)].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups = df_startups.merge(websites[['ehraid', 'company_url']], on='ehraid', how='left')\n",
    "print(f'Percentage of companies with found website: {len(df_startups[df_startups.company_url != 'no website available']) / len(df_startups) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[df_startups.company_url != 'no website available'].to_csv(RAW_DATA_DIR / 'company_sample' / 'company_sample_website.csv', index=False)\n",
    "df_startups[df_startups.company_url == 'no website available'].to_csv(RAW_DATA_DIR / 'company_sample' / 'company_sample_no_website.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_startups.shape[0]\n",
    "num_website = df_startups[(df_startups.company_url != 'no website available')].shape[0]\n",
    "num_no_website = df_startups[(df_startups.company_url == 'no website available')].shape[0]\n",
    "\n",
    "num_exits = df_startups[(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_exits_website = df_startups[(df_startups.company_url != 'no website available') & (df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_exits_no_website = df_startups[(df_startups.company_url == 'no website available') & (df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_survival = df_startups[~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_survival_website = df_startups[(df_startups.company_url != 'no website available') & ~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_survival_no_website = df_startups[(df_startups.company_url == 'no website available') & ~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "print(f'Percentage of firms having website: {num_website / total:.4f}')\n",
    "print(f'Percentage of exited firms having website: {num_exits_website / num_exits:.4f}')\n",
    "print(f'Percentage of survived firms having website: {num_survival_website / num_survival:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ratio of survival to exit: {num_survival / num_exits:.2f}')\n",
    "print(f'Ratio of survival with website to exit with website: {num_survival_website / num_exits_website:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)]\n",
    "df_startups[(df_startups.company_url != 'no website available') & ~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)]\n",
    "df_startups[(df_startups.company_url != 'no website available') & (df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FEATURE ENCODING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups = pd.read_csv(RAW_DATA_DIR / 'company_sample' / 'full_company_sample.csv')\n",
    "df_startups['founding_date'] = pd.to_datetime(df_startups['founding_date'])\n",
    "df_startups['dissolution_date'] = pd.to_datetime(df_startups['dissolution_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[(df_startups['founding_date'] <= '2022-04-01') & (df_startups['company_url'] != 'no website available') & (df_startups['company_url'].notnull())].to_csv(RAW_DATA_DIR / 'company_sample' / 'sample_2022-04-01_website.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0001"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(-1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ehraid', 'uid', 'delete_date', 'dissolution_date',\n",
       "       'reason_for_dissolution', 'liquidation', 'bankruptcy', 'other_exit',\n",
       "       'current_name', 'founding_name', 'current_legal_form',\n",
       "       'founding_legal_form', 'current_purpose', 'founding_purpose',\n",
       "       'current_street', 'current_town', 'current_zip_code', 'current_country',\n",
       "       'founding_street', 'founding_town', 'founding_zip_code',\n",
       "       'founding_bfs_code', 'shab_id', 'founding_date', 'founding_message',\n",
       "       'combined_address', 'geocoded_address', 'latitude', 'longitude',\n",
       "       'population', 'canton_id', 'district_id', 'urban_rural', 'typology_9c',\n",
       "       'typology_25c', 'capital_chf', 'company_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE BASIC FIRM CHARACTERISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantiles(df: pd.DataFrame, column: str, q: list[float] = [.2, .4, .6, .8]) -> int:\n",
    "    quantiles = df[column].quantile(q)\n",
    "    def get_numeric_quintile(target: int, quantiles: list[float]):\n",
    "        for i, tau in enumerate(quantiles, start=1):\n",
    "            if target <= tau:\n",
    "                return i\n",
    "        return len(quantiles) + 1\n",
    "    df[f'{column}_quantiles_{len(quantiles) + 1}'] = df[column].apply(lambda x: get_numeric_quintile(x, quantiles))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encode legal form as dummies (1-N dummies)\n",
    "df_startups = pd.concat([df_startups, pd.get_dummies(df_startups['founding_legal_form'], prefix='legal_form_binary', drop_first=True, dtype=int)], axis=1)\n",
    "\n",
    "# 2. Encode NOGA sectors\n",
    "pass\n",
    "\n",
    "# 3. Encode typology_25c as dummies\n",
    "df_startups = pd.concat([df_startups, pd.get_dummies(df_startups['typology_25c'], prefix='typology_25c', drop_first=True, dtype=int)], axis=1)\n",
    "\n",
    "# 4. Encode typology_9c as dummies\n",
    "df_startups = pd.concat([df_startups, pd.get_dummies(df_startups['typology_9c'], prefix='typology_9c', drop_first=True, dtype=int)], axis=1)\n",
    "\n",
    "# 5. Encode urban-rural as dummies\n",
    "df_startups = pd.concat([df_startups, pd.get_dummies(df_startups['urban_rural'], prefix='urban_rural', drop_first=True, dtype=int)], axis=1)\n",
    "\n",
    "# 6. Encode population quintiles\n",
    "# df_startups = get_quantiles(df_startups, 'population')\n",
    "# df_startups = pd.concat([df_startups, pd.get_dummies(df_startups['population_quantiles_5'], prefix='population_quintiles', drop_first=True, dtype=int)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE ADDRESS FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_spatial_features(df, lat_col='latitude', lon_col='longitude', founding_col='founding_date', dissolution_col='dissolution_date'):\n",
    "    # Ensure date columns are datetime\n",
    "    df = df.copy()\n",
    "    df[founding_col] = pd.to_datetime(df[founding_col])\n",
    "    df[dissolution_col] = pd.to_datetime(df[dissolution_col])\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    sindex = gdf.sindex\n",
    "\n",
    "    same_address_counts = []\n",
    "    firms_within_1km = []\n",
    "    firms_within_2_5km = []\n",
    "    firms_within_10km = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        founding_date_i = row[founding_col]\n",
    "\n",
    "        # -- Same address (within 10m)\n",
    "        for radius, store in [\n",
    "            (10, same_address_counts),\n",
    "            (1000, firms_within_1km),\n",
    "            (2500, firms_within_2_5km),\n",
    "            (10_000, firms_within_10km)\n",
    "        ]:\n",
    "            possible_matches_idx = list(sindex.intersection(row.geometry.buffer(radius).bounds))\n",
    "            nearby = gdf.iloc[possible_matches_idx]\n",
    "            candidates = nearby[\n",
    "                (nearby[founding_col] <= founding_date_i) &\n",
    "                (\n",
    "                    nearby[dissolution_col].isna() |\n",
    "                    (nearby[dissolution_col] > founding_date_i)\n",
    "                )\n",
    "            ]\n",
    "            n_firms = candidates[candidates.geometry.distance(row.geometry) <= radius].shape[0] - 1  # exclude self\n",
    "            store.append(n_firms)\n",
    "\n",
    "    gdf['n_firms_same_address'] = same_address_counts\n",
    "    gdf['n_firms_within_1km'] = firms_within_1km\n",
    "    gdf['n_firms_within_2.5km'] = firms_within_2_5km\n",
    "    gdf['n_firms_within_10km'] = firms_within_10km\n",
    "\n",
    "    return gdf.drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two spacial variables for\n",
    "# 1. Number of firms (including the firm itself) at the same location (within 10 meters)\n",
    "# 2. Number of firms (including the firm itself) within 1km distance\n",
    "# Both are calculated only considering existing firms at the time of founding!\n",
    "\n",
    "df_startups = encode_spatial_features(df_startups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE FIRM NAME FEATUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "token_classifier = pipeline(\n",
    "  model=\"ZurichNLP/swissbert-ner\",\n",
    "  aggregation_strategy=\"simple\",\n",
    "  device='mps'\n",
    ")\n",
    "\n",
    "UMLAUT_REPLACEMENTS = {\n",
    "    'ä': 'ae',\n",
    "    'ö': 'oe',\n",
    "    'ü': 'ue',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_words(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    for char, replacement in UMLAUT_REPLACEMENTS.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    return unidecode(text.lower())\n",
    "\n",
    "def get_language(text: str) -> str:\n",
    "    language = detect(text)\n",
    "    return language.get('lang', 'de')\n",
    "\n",
    "def stem_text(text: str, lang_code: str) -> str:\n",
    "    code2lang = {\n",
    "        'de': 'german',\n",
    "        'en': 'english',\n",
    "        'fr': 'french',\n",
    "        'it': 'italian'\n",
    "    }\n",
    "    language = code2lang.get(lang_code, 'german')\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    return [porter_stemmer.stem(w) for w in tokens if not w.lower() in stop_words]\n",
    "\n",
    "def batch_ner_tag_bps(texts: list[str], token_classifier: pipeline):\n",
    "    batch_outputs = token_classifier(texts, batch_size=32)\n",
    "    results = []\n",
    "    for output in batch_outputs:\n",
    "        people = [entry['word'] for entry in output if entry['entity_group'] == 'PER']\n",
    "        locations = [entry['word'] for entry in output if entry['entity_group'] == 'LOC']\n",
    "        results.append((people, locations))\n",
    "    return results\n",
    "\n",
    "def process_df_with_ner(df_lang: pd.DataFrame, token_classifier: pipeline, batch_size: int):\n",
    "    texts = df_lang['founding_purpose'].fillna(\"\").tolist()\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        results.extend(batch_ner_tag_bps(batch, token_classifier))\n",
    "\n",
    "    df_lang[['people', 'locations']] = pd.DataFrame(results, index=df_lang.index)\n",
    "    return df_lang\n",
    "\n",
    "def contains_male_or_female_name(names, gendered_first_names):\n",
    "    for name in [n.split() for n in names]:\n",
    "        gender = name in gendered_first_names\n",
    "        if gender:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raumgliederungen = pd.read_excel(EXTERNAL_DATA_DIR / 'geo_data' / 'Raumgliederungen.xlsx')\n",
    "municipality_names = [re.sub(r'\\(.*?\\)', '', normalize_words(name)).strip() for name in raumgliederungen['Gemeindename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    gendered_names = read_from_database(con, \"SELECT * FROM zefix.founders_gender_mapping WHERE gender != 'u' AND request_type = 'first_name' AND probability >= 0.95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_names['split_names'] = gendered_names['name'].str.split()\n",
    "\n",
    "certain_gender = gendered_names[gendered_names['split_names'].apply(len) == 1]\n",
    "certain_gender = certain_gender[certain_gender['name'].str.isalpha() & (certain_gender['name'].str.len() >= 4)].copy()\n",
    "\n",
    "female_names = set(certain_gender[certain_gender['gender'] == 'f']['name'].apply(normalize_words))\n",
    "male_names = set(certain_gender[certain_gender['gender'] == 'm']['name'].apply(normalize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get external data to create firm name and BPS features (e.g. includes female name)\n",
    "official_male_names = pd.read_csv(EXTERNAL_DATA_DIR / 'newborn_names' / 'maennliche_vornamen.csv', encoding='ISO-8859-15', usecols=['Vorname'])\n",
    "official_female_names = pd.read_csv(EXTERNAL_DATA_DIR / 'newborn_names' / 'weibliche_vornamen.csv', encoding='ISO-8859-15', usecols=['Vorname'])\n",
    "\n",
    "official_male_names = set([normalize_words(name) for name in official_male_names['Vorname']])\n",
    "official_female_names = set([normalize_words(name) for name in official_female_names['Vorname']])\n",
    "\n",
    "female_names = female_names.union(official_female_names)\n",
    "male_names = male_names.union(official_male_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7198\n",
      "10433\n"
     ]
    }
   ],
   "source": [
    "print(len(female_names))\n",
    "print(len(male_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Firm name length\n",
    "df_startups['firm_name_length'] = df_startups['founding_name'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['founding_name_norm'] = df_startups['founding_name'].apply(normalize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Contains swiss reference\n",
    "swiss_terms = [\n",
    "    \"switzerland\",\n",
    "    \"swiss\", \n",
    "    \"schweiz\",  # Covers \"schweiz\", \"schweizer\", \"schweizerische\", etc.\n",
    "    \"swi\",  # e.g. swica\n",
    "    \"sui\",\n",
    "    \"suisse\",\n",
    "    \"helvet\",  # Covers Helvetia, Helvetica, etc.\n",
    "    \"confed\",  # Covers Confederation, \"confédération\", \"confederazione\"\n",
    "    \"sviz\",  # Covers \"Svizzera\", \"Svizzero\", \"Svizzere\", \"Svizzeri\", \"svizra\", etc.\n",
    "    \"eidgen\",  # Covers \"eidgenossenschaft\", \"eidgenössisch\",\n",
    "]\n",
    "\n",
    "df_startups['founding_name_norm'] = df_startups['founding_name'].apply(normalize_words)\n",
    "df_startups['firm_name_swiss_ref'] = df_startups['founding_name_norm'].str.contains('|'.join(swiss_terms)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Contains holding reference\n",
    "holding_terms = [\n",
    "    \"holding\",\n",
    "    \"beteiligung\",\n",
    "    \"participation\",\n",
    "    \"partecipazion\",\n",
    "    \"anteil\",\n",
    "    \"capital\",\n",
    "    \"kapital\",\n",
    "    \"invest\",\n",
    "    \"share\",\n",
    "    \"aktie\",\n",
    "    \"action\",\n",
    "    \"azion\"\n",
    "]\n",
    "\n",
    "df_startups['founding_name_norm'] = df_startups['founding_name'].apply(normalize_words)\n",
    "df_startups['firm_name_swiss_ref'] = df_startups['founding_name_norm'].str.contains('|'.join(swiss_terms)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Contains geographic term (municipality name)\n",
    "def has_geographic_term(firm_name, geographic_terms):\n",
    "    names = [n for n in firm_name.split() if len(n) > 2]\n",
    "    if any(name in geographic_terms for name in names):\n",
    "        return 1\n",
    "    return 0\n",
    "    \n",
    "df_startups['firm_name_geog_ref'] = df_startups['founding_name_norm'].apply(lambda x: has_geographic_term(x, municipality_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "      <th>delete_date</th>\n",
       "      <th>dissolution_date</th>\n",
       "      <th>reason_for_dissolution</th>\n",
       "      <th>liquidation</th>\n",
       "      <th>bankruptcy</th>\n",
       "      <th>other_exit</th>\n",
       "      <th>current_name</th>\n",
       "      <th>founding_name</th>\n",
       "      <th>...</th>\n",
       "      <th>population_quintiles_5</th>\n",
       "      <th>n_firms_same_address</th>\n",
       "      <th>n_firms_within_1km</th>\n",
       "      <th>n_firms_within_2.5km</th>\n",
       "      <th>n_firms_within_10km</th>\n",
       "      <th>firm_name_length</th>\n",
       "      <th>founding_name_norm</th>\n",
       "      <th>firm_name_swiss_ref</th>\n",
       "      <th>firm_name_geog_ref</th>\n",
       "      <th>firm_name_founder_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1251327</td>\n",
       "      <td>CHE473646370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>wissler consulting gmbh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1251329</td>\n",
       "      <td>CHE190527339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>['Mit Entscheid vom 27.09.2022 , 9.15 Uhr , ha...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>AHAS GmbH in Liquidation</td>\n",
       "      <td>AHAS GmbH</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>ahas gmbh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1251336</td>\n",
       "      <td>CHE350451441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roof &amp; Terrace AG</td>\n",
       "      <td>Roof &amp; Terrace AG</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>roof &amp; terrace ag</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1251338</td>\n",
       "      <td>CHE497156719</td>\n",
       "      <td>2024-03-22</td>\n",
       "      <td>2024-03-22</td>\n",
       "      <td>['Aktiven und Passiven ( Fremdkapital ) gehen ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>SP Ventures GmbH</td>\n",
       "      <td>SP Ventures GmbH</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>sp ventures gmbh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1251341</td>\n",
       "      <td>CHE256940465</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Food Bus, Yanev &amp; Co</td>\n",
       "      <td>The Food Bus , Yanev &amp; Co</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>the food bus , yanev &amp; co</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ehraid           uid delete_date dissolution_date  \\\n",
       "0  1251327  CHE473646370         NaN              NaT   \n",
       "1  1251329  CHE190527339         NaN       2022-10-03   \n",
       "2  1251336  CHE350451441         NaN              NaT   \n",
       "3  1251338  CHE497156719  2024-03-22       2024-03-22   \n",
       "4  1251341  CHE256940465         NaN              NaT   \n",
       "\n",
       "                              reason_for_dissolution liquidation bankruptcy  \\\n",
       "0                                                NaN         NaN        NaN   \n",
       "1  ['Mit Entscheid vom 27.09.2022 , 9.15 Uhr , ha...       False       True   \n",
       "2                                                NaN         NaN        NaN   \n",
       "3  ['Aktiven und Passiven ( Fremdkapital ) gehen ...       False      False   \n",
       "4                                                NaN         NaN        NaN   \n",
       "\n",
       "  other_exit              current_name              founding_name  ...  \\\n",
       "0        NaN   Wissler Consulting GmbH    Wissler Consulting GmbH  ...   \n",
       "1      False  AHAS GmbH in Liquidation                  AHAS GmbH  ...   \n",
       "2        NaN         Roof & Terrace AG          Roof & Terrace AG  ...   \n",
       "3       True          SP Ventures GmbH           SP Ventures GmbH  ...   \n",
       "4        NaN  The Food Bus, Yanev & Co  The Food Bus , Yanev & Co  ...   \n",
       "\n",
       "   population_quintiles_5  n_firms_same_address n_firms_within_1km  \\\n",
       "0                       0                     0                  0   \n",
       "1                       0                     0                  1   \n",
       "2                       0                     0                  2   \n",
       "3                       0                     0                  0   \n",
       "4                       0                     0                  0   \n",
       "\n",
       "  n_firms_within_2.5km n_firms_within_10km firm_name_length  \\\n",
       "0                    0                   1             23.0   \n",
       "1                    1                   1              9.0   \n",
       "2                    3                   3             17.0   \n",
       "3                    3                   3             16.0   \n",
       "4                    0                   0             25.0   \n",
       "\n",
       "          founding_name_norm firm_name_swiss_ref firm_name_geog_ref  \\\n",
       "0    wissler consulting gmbh                   0                  0   \n",
       "1                  ahas gmbh                   0                  0   \n",
       "2          roof & terrace ag                   0                  0   \n",
       "3           sp ventures gmbh                   0                  0   \n",
       "4  the food bus , yanev & co                   0                  0   \n",
       "\n",
       "  firm_name_founder_match  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Contains founder names\n",
    "def has_founder_name(firm_name, founders):\n",
    "    if founders:\n",
    "        founder_name_list = []\n",
    "        for fn, ln in founders.get('founder_names', []):\n",
    "            fn = [n for n in re.split(r'[- ]', fn) if len(n) > 2]  # Avoid really short names to decrease likelihood of false positive\n",
    "            ln = [n for n in re.split(r'[- ]', ln) if len(n) > 2]\n",
    "            founder_name_list.extend(fn + ln)\n",
    "        return int(any(name in firm_name for name in founder_name_list))\n",
    "    return 0\n",
    "\n",
    "df_startups['firm_name_founder_match'] = df_startups.apply(lambda row: has_founder_name(row['founding_name_norm'], founder_lists.get(row['ehraid'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Contains gendered name\n",
    "def contains_male_or_female_name(names: list[str] | str, gendered_first_names):\n",
    "    if isinstance(names, list):\n",
    "        names = [n for name in names for n in name.split() if len(n) > 2]\n",
    "    elif isinstance(names, str):\n",
    "        names = names.split()\n",
    "    for name in names:\n",
    "        gender = name in gendered_first_names\n",
    "        if gender:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "df_startups['firm_name_male_match'] = df_startups['founding_name_norm'].apply(lambda x: contains_male_or_female_name(x, male_names))\n",
    "df_startups['firm_name_female_match'] = df_startups['founding_name_norm'].apply(lambda x: contains_male_or_female_name(x, female_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE BPS FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups.loc[df_startups.founding_purpose.isna(), 'founding_purpose'] = df_startups['current_purpose']  # Use current purpose if founding purpose is missing\n",
    "df_startups['bps_language'] = df_startups['founding_purpose'].apply(get_language)\n",
    "\n",
    "corrections = {\n",
    "    'cs': 'de',\n",
    "    'en': 'de',\n",
    "    'sv': 'de',\n",
    "    'es': 'fr',\n",
    "    'pt': 'de',\n",
    "    'pl': 'de',\n",
    "    'nl': 'de',\n",
    "    'ca': 'it',\n",
    "    'et': 'de'\n",
    "}\n",
    "\n",
    "df_startups['bps_language'] = df_startups['bps_language'].replace(corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stop-word removal and stemming\n",
    "df_startups['bps_normalized'] = df_startups.apply(lambda row: stem_text(row['founding_purpose'], lang_code=row['bps_language']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encode bps length\n",
    "df_startups['bps_length'] = df_startups['bps_normalized'].apply(lambda x: len(' '.join(x)))  # bps length\n",
    "\n",
    "# 2. Encode mean word length\n",
    "df_startups['bps_mean_word_length'] = [(1 / len(word_list)) * np.array([len(w) for w in word_list]).sum() for word_list in df_startups['bps_normalized']]  # average word length per bps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get quantiles of length metrics\n",
    "df_startups = get_quantiles(df_startups, 'bps_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate LIX\n",
    "def calculate_lix(word_list: list[str]):\n",
    "    pct_above_six = len([w for w in word_list if len(w) > 6])  / len(word_list) * 100\n",
    "    return len(word_list) + pct_above_six\n",
    "\n",
    "df_startups['bps_lix'] = df_startups['bps_normalized'].apply(calculate_lix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Word-frequency Features\n",
    "word_dictionary_de = [token for sublist in df_startups[df_startups['bps_language'] == 'de']['bps_normalized'] for token in sublist if token.isalpha() and len(token) > 4]\n",
    "word_freqencies_de = Counter(word_dictionary_de)\n",
    "\n",
    "word_dictionary_fr = [token for sublist in df_startups[df_startups['bps_language'] == 'fr']['bps_normalized'] for token in sublist if token.isalpha() and len(token) > 4]\n",
    "word_freqencies_fr = Counter(word_dictionary_fr)\n",
    "\n",
    "word_dictionary_it = [token for sublist in df_startups[df_startups['bps_language'] == 'it']['bps_normalized'] for token in sublist if token.isalpha() and len(token) > 4]\n",
    "word_freqencies_it = Counter(word_dictionary_it)\n",
    "\n",
    "def compute_specificity_features(bps_tokens: list[str], lang_code: str) -> tuple[int, int, float]:\n",
    "    if not isinstance(bps_tokens, list):\n",
    "        print(bps_tokens)\n",
    "    if lang_code == 'fr':\n",
    "        word_freqencies = word_freqencies_fr\n",
    "    elif lang_code == 'it':\n",
    "        word_freqencies = word_freqencies_it\n",
    "    else:\n",
    "        word_freqencies = word_freqencies_de\n",
    "    token_freqs = [word_freqencies[token] for token in bps_tokens if token in word_freqencies]\n",
    "    if not token_freqs:\n",
    "        return (0, 0, 0.0)\n",
    "\n",
    "    total_tokens = sum(word_freqencies.values())\n",
    "    # Normalize it by total number of tokens to account for differences between languages\n",
    "    min_freq_norm, max_freq_norm = min(token_freqs) / total_tokens, max(token_freqs) / total_tokens\n",
    "\n",
    "    ratio = min_freq_norm / max_freq_norm if max_freq_norm > 0 else 0.0\n",
    "    return (min_freq_norm, max_freq_norm, ratio)\n",
    "\n",
    "df_startups[['bps_min_word_freq_norm', 'bps_max_word_freq_norm', 'bps_freq_ratio_norm']] = df_startups.apply(lambda row: compute_specificity_features(row['bps_normalized'], row['bps_language']), axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1191/1191 [29:19<00:00,  1.48s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[319]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mhas_location\u001b[39m\u001b[33m'\u001b[39m] = df_lang[\u001b[33m'\u001b[39m\u001b[33mlocations\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x) > \u001b[32m0\u001b[39m))\n\u001b[32m     29\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m] = df_lang[\u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m names: [normalize_words(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mhas_male_name\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_lang\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpeople\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontains_male_or_female_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmale_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mhas_female_name\u001b[39m\u001b[33m'\u001b[39m] = df_lang[\u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: contains_male_or_female_name(x, female_names))\n\u001b[32m     33\u001b[39m dfs.append(df_lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/for_work/company_success_prediction/.venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/for_work/company_success_prediction/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/for_work/company_success_prediction/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/for_work/company_success_prediction/.venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/for_work/company_success_prediction/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[319]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     28\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mhas_location\u001b[39m\u001b[33m'\u001b[39m] = df_lang[\u001b[33m'\u001b[39m\u001b[33mlocations\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x) > \u001b[32m0\u001b[39m))\n\u001b[32m     29\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m] = df_lang[\u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m names: [normalize_words(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mhas_male_name\u001b[39m\u001b[33m'\u001b[39m] = df_lang[\u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mcontains_male_or_female_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmale_names\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     31\u001b[39m df_lang[\u001b[33m'\u001b[39m\u001b[33mhas_female_name\u001b[39m\u001b[33m'\u001b[39m] = df_lang[\u001b[33m'\u001b[39m\u001b[33mpeople\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: contains_male_or_female_name(x, female_names))\n\u001b[32m     33\u001b[39m dfs.append(df_lang)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[288]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcontains_male_or_female_name\u001b[39m\u001b[34m(names, gendered_first_names)\u001b[39m\n\u001b[32m      6\u001b[39m     names = names.split()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     gender = \u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgendered_first_names\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gender:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# 5. Encode geographic and name features\n",
    "dfs = []\n",
    "for language in ['de', 'fr', 'it']:\n",
    "    token_classifier.model.set_default_language(f\"{language}_CH\")\n",
    "    df_lang = df_startups[df_startups['bps_language'] == language].copy()\n",
    "    df_lang = process_df_with_ner(df_lang, token_classifier, batch_size=128)\n",
    "\n",
    "    df_lang['has_location'] = df_lang['locations'].apply(lambda x: int(len(x) > 0))\n",
    "    df_lang['people'] = df_lang['people'].apply(lambda names: [normalize_words(name) for name in names])\n",
    "    df_lang['has_male_name'] = df_lang['people'].apply(lambda x: contains_male_or_female_name(x, male_names))\n",
    "    df_lang['has_female_name'] = df_lang['people'].apply(lambda x: contains_male_or_female_name(x, female_names))\n",
    "\n",
    "    dfs.append(df_lang)\n",
    "\n",
    "df_startups = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE FOUNDER FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/6qys5jn501gbqmkkkgmvpf0h0000gn/T/ipykernel_69153/3770396621.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_startups['n_inscribed_firms'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. Encode number of founders\n",
    "df_startups['n_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_founders', 1))\n",
    "df_startups.loc[df_startups['n_founders'] == 0, 'n_founders'] = 1  # Firm must have at least one founder\n",
    "\n",
    "# 2. Encode number of inscribed firms\n",
    "df_startups = df_startups.merge(grouped_insc_firms, on='ehraid', how='left')\n",
    "df_startups['founding_date'] = pd.to_datetime(df_startups['founding_date'])\n",
    "df_startups['firm_inscription_date'] = pd.to_datetime(df_startups['founding_date'])\n",
    "\n",
    "df_startups.loc[df_startups['firm_inscription_date'] > df_startups['founding_date'], 'n_inscribed_firms'] = 0\n",
    "df_startups.drop(columns=['firm_inscription_date'], inplace=True)\n",
    "df_startups['n_inscribed_firms'].fillna(0, inplace=True)\n",
    "\n",
    "# 3. Percentage of female founders\n",
    "df_startups['n_female_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_female_founders', np.nan))\n",
    "df_startups['pct_female_founders'] = df_startups['n_female_founders'] / df_startups['n_founders']\n",
    "\n",
    "# 4. Number of distinct nationalities\n",
    "def get_distinct_nationalities(founder_data):\n",
    "    if not founder_data:\n",
    "        return 0\n",
    "    return len(set([nat for nationalities in founder_data.get('founder_nationalities', []) for nat in nationalities]))\n",
    "\n",
    "df_startups['n_distinct_nationalities'] = df_startups['ehraid'].apply(lambda x: get_distinct_nationalities(founder_lists.get('ehraid')))\n",
    "\n",
    "# 5. Number of Swiss founders\n",
    "df_startups['n_swiss_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_swiss_founders', np.nan))\n",
    "\n",
    "# 6. Number of foreign founders\n",
    "df_startups['n_foreign_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_foreign_founders', np.nan))\n",
    "\n",
    "# 7. Number of founders with Dr. PhD. Prof. in name\n",
    "df_startups['n_dr_titles'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_dr_titles', np.nan))\n",
    "\n",
    "# 8. Founders with same municipality than firm\n",
    "def get_distinct_nationalities(firm_bfs_code, founder_data):\n",
    "    if not founder_data:\n",
    "        return 0\n",
    "    n_founders_with_same_bfs_code = 0\n",
    "    for founder_residencies in founder_data.get('founder_residencies', []):\n",
    "        n_founders_with_same_bfs_code += int(firm_bfs_code in founder_residencies)\n",
    "    return n_founders_with_same_bfs_code\n",
    "\n",
    "df_startups['n_founders_same_residence'] = df_startups.apply(lambda row: get_distinct_nationalities(row['founding_bfs_code'], founder_lists.get(row['ehraid'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226553/226553 [09:19<00:00, 405.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# 9. Encode prior founding experience\n",
    "df_startups['founder_fids'] = df_startups['ehraid'].apply(lambda x: founder_lists.get(x, {}).get('founder_fids', []))\n",
    "\n",
    "prior_foundings = {}\n",
    "for ehraid, data in founder_lists.items():\n",
    "    founders = data.get('founder_fids', [])\n",
    "    for fid in founders:\n",
    "        if fid not in prior_foundings:\n",
    "            prior_foundings[fid] = [ehraid]\n",
    "        else:\n",
    "            prior_foundings[fid].append(ehraid)\n",
    "\n",
    "prior_founding = []\n",
    "prior_failed = []\n",
    "prior_existing = []\n",
    "for i, row in tqdm(df_startups.iterrows(), total=df_startups.shape[0]):\n",
    "    foundings = 0\n",
    "    failed = set()\n",
    "    existing = set()\n",
    "    for fid in row['founder_fids']:\n",
    "        matches = df_startups[\n",
    "            df_startups['ehraid'].isin(prior_foundings[fid])\n",
    "            & (df_startups['ehraid'] != row['ehraid'])\n",
    "            & (df_startups['founding_date'] <= row['founding_date'])]\n",
    "        if matches.empty:\n",
    "            continue\n",
    "        foundings += 1\n",
    "        failed_ehraids = matches[matches['dissolution_date'] < row['founding_date']]['ehraid'].tolist()\n",
    "        for eid in failed_ehraids:\n",
    "            failed.add(eid)\n",
    "        existing_ehraids = [ehraid for ehraid in matches['ehraid'] if ehraid not in failed_ehraids]\n",
    "        for eid in existing_ehraids:\n",
    "            existing.add(eid)\n",
    "\n",
    "    prior_founding.append(foundings)\n",
    "    prior_failed.append(len(failed))\n",
    "    prior_existing.append(len(existing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['n_founders_with_experience'] = prior_founding\n",
    "df_startups['pct_founders_with_experience'] = df_startups['n_founders_with_experience'] / df_startups['n_founders']\n",
    "df_startups['prior_failed'] = prior_failed\n",
    "df_startups['prior_existing'] = prior_existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ehraid', 'uid', 'delete_date', 'dissolution_date',\n",
       "       'reason_for_dissolution', 'liquidation', 'bankruptcy', 'other_exit',\n",
       "       'current_name', 'founding_name', 'current_legal_form',\n",
       "       'founding_legal_form', 'current_purpose', 'founding_purpose',\n",
       "       'current_street', 'current_town', 'current_zip_code', 'current_country',\n",
       "       'founding_street', 'founding_town', 'founding_zip_code',\n",
       "       'founding_bfs_code', 'shab_id', 'founding_date', 'founding_message',\n",
       "       'combined_address', 'geocoded_address', 'latitude', 'longitude',\n",
       "       'population', 'canton_id', 'district_id', 'urban_rural', 'typology_9c',\n",
       "       'typology_25c', 'capital_chf', 'company_url', 'legal_form_binary_3.0',\n",
       "       'legal_form_binary_4.0', 'legal_form_binary_5.0',\n",
       "       'legal_form_binary_6.0', 'legal_form_binary_8.0',\n",
       "       'legal_form_binary_10.0', 'legal_form_binary_12.0',\n",
       "       'legal_form_binary_13.0', 'legal_form_binary_15.0',\n",
       "       'legal_form_binary_16.0', 'legal_form_binary_17.0',\n",
       "       'legal_form_binary_19.0', 'typology_9c_12.0', 'typology_9c_13.0',\n",
       "       'typology_9c_21.0', 'typology_9c_22.0', 'typology_9c_23.0',\n",
       "       'typology_9c_31.0', 'typology_9c_32.0', 'typology_9c_33.0',\n",
       "       'urban_rural_2.0', 'urban_rural_3.0', 'population_quantiles_5',\n",
       "       'population_quintiles_2', 'population_quintiles_3',\n",
       "       'population_quintiles_4', 'population_quintiles_5',\n",
       "       'n_firms_same_address', 'n_firms_within_1km', 'n_firms_within_2.5km',\n",
       "       'n_firms_within_10km', 'firm_name_length', 'founding_name_norm',\n",
       "       'firm_name_swiss_ref', 'firm_name_geog_ref', 'firm_name_founder_match',\n",
       "       'firm_name_male_match', 'firm_name_female_match', 'bps_language',\n",
       "       'bps_normalized', 'bps_length', 'bps_mean_word_length',\n",
       "       'bps_length_quantiles_5', 'bps_lix', 'bps_min_word_freq_norm',\n",
       "       'bps_max_word_freq_norm', 'bps_freq_ratio_norm',\n",
       "       'n_founders_same_residence', 'founder_fids',\n",
       "       'n_founders_with_experience', 'n_founders', 'n_inscribed_firms',\n",
       "       'n_female_founders', 'pct_female_founders', 'n_distinct_nationalities',\n",
       "       'n_swiss_founders', 'n_foreign_founders', 'n_dr_titles',\n",
       "       'pct_founders_with_experience', 'prior_failed', 'prior_existing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2099"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups.founding_bfs_code.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD CONTROL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of days that we have a history to inform the model\n",
    "# about missing history for variables depending on the history (e.g. n-firms at address, prior founding experiance)\n",
    "df_startups['founding_date'] = pd.to_datetime(df_startups['founding_date'])\n",
    "min_date = df_startups['founding_date'].agg('min')\n",
    "df_startups['days_of_prior_observations'] = (df_startups['founding_date'] - min_date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PROCESS ADDITIONAL OUTPUT FEATURES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVOLUNTARY EXIT TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACQUISITION TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation mergers are determined statistically via a name similarity index\n",
    "# The cutoff is set to 0.7\n",
    "\n",
    "query_merger = \"\"\" \n",
    "    SELECT ehraid_acquirer, ehraid_acquiree, merger_date, name_similarity FROM zefix.merger_relation WHERE merger_date > '2016-01-01' AND name_similarity < 0.7;\n",
    "\"\"\"\n",
    "\n",
    "with connect_database() as con:\n",
    "    df_merger = read_from_database(connection=con, query=query_merger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid_acquirer</th>\n",
       "      <th>ehraid_acquiree</th>\n",
       "      <th>merger_date</th>\n",
       "      <th>name_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20661</td>\n",
       "      <td>842122</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>0.328784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20661</td>\n",
       "      <td>808336</td>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>0.321066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>186653</td>\n",
       "      <td>1376873</td>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>0.432677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519390</td>\n",
       "      <td>1329420</td>\n",
       "      <td>2023-06-23</td>\n",
       "      <td>0.337552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>672075</td>\n",
       "      <td>882324</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>0.294701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15099</th>\n",
       "      <td>402443</td>\n",
       "      <td>367910</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>0.366375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100</th>\n",
       "      <td>893657</td>\n",
       "      <td>87137</td>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>0.408320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15101</th>\n",
       "      <td>1452892</td>\n",
       "      <td>1250987</td>\n",
       "      <td>2023-04-19</td>\n",
       "      <td>0.442617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15102</th>\n",
       "      <td>133745</td>\n",
       "      <td>991716</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>0.288470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15103</th>\n",
       "      <td>415941</td>\n",
       "      <td>369120</td>\n",
       "      <td>2016-06-06</td>\n",
       "      <td>0.405934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15104 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ehraid_acquirer  ehraid_acquiree merger_date  name_similarity\n",
       "0                20661           842122  2018-07-31         0.328784\n",
       "1                20661           808336  2020-10-06         0.321066\n",
       "2               186653          1376873  2020-11-27         0.432677\n",
       "3               519390          1329420  2023-06-23         0.337552\n",
       "4               672075           882324  2024-06-27         0.294701\n",
       "...                ...              ...         ...              ...\n",
       "15099           402443           367910  2016-05-24         0.366375\n",
       "15100           893657            87137  2022-09-13         0.408320\n",
       "15101          1452892          1250987  2023-04-19         0.442617\n",
       "15102           133745           991716  2016-07-04         0.288470\n",
       "15103           415941           369120  2016-06-06         0.405934\n",
       "\n",
       "[15104 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid_acquirer</th>\n",
       "      <th>ehraid_acquiree</th>\n",
       "      <th>merger_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20661</td>\n",
       "      <td>842122</td>\n",
       "      <td>2018-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20661</td>\n",
       "      <td>808336</td>\n",
       "      <td>2020-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>186653</td>\n",
       "      <td>1376873</td>\n",
       "      <td>2020-11-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519390</td>\n",
       "      <td>1329420</td>\n",
       "      <td>2023-06-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>672075</td>\n",
       "      <td>882324</td>\n",
       "      <td>2024-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15099</th>\n",
       "      <td>402443</td>\n",
       "      <td>367910</td>\n",
       "      <td>2016-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100</th>\n",
       "      <td>893657</td>\n",
       "      <td>87137</td>\n",
       "      <td>2022-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15101</th>\n",
       "      <td>1452892</td>\n",
       "      <td>1250987</td>\n",
       "      <td>2023-04-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15102</th>\n",
       "      <td>133745</td>\n",
       "      <td>991716</td>\n",
       "      <td>2016-07-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15103</th>\n",
       "      <td>415941</td>\n",
       "      <td>369120</td>\n",
       "      <td>2016-06-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ehraid_acquirer  ehraid_acquiree merger_date\n",
       "0                20661           842122  2018-07-31\n",
       "1                20661           808336  2020-10-06\n",
       "2               186653          1376873  2020-11-27\n",
       "3               519390          1329420  2023-06-23\n",
       "4               672075           882324  2024-06-27\n",
       "...                ...              ...         ...\n",
       "15099           402443           367910  2016-05-24\n",
       "15100           893657            87137  2022-09-13\n",
       "15101          1452892          1250987  2023-04-19\n",
       "15102           133745           991716  2016-07-04\n",
       "15103           415941           369120  2016-06-06\n",
       "\n",
       "[15104 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNDING TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = pd.read_csv(PROCESSED_DATA_DIR / 'funding_data' / 'startup-ch_funding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>firm_name</th>\n",
       "      <th>amount_chf</th>\n",
       "      <th>type</th>\n",
       "      <th>location</th>\n",
       "      <th>firm_name_norm</th>\n",
       "      <th>firm_name_clean</th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "      <th>legal_seat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>Oviva AG</td>\n",
       "      <td>800'000</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Altendorf</td>\n",
       "      <td>oviva ag</td>\n",
       "      <td>oviva ag</td>\n",
       "      <td>1153456.0</td>\n",
       "      <td>CHE442270754</td>\n",
       "      <td>Altendorf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>SteriLux SA</td>\n",
       "      <td>100'000</td>\n",
       "      <td>Loan</td>\n",
       "      <td>Prilly</td>\n",
       "      <td>sterilux sa</td>\n",
       "      <td>sterilux sa</td>\n",
       "      <td>1191889.0</td>\n",
       "      <td>CHE197903656</td>\n",
       "      <td>Renens (VD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>wefox Holding AG</td>\n",
       "      <td>5'500'000</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>wefox holding ag</td>\n",
       "      <td>wefox holding ag</td>\n",
       "      <td>1201794.0</td>\n",
       "      <td>CHE375651476</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-09</td>\n",
       "      <td>rqmicro AG</td>\n",
       "      <td>3'700'000</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Schlieren</td>\n",
       "      <td>rqmicro ag</td>\n",
       "      <td>rqmicro ag</td>\n",
       "      <td>1139718.0</td>\n",
       "      <td>CHE412218826</td>\n",
       "      <td>Schlieren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>Goodwall SA</td>\n",
       "      <td>2'100'000</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Genève</td>\n",
       "      <td>goodwall sa</td>\n",
       "      <td>goodwall sa</td>\n",
       "      <td>1059251.0</td>\n",
       "      <td>CHE136959165</td>\n",
       "      <td>Genève</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>2022-03-01</td>\n",
       "      <td>pier wallet</td>\n",
       "      <td>1'000'000</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Oberwil b.Zug</td>\n",
       "      <td>pier wallet</td>\n",
       "      <td>pier wallet</td>\n",
       "      <td>1522371</td>\n",
       "      <td>CHE421267195</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>pier wallet</td>\n",
       "      <td>100'000</td>\n",
       "      <td>Grant</td>\n",
       "      <td>Oberwil b.Zug</td>\n",
       "      <td>pier wallet</td>\n",
       "      <td>pier wallet</td>\n",
       "      <td>1522371</td>\n",
       "      <td>CHE421267195</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>2018-12-19</td>\n",
       "      <td>readydata AG (immoledo)</td>\n",
       "      <td>Undisclosed</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>readydata ag</td>\n",
       "      <td>readydata ag</td>\n",
       "      <td>974688</td>\n",
       "      <td>CHE115498230</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>2020-09-17</td>\n",
       "      <td>the GO corporation (EGO Movement)</td>\n",
       "      <td>3'000'000</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Zurich</td>\n",
       "      <td>the go corporation</td>\n",
       "      <td>the go corporation</td>\n",
       "      <td>1217368</td>\n",
       "      <td>CHE439270305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>2021-09-17</td>\n",
       "      <td>the GO corporation (EGO Movement)</td>\n",
       "      <td>Undisclosed</td>\n",
       "      <td>Equity</td>\n",
       "      <td>Zurich</td>\n",
       "      <td>the go corporation</td>\n",
       "      <td>the go corporation</td>\n",
       "      <td>1217368</td>\n",
       "      <td>CHE439270305</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4511 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                          firm_name   amount_chf    type  \\\n",
       "0     2016-01-01                           Oviva AG      800'000  Equity   \n",
       "1     2016-01-01                        SteriLux SA      100'000    Loan   \n",
       "2     2016-01-01                   wefox Holding AG    5'500'000  Equity   \n",
       "3     2016-01-09                         rqmicro AG    3'700'000  Equity   \n",
       "4     2016-01-14                        Goodwall SA    2'100'000  Equity   \n",
       "...          ...                                ...          ...     ...   \n",
       "4595  2022-03-01                        pier wallet    1'000'000  Equity   \n",
       "4596  2022-02-01                        pier wallet      100'000   Grant   \n",
       "4597  2018-12-19            readydata AG (immoledo)  Undisclosed  Equity   \n",
       "4598  2020-09-17  the GO corporation (EGO Movement)    3'000'000  Equity   \n",
       "4599  2021-09-17  the GO corporation (EGO Movement)  Undisclosed  Equity   \n",
       "\n",
       "           location       firm_name_norm      firm_name_clean     ehraid  \\\n",
       "0         Altendorf             oviva ag             oviva ag  1153456.0   \n",
       "1            Prilly          sterilux sa          sterilux sa  1191889.0   \n",
       "2            Zürich     wefox holding ag     wefox holding ag  1201794.0   \n",
       "3         Schlieren           rqmicro ag           rqmicro ag  1139718.0   \n",
       "4            Genève          goodwall sa          goodwall sa  1059251.0   \n",
       "...             ...                  ...                  ...        ...   \n",
       "4595  Oberwil b.Zug          pier wallet          pier wallet    1522371   \n",
       "4596  Oberwil b.Zug          pier wallet          pier wallet    1522371   \n",
       "4597         Zürich        readydata ag         readydata ag      974688   \n",
       "4598         Zurich  the go corporation   the go corporation     1217368   \n",
       "4599         Zurich  the go corporation   the go corporation     1217368   \n",
       "\n",
       "               uid   legal_seat  \n",
       "0     CHE442270754    Altendorf  \n",
       "1     CHE197903656  Renens (VD)  \n",
       "2     CHE375651476       Zürich  \n",
       "3     CHE412218826    Schlieren  \n",
       "4     CHE136959165       Genève  \n",
       "...            ...          ...  \n",
       "4595  CHE421267195          NaN  \n",
       "4596  CHE421267195          NaN  \n",
       "4597  CHE115498230          NaN  \n",
       "4598  CHE439270305          NaN  \n",
       "4599  CHE439270305          NaN  \n",
       "\n",
       "[4511 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funding_df[~funding_df.ehraid.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INNOVATION SUBSIDY TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inno_df = pd.read_csv(PROCESSED_DATA_DIR / 'funding_data' / 'innosuisse_grants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>nabs_policy_domain</th>\n",
       "      <th>project_id</th>\n",
       "      <th>project_number</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_url</th>\n",
       "      <th>granted_total_costs</th>\n",
       "      <th>abstract</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>contact_person</th>\n",
       "      <th>scientific_management</th>\n",
       "      <th>implementation_partner</th>\n",
       "      <th>canton</th>\n",
       "      <th>city</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>street</th>\n",
       "      <th>firm_name_original</th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>36489</td>\n",
       "      <td>17743.1 PFIW-IW</td>\n",
       "      <td>Research Proposal on Experimental and Numerica...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>85500.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>[{'first_name': 'Reza', 'last_name': 'Abhari',...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'first_name': 'René', 'last_name': 'Hunziker'...</td>\n",
       "      <td>AG</td>\n",
       "      <td>Baden</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>Bruggerstrasse 71a</td>\n",
       "      <td>ABB Turbo Systems AG</td>\n",
       "      <td>329381.0</td>\n",
       "      <td>CHE101538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>37239</td>\n",
       "      <td>17856.2 PFES-ES</td>\n",
       "      <td>Green spaces toolbox for high quality developm...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>336000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>[{'first_name': 'Joachim', 'last_name': 'Schöf...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'first_name': 'Heinz', 'last_name': 'Beiner',...</td>\n",
       "      <td>ZH</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>8001.0</td>\n",
       "      <td>Obere Zäune 12</td>\n",
       "      <td>Planpartner AG</td>\n",
       "      <td>137360.0</td>\n",
       "      <td>CHE107822340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>37290</td>\n",
       "      <td>18589.1 PFIW-IW</td>\n",
       "      <td>Design and development of an industrial proces...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>458403.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>[{'first_name': 'Frédéric', 'last_name': 'Pich...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'first_name': 'Pascal', 'last_name': 'Queloz'...</td>\n",
       "      <td>NE</td>\n",
       "      <td>La Chaux-de-Fonds</td>\n",
       "      <td>2302.0</td>\n",
       "      <td>Louis-Joseph Chevrolet 46</td>\n",
       "      <td>OPAL création SA</td>\n",
       "      <td>180947.0</td>\n",
       "      <td>CHE101886525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>37295</td>\n",
       "      <td>18022.1 PFLS-LS</td>\n",
       "      <td>Structure-based drug discovery with thermostab...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>886937.00</td>\n",
       "      <td>G protein-coupled receptors (GPCRs) are the si...</td>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>[{'first_name': 'Andreas', 'last_name': 'Plück...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'first_name': 'Carlo', 'last_name': 'Bertozzi...</td>\n",
       "      <td>ZH</td>\n",
       "      <td>Schlieren</td>\n",
       "      <td>8952.0</td>\n",
       "      <td>Grabenstrasse 11a</td>\n",
       "      <td>Heptares Therapeutics Zurich AG</td>\n",
       "      <td>1146674.0</td>\n",
       "      <td>CHE488613942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>37301</td>\n",
       "      <td>18536.1 PFLS-LS</td>\n",
       "      <td>Integrated control of Colletotrichum coccodes ...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>444840.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>[{'first_name': 'Peter', 'last_name': 'Spring'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'first_name': 'Ernst', 'last_name': 'Wullschl...</td>\n",
       "      <td>BE</td>\n",
       "      <td>Bern</td>\n",
       "      <td>3007.0</td>\n",
       "      <td>Belpstrasse 26</td>\n",
       "      <td>CH Branchenorganisation der</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>56829</td>\n",
       "      <td>123.400 IP-LS</td>\n",
       "      <td>A Non-Toxic, Long Lasting Anti-Microbial Polym...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>736193.95</td>\n",
       "      <td>We target a non-toxic (no harmful elements, e....</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>2027-08-01</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>{'first_name': None, 'last_name': None, 'canto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Müri Prototech AG</td>\n",
       "      <td>908535.0</td>\n",
       "      <td>CHE114294676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>56829</td>\n",
       "      <td>123.400 IP-LS</td>\n",
       "      <td>A Non-Toxic, Long Lasting Anti-Microbial Polym...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>736193.95</td>\n",
       "      <td>We target a non-toxic (no harmful elements, e....</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>2027-08-01</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>{'first_name': None, 'last_name': None, 'canto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TopCoat GmbH</td>\n",
       "      <td>634713.0</td>\n",
       "      <td>CHE109287995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>56829</td>\n",
       "      <td>123.400 IP-LS</td>\n",
       "      <td>A Non-Toxic, Long Lasting Anti-Microbial Polym...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>736193.95</td>\n",
       "      <td>We target a non-toxic (no harmful elements, e....</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>2027-08-01</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>{'first_name': None, 'last_name': None, 'canto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMS-CHEMIE AG</td>\n",
       "      <td>52213.0</td>\n",
       "      <td>CHE105904234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>56832</td>\n",
       "      <td>120.818 IP-LS</td>\n",
       "      <td>Identification of novel prognostic and monitor...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>856312.95</td>\n",
       "      <td>Current cancer monitoring methods are often in...</td>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>2028-01-01</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>{'first_name': None, 'last_name': None, 'canto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oncobit AG</td>\n",
       "      <td>1347031.0</td>\n",
       "      <td>CHE193564727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>INNOSUISSE</td>\n",
       "      <td>Industrial production and technology</td>\n",
       "      <td>56833</td>\n",
       "      <td>116.621 IP-ICT</td>\n",
       "      <td>PolyNova: Intelligent Design for Next-Gen Mate...</td>\n",
       "      <td>https://www.aramis.admin.ch/Beteiligte/?Projec...</td>\n",
       "      <td>626867.30</td>\n",
       "      <td>This project combines EMS-CHEMIE's polymer exp...</td>\n",
       "      <td>2024-12-15</td>\n",
       "      <td>2027-04-01</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>[{'first_name': None, 'last_name': None, 'cant...</td>\n",
       "      <td>{'first_name': None, 'last_name': None, 'canto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMS-CHEMIE AG</td>\n",
       "      <td>52213.0</td>\n",
       "      <td>CHE105904234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7605 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      department                    nabs_policy_domain  project_id  \\\n",
       "0     INNOSUISSE  Industrial production and technology       36489   \n",
       "1     INNOSUISSE  Industrial production and technology       37239   \n",
       "2     INNOSUISSE  Industrial production and technology       37290   \n",
       "3     INNOSUISSE  Industrial production and technology       37295   \n",
       "4     INNOSUISSE  Industrial production and technology       37301   \n",
       "...          ...                                   ...         ...   \n",
       "7600  INNOSUISSE  Industrial production and technology       56829   \n",
       "7601  INNOSUISSE  Industrial production and technology       56829   \n",
       "7602  INNOSUISSE  Industrial production and technology       56829   \n",
       "7603  INNOSUISSE  Industrial production and technology       56832   \n",
       "7604  INNOSUISSE  Industrial production and technology       56833   \n",
       "\n",
       "       project_number                                      project_title  \\\n",
       "0     17743.1 PFIW-IW  Research Proposal on Experimental and Numerica...   \n",
       "1     17856.2 PFES-ES  Green spaces toolbox for high quality developm...   \n",
       "2     18589.1 PFIW-IW  Design and development of an industrial proces...   \n",
       "3     18022.1 PFLS-LS  Structure-based drug discovery with thermostab...   \n",
       "4     18536.1 PFLS-LS  Integrated control of Colletotrichum coccodes ...   \n",
       "...               ...                                                ...   \n",
       "7600    123.400 IP-LS  A Non-Toxic, Long Lasting Anti-Microbial Polym...   \n",
       "7601    123.400 IP-LS  A Non-Toxic, Long Lasting Anti-Microbial Polym...   \n",
       "7602    123.400 IP-LS  A Non-Toxic, Long Lasting Anti-Microbial Polym...   \n",
       "7603    120.818 IP-LS  Identification of novel prognostic and monitor...   \n",
       "7604   116.621 IP-ICT  PolyNova: Intelligent Design for Next-Gen Mate...   \n",
       "\n",
       "                                            project_url  granted_total_costs  \\\n",
       "0     https://www.aramis.admin.ch/Beteiligte/?Projec...             85500.00   \n",
       "1     https://www.aramis.admin.ch/Beteiligte/?Projec...            336000.00   \n",
       "2     https://www.aramis.admin.ch/Beteiligte/?Projec...            458403.00   \n",
       "3     https://www.aramis.admin.ch/Beteiligte/?Projec...            886937.00   \n",
       "4     https://www.aramis.admin.ch/Beteiligte/?Projec...            444840.00   \n",
       "...                                                 ...                  ...   \n",
       "7600  https://www.aramis.admin.ch/Beteiligte/?Projec...            736193.95   \n",
       "7601  https://www.aramis.admin.ch/Beteiligte/?Projec...            736193.95   \n",
       "7602  https://www.aramis.admin.ch/Beteiligte/?Projec...            736193.95   \n",
       "7603  https://www.aramis.admin.ch/Beteiligte/?Projec...            856312.95   \n",
       "7604  https://www.aramis.admin.ch/Beteiligte/?Projec...            626867.30   \n",
       "\n",
       "                                               abstract  start_date  \\\n",
       "0                                                   NaN  2016-01-01   \n",
       "1                                                   NaN  2016-02-01   \n",
       "2                                                   NaN  2016-02-01   \n",
       "3     G protein-coupled receptors (GPCRs) are the si...  2016-03-01   \n",
       "4                                                   NaN  2016-03-01   \n",
       "...                                                 ...         ...   \n",
       "7600  We target a non-toxic (no harmful elements, e....  2025-03-01   \n",
       "7601  We target a non-toxic (no harmful elements, e....  2025-03-01   \n",
       "7602  We target a non-toxic (no harmful elements, e....  2025-03-01   \n",
       "7603  Current cancer monitoring methods are often in...  2024-11-15   \n",
       "7604  This project combines EMS-CHEMIE's polymer exp...  2024-12-15   \n",
       "\n",
       "        end_date                                     contact_person  \\\n",
       "0     2017-01-01  [{'first_name': 'Reza', 'last_name': 'Abhari',...   \n",
       "1     2018-05-01  [{'first_name': 'Joachim', 'last_name': 'Schöf...   \n",
       "2     2018-11-01  [{'first_name': 'Frédéric', 'last_name': 'Pich...   \n",
       "3     2019-03-01  [{'first_name': 'Andreas', 'last_name': 'Plück...   \n",
       "4     2019-09-01  [{'first_name': 'Peter', 'last_name': 'Spring'...   \n",
       "...          ...                                                ...   \n",
       "7600  2027-08-01  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7601  2027-08-01  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7602  2027-08-01  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7603  2028-01-01  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7604  2027-04-01  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "\n",
       "                                  scientific_management  \\\n",
       "0                                                    []   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "7600  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7601  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7602  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7603  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "7604  [{'first_name': None, 'last_name': None, 'cant...   \n",
       "\n",
       "                                 implementation_partner canton  \\\n",
       "0     {'first_name': 'René', 'last_name': 'Hunziker'...     AG   \n",
       "1     {'first_name': 'Heinz', 'last_name': 'Beiner',...     ZH   \n",
       "2     {'first_name': 'Pascal', 'last_name': 'Queloz'...     NE   \n",
       "3     {'first_name': 'Carlo', 'last_name': 'Bertozzi...     ZH   \n",
       "4     {'first_name': 'Ernst', 'last_name': 'Wullschl...     BE   \n",
       "...                                                 ...    ...   \n",
       "7600  {'first_name': None, 'last_name': None, 'canto...    NaN   \n",
       "7601  {'first_name': None, 'last_name': None, 'canto...    NaN   \n",
       "7602  {'first_name': None, 'last_name': None, 'canto...    NaN   \n",
       "7603  {'first_name': None, 'last_name': None, 'canto...    NaN   \n",
       "7604  {'first_name': None, 'last_name': None, 'canto...    NaN   \n",
       "\n",
       "                   city  zip_code                     street  \\\n",
       "0                 Baden    5400.0         Bruggerstrasse 71a   \n",
       "1                Zürich    8001.0             Obere Zäune 12   \n",
       "2     La Chaux-de-Fonds    2302.0  Louis-Joseph Chevrolet 46   \n",
       "3             Schlieren    8952.0          Grabenstrasse 11a   \n",
       "4                  Bern    3007.0             Belpstrasse 26   \n",
       "...                 ...       ...                        ...   \n",
       "7600                NaN       NaN                        NaN   \n",
       "7601                NaN       NaN                        NaN   \n",
       "7602                NaN       NaN                        NaN   \n",
       "7603                NaN       NaN                        NaN   \n",
       "7604                NaN       NaN                        NaN   \n",
       "\n",
       "                   firm_name_original     ehraid           uid  \n",
       "0                ABB Turbo Systems AG   329381.0  CHE101538426  \n",
       "1                      Planpartner AG   137360.0  CHE107822340  \n",
       "2                    OPAL création SA   180947.0  CHE101886525  \n",
       "3     Heptares Therapeutics Zurich AG  1146674.0  CHE488613942  \n",
       "4         CH Branchenorganisation der        NaN           NaN  \n",
       "...                               ...        ...           ...  \n",
       "7600                Müri Prototech AG   908535.0  CHE114294676  \n",
       "7601                     TopCoat GmbH   634713.0  CHE109287995  \n",
       "7602                    EMS-CHEMIE AG    52213.0  CHE105904234  \n",
       "7603                       Oncobit AG  1347031.0  CHE193564727  \n",
       "7604                    EMS-CHEMIE AG    52213.0  CHE105904234  \n",
       "\n",
       "[7605 rows x 20 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inno_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
