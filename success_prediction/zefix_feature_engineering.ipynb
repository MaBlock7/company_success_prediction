{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-14 12:30:36.398\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msuccess_prediction.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/manuelbolz/Documents/git/for_work/company_success_prediction\u001b[0m\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/manuelbolz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/manuelbolz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/manuelbolz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ftlangdetect import detect\n",
    "from geopy.geocoders import Nominatim, GoogleV3\n",
    "from transformers import pipeline\n",
    "\n",
    "from pocketknife.database import (\n",
    "    connect_database, read_from_database)\n",
    "\n",
    "from success_prediction.config import (\n",
    "    PROJ_ROOT, RAW_DATA_DIR, EXTERNAL_DATA_DIR, PROCESSED_DATA_DIR)\n",
    "from success_prediction.zefix_processing.clustering import PersonClustering\n",
    "\n",
    "dotenv_path = os.path.join(PROJ_ROOT, '.env')\n",
    "dotenv.load_dotenv(dotenv_path)\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DATE = '2020-04-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARE INSCRIBED PEOPLE/FIRMS DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_inscribed_people = f\"\"\" \n",
    "    SELECT * FROM zefix.history_inscribed_people WHERE founders = TRUE AND shab_date <= '{CUTOFF_DATE}';\n",
    "\"\"\"\n",
    "\n",
    "query_inscribed_firms = f\"\"\" \n",
    "    SELECT * FROM zefix.history_inscribed_firms WHERE shab_date <= '{CUTOFF_DATE}';\n",
    "\"\"\"\n",
    "\n",
    "with connect_database() as con:\n",
    "    df_insc_people = read_from_database(connection=con, query=query_inscribed_people)\n",
    "    df_insc_firms = read_from_database(connection=con, query=query_inscribed_firms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_fids_with_combine_first(df, group_cols=['ehraid', 'fid']):\n",
    "    \"\"\"\n",
    "    Combines instances of the same fid within a company and fills missing\n",
    "    to create a complete representation for each founder\n",
    "    \"\"\"\n",
    "    filled_rows = []\n",
    "    for _, group in df.groupby(group_cols, sort=False):\n",
    "        # Use first entry as base instance of fid\n",
    "        combined = group.iloc[0]\n",
    "        # Iteratively combine_first with the next row(s)\n",
    "        for i in range(1, len(group)):\n",
    "            combined = combined.combine_first(group.iloc[i])\n",
    "        filled_rows.append(combined)\n",
    "    return pd.DataFrame(filled_rows).reset_index(drop=True)\n",
    "\n",
    "def build_founder_dict(df):\n",
    "    founder_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        ehraid = row['ehraid']\n",
    "        fid = row['fid']\n",
    "        \n",
    "        # Handle BFS code lists for hometown (1–5)\n",
    "        hometown_bfs_codes_latest = [\n",
    "            int(code) for code in [\n",
    "                row.get('hometown_1_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_2_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_3_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_4_bfs_gmde_code_latest'),\n",
    "                row.get('hometown_5_bfs_gmde_code_latest'),\n",
    "            ] if pd.notnull(code)\n",
    "        ]\n",
    "        \n",
    "        # Handle BFS code lists for places of residence (1–2)\n",
    "        place_of_residence_bfs_codes_latest = [\n",
    "            int(code) for code in [\n",
    "                row.get('place_of_residence_1_bfs_gmde_code_latest'),\n",
    "                row.get('place_of_residence_2_bfs_gmde_code_latest'),\n",
    "            ] if pd.notnull(code)\n",
    "        ]\n",
    "        \n",
    "        # Handle nationality codes (1–3)\n",
    "        nationality_iso_codes = [\n",
    "            str(code) for code in [\n",
    "                row.get('nationality_1_iso_3166_1_alpha_2'),\n",
    "                row.get('nationality_2_iso_3166_1_alpha_2'),\n",
    "                row.get('nationality_3_iso_3166_1_alpha_2'),\n",
    "            ] if pd.notnull(code) and code != np.nan\n",
    "        ]\n",
    "        \n",
    "        # Prepare entry\n",
    "        founder_data = {\n",
    "            'first_name': row.get('first_name') or '',\n",
    "            'last_name': row.get('last_name') or '',\n",
    "            'first_name_norm': row.get('first_name_norm') or '',\n",
    "            'last_name_norm': row.get('last_name_norm') or '',\n",
    "            'gender': row.get('gender') or '',\n",
    "            'job_title': row.get('job_title') or '',\n",
    "            'dr_title': row.get('founder_with_academic_title'),\n",
    "            'signing_rights': row.get('signing_rights') or '',\n",
    "            'shares': row.get('shares') or '',\n",
    "            'hometown_bfs_codes_latest': hometown_bfs_codes_latest,\n",
    "            'place_of_residence_bfs_codes_latest': place_of_residence_bfs_codes_latest,\n",
    "            'nationality_iso_codes': nationality_iso_codes,\n",
    "        }\n",
    "        \n",
    "        # Insert into nested dictionary\n",
    "        if ehraid not in founder_dict:\n",
    "            founder_dict[ehraid] = {}\n",
    "        founder_dict[ehraid][fid] = founder_data\n",
    "    \n",
    "    return founder_dict\n",
    "\n",
    "def count_founders(founder_dict):\n",
    "    stats = {}\n",
    "    for ehraid, founders in founder_dict.items():\n",
    "        total = 0\n",
    "        male = 0\n",
    "        female = 0\n",
    "        swiss = 0\n",
    "        foreign = 0\n",
    "        dr = 0\n",
    "        for fdata in founders.values():\n",
    "            total += 1\n",
    "            gender = fdata.get('gender', '')\n",
    "            if gender == 'm':\n",
    "                male += 1\n",
    "            elif gender == 'f':\n",
    "                female += 1\n",
    "            nationalities = fdata.get('nationality_iso_codes', [])\n",
    "            if 'CH' in nationalities:\n",
    "                swiss += 1\n",
    "            else:\n",
    "                foreign += 1\n",
    "            dr += fdata.get('dr_title', 0)\n",
    "\n",
    "        stats[ehraid] = {\n",
    "            'n_founders': total,\n",
    "            'n_female_founders': female,\n",
    "            'n_male_founders': male,\n",
    "            'n_swiss_founders': swiss,\n",
    "            'n_foreign_founders': foreign,\n",
    "            'n_dr_titles': dr\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "def get_founder_lists(founder_dict):\n",
    "    stats = {}\n",
    "    for ehraid, founders in founder_dict.items():\n",
    "        names = [(fdata.get('first_name_norm', ''), fdata.get('last_name_norm', '')) for fdata in founders.values()]\n",
    "        fids = [k for k in founders.keys()]\n",
    "        nationalities = [fdata.get('nationality_iso_codes', []) for fdata in founders.values()]\n",
    "        hometowns = [fdata.get('hometown_bfs_codes_latest', []) for fdata in founders.values()]\n",
    "        residencies = [fdata.get('place_of_residence_bfs_codes_latest', []) for fdata in founders.values()]\n",
    "\n",
    "        stats[ehraid] = {\n",
    "            'founder_names': names,\n",
    "            'founder_fids': fids,\n",
    "            'founder_nationalities': nationalities,\n",
    "            'founder_hometowns': hometowns,\n",
    "            'founder_residencies': residencies\n",
    "        }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cluster people within company: 100%|██████████| 357357/357357 [09:28<00:00, 629.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pre-process and cluster the dataframe\n",
    "bfs_code_cols = [col for col in df_insc_people.columns if 'bfs_gmde_code_' in col]\n",
    "df_insc_people[bfs_code_cols] = df_insc_people[bfs_code_cols].astype(str).replace('0', np.nan)\n",
    "df_insc_people[bfs_code_cols] = df_insc_people[bfs_code_cols].replace('None', np.nan).replace('', np.nan)\n",
    "\n",
    "# Cluster the people\n",
    "clustering = PersonClustering(df_insc_people)\n",
    "clustered_df = clustering.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group fids within companies\n",
    "filled_df = group_fids_with_combine_first(clustered_df)\n",
    "for col in bfs_code_cols:\n",
    "    filled_df[col] = filled_df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctor_titles = [\n",
    "    r\"dr\\.?\",\n",
    "    r\"doctor\",\n",
    "    r\"doktor\",\n",
    "    r\"prof\\.?\",\n",
    "    r\"ph\\.?\\s*d\\.?\",         # PhD / Ph.D.\n",
    "    r\"dphil\\.?\",             # DPhil (Oxford)\n",
    "    r\"sc\\.?\\s*d\\.?\",         # ScD / Sc.D.\n",
    "    r\"dsc\\.?\", r\"drsc\\.?\",   # DSc / DrSc\n",
    "    r\"dr\\.?\\s*-?\\s*ing\\.?\",  # Dr-Ing.\n",
    "    r\"dott\\.?\",  # Italian variants\n",
    "    r\"dottore\",\n",
    "    r\"hdr\", # French short forms\n",
    "]\n",
    "pattern = re.compile(rf\"\\b(?:{'|'.join(doctor_titles)})\\b\", flags=re.IGNORECASE)\n",
    "filled_df['founder_with_academic_title'] = filled_df['first_name'].fillna('').str.contains(pattern) | filled_df['last_name'].fillna('').str.contains(pattern)\n",
    "filled_df['founder_with_academic_title'] = filled_df['founder_with_academic_title'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictionary with founders\n",
    "founder_dict = build_founder_dict(filled_df)\n",
    "\n",
    "# Count the total founders, female foundes, and male founders\n",
    "count_stats = count_founders(founder_dict)\n",
    "\n",
    "# Get the names, nationalities, hometowns, and place of residency of all founders as flat lists\n",
    "founder_lists = get_founder_lists(founder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the number of inscribed firms at founding\n",
    "grouped_insc_firms = df_insc_firms.groupby(['ehraid', 'shab_date']).agg({'shab_id': 'count'}).reset_index().rename(columns={'shab_id': 'n_inscribed_firms', 'shab_date': 'firm_inscription_date'})\n",
    "grouped_insc_firms = grouped_insc_firms.sort_values('firm_inscription_date').drop_duplicates(subset=['ehraid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PREPARE FIRM-LEVEL DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2legalform = {\n",
    "    1: 'Sole proprietorship',  # Einzelunternehmen  ->  EXCLUDE\n",
    "    2: 'General Partnership',  # Kollektivgesellschaft  ->  INCLUDE\n",
    "    3: 'Corporation',  # Aktiengesellschaft  ->  INCLUDE\n",
    "    4: 'Limited Liability Company',  # Gesellschaft mit beschränkter Haftung  ->  INCLUDE\n",
    "    5: 'Cooperative',  # Genossenschaft  ->  EXCLUDE\n",
    "    6: 'Association',  # Verein  ->  EXCLUDE\n",
    "    7: 'Foundation',  # Stiftung  ->  EXCLUDE\n",
    "    8: 'Public sector institution',  # Institut des öffentlichen Rechts  ->  EXCLUDE\n",
    "    9: 'Branch',  # Zweigniederlassung  ->  EXCLUDE\n",
    "    10: 'Limited Partnership',  # Kommanditgesellschaft  ->  INCLUDE\n",
    "    11: 'Foreign branch',  # Zweigniederlassung einer ausl. Gesellschaft  ->  EXCLUDE\n",
    "    12: 'Corporation with unlimited partners',  # Kommanditaktiengesellschaft  ->  INCLUDE\n",
    "    13: 'Special legal form',  # Besondere Rechtsform  ->  EXCLUDE\n",
    "    14: 'Ownership in undivided shares',  # Gemeinderschaft  ->  EXCLUDE\n",
    "    15: 'Limited Partnership for collective investment schemes with a fixed capital',  # Investmentgesellschaft mit festem Kapital  ->  INCLUDE\n",
    "    16: 'Limited Partnership for collective investment schemes with a variable capital',  # Investmentgesellschaft mit variablem Kapital  ->  INCLUDE\n",
    "    17: 'Limited Partnership for collective investment schemes',  # Kommanditgesellschaft für kollektive Kapitalanlagen  ->  INCLUDE\n",
    "    18: 'Non commercial power of attorney',  # Nichtkaufmännische Prokure  ->  EXCLUDE\n",
    "    19: '(unknown)',  # (unbekannt)  ->  EXCLUDE\n",
    "}\n",
    "\n",
    "growth_oriented_legal_forms = [2, 3, 4, 10, 12, 15, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query gets the sample of growth oriented firms that were founded between 2016 and current for the prediction sample\n",
    "\n",
    "query_founded_firms = f\"\"\" \n",
    "    SELECT\n",
    "        base.ehraid,\n",
    "        base.uid,\n",
    "\n",
    "        -- Names\n",
    "        base.name AS current_name,\n",
    "        founding_name.firm_name AS founding_name,\n",
    "\n",
    "        -- Legal forms\n",
    "        base.legal_form_id AS current_legal_form,\n",
    "        legal_form.legal_form_id AS founding_legal_form,\n",
    "\n",
    "        -- Purpose\n",
    "        base.purpose_raw AS current_purpose,\n",
    "        founding_purpose.purpose_raw AS founding_purpose,\n",
    "        founding_purpose.purpose_clean AS founding_purpose_clean,\n",
    "\n",
    "        -- Founding NOGA code\n",
    "        founding_purpose.section_1_label AS founding_noga_section_1,\n",
    "        founding_purpose.class_1_label AS founding_noga_class_1,\n",
    "        founding_purpose.prediction_1_score AS founding_noga_score_1,\n",
    "        founding_purpose.section_2_label AS founding_noga_section_2,\n",
    "        founding_purpose.class_2_label AS founding_noga_class_2,\n",
    "        founding_purpose.prediction_2_score AS founding_noga_score_2,\n",
    "        founding_purpose.section_2_label AS founding_noga_section_3,\n",
    "        founding_purpose.class_2_label AS founding_noga_class_3,\n",
    "        founding_purpose.prediction_2_score AS founding_noga_score_3,\n",
    "\n",
    "        -- Current address\n",
    "        COALESCE(address.street, '') || ' ' || COALESCE(address.house_number, '') AS current_street,\n",
    "        address.town AS current_town,\n",
    "        address.swiss_zip_code AS current_zip_code,\n",
    "        address.country AS current_country,\n",
    "\n",
    "        -- Founding address\n",
    "        founding_address.street AS founding_street,\n",
    "        founding_address.town AS founding_town,\n",
    "        founding_address.postal_code AS founding_zip_code,\n",
    "        founding_address.town_bfs_gmde_code_latest AS founding_bfs_code,\n",
    "\n",
    "        -- Founding SHAB entry\n",
    "        shab.shab_id,\n",
    "        shab.shab_date AS founding_date,\n",
    "        shab.message AS founding_message\n",
    "\n",
    "    FROM zefix_release_159.base base\n",
    "\n",
    "    -- Founding SHAB messages\n",
    "    INNER JOIN (\n",
    "        SELECT s.ehraid, s.shab_id, s.shab_date, s.message\n",
    "        FROM zefix_release_159.shab s\n",
    "        INNER JOIN zefix_release_159.shab_mutation sm ON s.shab_id = sm.shab_id\n",
    "        WHERE sm.description = 'status.neu'\n",
    "    ) AS shab ON base.ehraid = shab.ehraid\n",
    "\n",
    "    -- Current address\n",
    "    LEFT JOIN zefix_release_159.address address ON base.ehraid = address.ehraid\n",
    "\n",
    "    -- Founding address\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT hfa.ehraid, hfa.street, hfa.postal_code, hfa.town, hfa.town_bfs_gmde_code_latest\n",
    "        FROM zefix.history_firm_addresses hfa\n",
    "        WHERE founding = TRUE\n",
    "    ) AS founding_address ON base.ehraid = founding_address.ehraid\n",
    "\n",
    "    -- Founding name\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT hfn.ehraid, hfn.firm_name\n",
    "        FROM zefix.history_firm_names hfn\n",
    "        WHERE hfn.founding = TRUE\n",
    "    ) AS founding_name ON base.ehraid = founding_name.ehraid\n",
    "\n",
    "    -- Founding purpose\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT \n",
    "            hp.ehraid, hp.purpose_raw, sec.purpose as purpose_clean,\n",
    "            sec.section_1_label, sec.class_1_label, sec.section_1_label, sec.prediction_1_score,\n",
    "            sec.section_2_label, sec.class_2_label, sec.section_2_label, sec.prediction_2_score,\n",
    "            sec.section_3_label, sec.class_3_label, sec.section_3_label, sec.prediction_3_score\n",
    "        FROM zefix.history_purpose hp\n",
    "        LEFT JOIN zefix.history_sector sec\n",
    "        ON hp.ehraid = sec.ehraid AND hp.shab_id = sec.shab_id\n",
    "        WHERE hp.founding_purpose = TRUE\n",
    "    ) AS founding_purpose ON base.ehraid = founding_purpose.ehraid\n",
    "\n",
    "    -- Founding legal form\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT hlf.ehraid, hlf.legal_form_id\n",
    "        FROM zefix.history_founding_legal_form hlf\n",
    "    ) AS legal_form ON base.ehraid = legal_form.ehraid\n",
    "\n",
    "    -- Filter out irrelevant records\n",
    "    WHERE\n",
    "        NOT base.is_branch\n",
    "        AND shab.shab_date < '{CUTOFF_DATE}'\n",
    "        AND base.legal_form_id IN (2, 3, 4, 10, 12, 15, 16, 17)\n",
    "        AND LOWER(base.name) NOT LIKE '%zweigniederlassung%'\n",
    "        AND LOWER(base.name) NOT LIKE '%succursale%';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    df_startups = read_from_database(connection=con, query=query_founded_firms)\n",
    "\n",
    "df_startups['founding_date'] = pd.to_datetime(df_startups['founding_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(226559, 25)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observed duplicates stem from entries having multiple new inscriptions in Zefix. -> Remove them from the sample because history seems to contain errors\n",
    "display(df_startups[df_startups.duplicated(subset=['ehraid', 'founding_town'], keep=False)].uid.unique())\n",
    "df_startups = df_startups.drop_duplicates(subset=['ehraid'], keep=False)\n",
    "display(df_startups.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEO ENCODE ADDRESS INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current information if the founding address is missing\n",
    "df_startups['founding_street'] = df_startups['founding_street'].fillna(df_startups['current_street'])\n",
    "df_startups['founding_zip_code'] = df_startups['founding_zip_code'].fillna(df_startups['current_zip_code'])\n",
    "df_startups['founding_town'] = df_startups['founding_town'].fillna(df_startups['current_town'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_startups[df_startups['founding_street'].isna()].empty\n",
    "assert df_startups[df_startups['founding_zip_code'].isna()].empty\n",
    "assert df_startups[df_startups['founding_town'].isna()].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominatim_geolocator = Nominatim(\n",
    "    user_agent=\"local_geocoder\",\n",
    "    domain=\"localhost:8080\",\n",
    "    scheme=\"http\"\n",
    ")\n",
    "google_geolocator = GoogleV3(api_key=os.getenv('GOOGLE_GEOCODE_API_KEY'))\n",
    "\n",
    "\n",
    "def geocode_address(nominatim_geolocator, google_geolocator, row):\n",
    "    try:\n",
    "        location = nominatim_geolocator.geocode({\n",
    "            'street': row['founding_street'],\n",
    "            'city': row['founding_town'],\n",
    "            'postalcode': int(row['founding_zip_code']),\n",
    "            'country': 'Schweiz'\n",
    "        }, timeout=2)\n",
    "        if location:\n",
    "            return pd.Series([location.address, location.latitude, location.longitude])\n",
    "        else:\n",
    "            location = google_geolocator.geocode({\n",
    "                'street': row['founding_street'],\n",
    "                'city': row['founding_town'],\n",
    "                'postalcode': int(row['founding_zip_code']),\n",
    "                'country': 'Schweiz'\n",
    "            }, timeout=1)\n",
    "            if location:\n",
    "                return pd.Series([location.address, location.latitude, location.longitude])\n",
    "            return pd.Series([None, None, None])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return pd.Series([None, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[['geocoded_address', 'latitude', 'longitude']] = df_startups.apply(lambda row: geocode_address(nominatim_geolocator, google_geolocator, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['founding_bfs_code'] = df_startups['founding_bfs_code'].astype(int)\n",
    "df_startups['founding_zip_code'] = df_startups['founding_zip_code'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETERMINE BFS MUNICIPALITY CODE BY COORDINATES WHERE MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(EXTERNAL_DATA_DIR / 'geo_data' / 'swissBOUNDARIES3D_1_5_LV95_LN02.gdb', layer=\"TLM_HOHEITSGEBIET\")\n",
    "gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "gdf = gdf[['geometry', 'BFS_NUMMER', 'EINWOHNERZAHL']]\n",
    "\n",
    "df_startups = gpd.GeoDataFrame(\n",
    "    df_startups,\n",
    "    geometry=gpd.points_from_xy(df_startups['longitude'], df_startups['latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "df_startups = gpd.sjoin(df_startups, gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Replace where code is 0 (unmatched) or where it does not match the coordinates\n",
    "df_startups.loc[df_startups['founding_bfs_code'] == 0, 'founding_bfs_code'] = pd.NA\n",
    "df_startups.loc[(df_startups['founding_bfs_code'].astype(float) != df_startups['BFS_NUMMER']) & (~df_startups['BFS_NUMMER'].isna()), 'founding_bfs_code'] = pd.NA\n",
    "df_startups['founding_bfs_code'] = df_startups['founding_bfs_code'].fillna(df_startups['BFS_NUMMER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['founding_bfs_code'] = df_startups['founding_bfs_code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>founding_town</th>\n",
       "      <th>combined_address</th>\n",
       "      <th>founding_bfs_code</th>\n",
       "      <th>BFS_NUMMER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223603</th>\n",
       "      <td>Chiasso</td>\n",
       "      <td>Via Henry Dunant 1, 6830 Chiasso</td>\n",
       "      <td>5250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223613</th>\n",
       "      <td>Morcote</td>\n",
       "      <td>Via Isella 11, 6922 Morcote</td>\n",
       "      <td>5203</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223639</th>\n",
       "      <td>Brusino Arsizio</td>\n",
       "      <td>Via Lungolago 83, 6827 Brusino Arsizio</td>\n",
       "      <td>5160</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224506</th>\n",
       "      <td>San Bernardino</td>\n",
       "      <td>Residenza Mons Avium , appartamento 25, 6565 S...</td>\n",
       "      <td>3822</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224785</th>\n",
       "      <td>La Tène</td>\n",
       "      <td>route de Bellevue 7, 2074 La Tène</td>\n",
       "      <td>6513</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225829</th>\n",
       "      <td>Roggwil TG</td>\n",
       "      <td>Im Pünst 1, 9325 Roggwil TG</td>\n",
       "      <td>4431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225873</th>\n",
       "      <td>Bassins</td>\n",
       "      <td>Chemin de Raulan 24, 1269 Bassins</td>\n",
       "      <td>5703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226046</th>\n",
       "      <td>Warth</td>\n",
       "      <td>Kartause Ittingen, 8532 Warth</td>\n",
       "      <td>4621</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          founding_town                                   combined_address  \\\n",
       "223603          Chiasso                   Via Henry Dunant 1, 6830 Chiasso   \n",
       "223613          Morcote                        Via Isella 11, 6922 Morcote   \n",
       "223639  Brusino Arsizio             Via Lungolago 83, 6827 Brusino Arsizio   \n",
       "224506   San Bernardino  Residenza Mons Avium , appartamento 25, 6565 S...   \n",
       "224785          La Tène                  route de Bellevue 7, 2074 La Tène   \n",
       "225829       Roggwil TG                        Im Pünst 1, 9325 Roggwil TG   \n",
       "225873          Bassins                  Chemin de Raulan 24, 1269 Bassins   \n",
       "226046            Warth                      Kartause Ittingen, 8532 Warth   \n",
       "\n",
       "        founding_bfs_code  BFS_NUMMER  \n",
       "223603               5250         NaN  \n",
       "223613               5203         NaN  \n",
       "223639               5160         NaN  \n",
       "224506               3822         NaN  \n",
       "224785               6513         NaN  \n",
       "225829               4431         NaN  \n",
       "225873               5703         NaN  \n",
       "226046               4621         NaN  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups[df_startups.founding_bfs_code.astype(float) != df_startups.BFS_NUMMER][['founding_town', 'combined_address', 'founding_bfs_code', 'BFS_NUMMER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups.drop(columns=['geometry', 'index_right', 'Unnamed: 0', 'BFS_NUMMER'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD MUNICIPALITY TYPOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typology = pd.read_excel(EXTERNAL_DATA_DIR / 'geo_data' / 'Raumgliederungen.xlsx')\n",
    "df_typology.drop(columns=['Gemeindename', 'Bezirksname', 'Kanton'], inplace=True)\n",
    "df_typology = df_typology.rename(columns={'BFS Gde-nummer': 'founding_bfs_code', 'Bezirks-nummer': 'district_id', 'Kantons-nummer': 'canton_id', 'Stadt/Land-Typologie': 'urban_rural', 'Gemeindetypologie (9 Typen)': 'typology_9c', 'Gemeindetypologie (25 Typen)': 'typology_25c'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups = df_startups.merge(df_typology, on='founding_bfs_code', how='left')\n",
    "df_startups.rename(columns={'EINWOHNERZAHL': 'population'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "      <th>delete_date</th>\n",
       "      <th>reason_for_dissolution</th>\n",
       "      <th>liquidation</th>\n",
       "      <th>bankruptcy</th>\n",
       "      <th>other_exit</th>\n",
       "      <th>current_name</th>\n",
       "      <th>founding_name</th>\n",
       "      <th>current_legal_form</th>\n",
       "      <th>...</th>\n",
       "      <th>combined_address</th>\n",
       "      <th>geocoded_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "      <th>canton_id</th>\n",
       "      <th>Bezirks-nummer</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>typology_9c</th>\n",
       "      <th>typology_25c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1255845</td>\n",
       "      <td>CHE395917849</td>\n",
       "      <td>2020-06-19</td>\n",
       "      <td>['Nachdem kein begründeter Einspruch gegen die...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Same Same GmbH in Liquidation</td>\n",
       "      <td>Same Same GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Seeplatz 1, 8820 Wädenswil</td>\n",
       "      <td>1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...</td>\n",
       "      <td>47.228758</td>\n",
       "      <td>8.676404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80542</th>\n",
       "      <td>1389236</td>\n",
       "      <td>CHE291873431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MS Glärnisch AG</td>\n",
       "      <td>MS Glärnisch AG</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Seeplatz 1, 8820 Wädenswil</td>\n",
       "      <td>1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...</td>\n",
       "      <td>47.228758</td>\n",
       "      <td>8.676404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223147</th>\n",
       "      <td>1310452</td>\n",
       "      <td>CHE338654358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mit Entscheid vom 07.01.2025 hat der Einzelr...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Peter Jegen GmbH in Liquidation</td>\n",
       "      <td>Peter Jegen GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Sagastrasse 3, 7214 Grüsch</td>\n",
       "      <td>Sägastrasse 3, 9495 Triesen, Liechtenstein</td>\n",
       "      <td>47.088149</td>\n",
       "      <td>9.522204</td>\n",
       "      <td>5532.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ehraid           uid delete_date  \\\n",
       "2469    1255845  CHE395917849  2020-06-19   \n",
       "80542   1389236  CHE291873431         NaN   \n",
       "223147  1310452  CHE338654358         NaN   \n",
       "\n",
       "                                   reason_for_dissolution liquidation  \\\n",
       "2469    ['Nachdem kein begründeter Einspruch gegen die...       False   \n",
       "80542                                                 NaN         NaN   \n",
       "223147  ['Mit Entscheid vom 07.01.2025 hat der Einzelr...       False   \n",
       "\n",
       "       bankruptcy other_exit                     current_name  \\\n",
       "2469        False       True    Same Same GmbH in Liquidation   \n",
       "80542         NaN        NaN                  MS Glärnisch AG   \n",
       "223147       True      False  Peter Jegen GmbH in Liquidation   \n",
       "\n",
       "           founding_name  current_legal_form  ...            combined_address  \\\n",
       "2469      Same Same GmbH                   4  ...  Seeplatz 1, 8820 Wädenswil   \n",
       "80542    MS Glärnisch AG                   3  ...  Seeplatz 1, 8820 Wädenswil   \n",
       "223147  Peter Jegen GmbH                   4  ...  Sagastrasse 3, 7214 Grüsch   \n",
       "\n",
       "                                         geocoded_address   latitude  \\\n",
       "2469    1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...  47.228758   \n",
       "80542   1, Seeplatz, Wädenswil, Bezirk Horgen, Zürich,...  47.228758   \n",
       "223147         Sägastrasse 3, 9495 Triesen, Liechtenstein  47.088149   \n",
       "\n",
       "       longitude population  canton_id Bezirks-nummer urban_rural typology_9c  \\\n",
       "2469    8.676404        0.0        NaN            NaN         NaN         NaN   \n",
       "80542   8.676404        0.0        NaN            NaN         NaN         NaN   \n",
       "223147  9.522204     5532.0        NaN            NaN         NaN         NaN   \n",
       "\n",
       "        typology_25c  \n",
       "2469             NaN  \n",
       "80542            NaN  \n",
       "223147           NaN  \n",
       "\n",
       "[3 rows x 34 columns]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups[df_startups.canton_id.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD STARTING CAPITAL TO COMPANY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical exchange rates\n",
    "import yfinance as yf\n",
    "\n",
    "exchange_rate_dfs = []\n",
    "for symbol in ['EUR', 'GBP', 'USD']:\n",
    "    ticker = yf.Ticker(f'{symbol}CHF=X')\n",
    "    df_ticker = ticker.history(start='2016-01-01', end='2024-01-01')\n",
    "    df_ticker['symbol'] = symbol\n",
    "    exchange_rate_dfs.append(df_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exchange_rates = pd.concat(exchange_rate_dfs).reset_index()[['Date', 'symbol', 'Open']]\n",
    "\n",
    "df_exchange_rates = df_exchange_rates.rename(columns={'Date': 'founding_date'})\n",
    "df_exchange_rates['founding_date'] = pd.to_datetime(df_exchange_rates['founding_date']).dt.date\n",
    "df_exchange_rates['founding_date'] = pd.to_datetime(df_exchange_rates['founding_date'])\n",
    "\n",
    "df_temp = pd.DataFrame({'founding_date': pd.date_range(start='2016-01-01', end='2024-01-01').tolist() * 3})\n",
    "df_temp['symbol'] = ['EUR'] * int(len(df_temp) / 3) + ['GBP'] * int(len(df_temp) / 3) + ['USD'] * int(len(df_temp) / 3)\n",
    "\n",
    "df_exchange_rates = df_temp.merge(df_exchange_rates, on=['founding_date', 'symbol'], how='left')\n",
    "df_exchange_rates['Open'] = df_exchange_rates['Open'].ffill()\n",
    "df_exchange_rates['symbol'] = df_exchange_rates['symbol'].ffill()\n",
    "\n",
    "df_exchange_rates.to_csv(EXTERNAL_DATA_DIR / 'exchange_rates' / 'exchange_rates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_capital = \"\"\" \n",
    "    SELECT * FROM zefix.history_registered_capital WHERE shab_date < '2024-01-01';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    df_capital = read_from_database(connection=con, query=query_capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capital = df_capital.rename(columns={'shab_date': 'founding_date', 'currency_new': 'symbol'})\n",
    "df_capital['founding_date'] = pd.to_datetime(df_capital['founding_date'])\n",
    "\n",
    "mapping = {\n",
    "    'Euro': 'EUR',\n",
    "    'Eur': 'EUR',\n",
    "    'EURO': 'EUR',\n",
    "    '€': 'EUR',\n",
    "    'fr': 'CHF',\n",
    "    'Fr.': 'CHF',\n",
    "    'CHE': 'CHF',\n",
    "    '£': 'GBP',\n",
    "    'US': 'USD'\n",
    "}\n",
    "df_capital['symbol'] = df_capital['symbol'].replace(mapping)\n",
    "\n",
    "# Drop duplicate entries where we have libaration information do avoid duplicates before aggregation\n",
    "df_capital = df_capital[~df_capital.duplicated(subset=['ehraid', 'founding_date'], keep=False) | (df_capital.duplicated(subset=['ehraid', 'founding_date'], keep=False) & ~(df_capital['keyword'].str.contains('liberierung|liberato|libéré', regex=True)))]\n",
    "\n",
    "# Drop entries where the currency is not a common currency\n",
    "df_capital = df_capital[df_capital.symbol.isin(['CHF', 'EUR', 'USD', 'GBP'])]\n",
    "\n",
    "# Add exchange rates and convert registered capital\n",
    "df_capital = df_capital.merge(df_exchange_rates, on=['symbol', 'founding_date'], how='left')\n",
    "df_capital['Open'] = df_capital['Open'].fillna(1.0)\n",
    "df_capital['capital_chf'] = df_capital['capital_new'].astype(float) * df_capital['Open'].astype(float)\n",
    "\n",
    "# Aggregate capital into one value for registered capital\n",
    "df_capital_agg = df_capital.groupby(['ehraid', 'founding_date']).agg({'capital_chf': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups = df_startups.merge(df_capital_agg[['ehraid', 'founding_date', 'capital_chf']], on=['ehraid', 'founding_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from legal form to required minimum capital (at registration)\n",
    "legalform_min_capital = {\n",
    "    2: 0,        # General Partnership\n",
    "    3: 100_000,  # Corporation (AG)\n",
    "    4: 20_000,   # GmbH\n",
    "    10: 0,       # Limited Partnership\n",
    "    12: 100_000, # Corporation with unlimited partners\n",
    "    15: 500_000, # Investmentgesellschaft mit festem Kapital\n",
    "    16: 5_000_000,  # SICAV (see note)\n",
    "    17: 100_000, # Kommanditgesellschaft für kollektive Kapitalanlagen\n",
    "}\n",
    "\n",
    "# Clean up legal form column\n",
    "df_startups['founding_legal_form'] = df_startups['founding_legal_form'].fillna(2).astype(int)  # Assume missings are General Partnerships\n",
    "\n",
    "# Only update capital where it is missing\n",
    "mask = df_startups['capital_chf'].isna()\n",
    "df_startups.loc[mask, 'capital_chf'] = (\n",
    "    df_startups.loc[mask, 'founding_legal_form']\n",
    "    .map(legalform_min_capital)\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "      <th>delete_date</th>\n",
       "      <th>reason_for_dissolution</th>\n",
       "      <th>liquidation</th>\n",
       "      <th>bankruptcy</th>\n",
       "      <th>other_exit</th>\n",
       "      <th>current_name</th>\n",
       "      <th>founding_name</th>\n",
       "      <th>current_legal_form</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>population</th>\n",
       "      <th>canton_id</th>\n",
       "      <th>district_id</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>typology_9c</th>\n",
       "      <th>typology_25c</th>\n",
       "      <th>capital_chf</th>\n",
       "      <th>company_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1251325</td>\n",
       "      <td>CHE153193257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mit Urteil des Gerichtspräsidenten des Zivil...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Arlez Carrosserie GmbH in Liquidation</td>\n",
       "      <td>Arlez Carrosserie GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.460137</td>\n",
       "      <td>7.861180</td>\n",
       "      <td>6296.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>http://www.arlez-carrosserie.ch/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1251326</td>\n",
       "      <td>CHE392024369</td>\n",
       "      <td>2020-11-11</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Vista Coaching GmbH in Liquidation</td>\n",
       "      <td>Vista Coaching GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.504062</td>\n",
       "      <td>7.724207</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>http://vista-coaching.ch/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1251327</td>\n",
       "      <td>CHE473646370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.523075</td>\n",
       "      <td>7.845789</td>\n",
       "      <td>941.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>no website available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1251328</td>\n",
       "      <td>CHE205344235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wolf Regio GmbH</td>\n",
       "      <td>Wolf Regio GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.518085</td>\n",
       "      <td>7.603328</td>\n",
       "      <td>12304.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>http://wolfregio.ch/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1251329</td>\n",
       "      <td>CHE190527339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Mit Entscheid vom 27.09.2022 , 9.15 Uhr , ha...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>AHAS GmbH in Liquidation</td>\n",
       "      <td>AHAS GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>47.036300</td>\n",
       "      <td>8.177812</td>\n",
       "      <td>7771.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>no website available</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ehraid           uid delete_date  \\\n",
       "0  1251325  CHE153193257         NaN   \n",
       "1  1251326  CHE392024369  2020-11-11   \n",
       "2  1251327  CHE473646370         NaN   \n",
       "3  1251328  CHE205344235         NaN   \n",
       "4  1251329  CHE190527339         NaN   \n",
       "\n",
       "                              reason_for_dissolution liquidation bankruptcy  \\\n",
       "0  ['Mit Urteil des Gerichtspräsidenten des Zivil...        True      False   \n",
       "1                                                 []       False      False   \n",
       "2                                                NaN         NaN        NaN   \n",
       "3                                                NaN         NaN        NaN   \n",
       "4  ['Mit Entscheid vom 27.09.2022 , 9.15 Uhr , ha...       False       True   \n",
       "\n",
       "  other_exit                           current_name            founding_name  \\\n",
       "0      False  Arlez Carrosserie GmbH in Liquidation   Arlez Carrosserie GmbH   \n",
       "1       True     Vista Coaching GmbH in Liquidation      Vista Coaching GmbH   \n",
       "2        NaN                Wissler Consulting GmbH  Wissler Consulting GmbH   \n",
       "3        NaN                        Wolf Regio GmbH          Wolf Regio GmbH   \n",
       "4      False               AHAS GmbH in Liquidation                AHAS GmbH   \n",
       "\n",
       "   current_legal_form  ...   latitude longitude population canton_id  \\\n",
       "0                   4  ...  47.460137  7.861180     6296.0      13.0   \n",
       "1                   4  ...  47.504062  7.724207     4700.0      13.0   \n",
       "2                   4  ...  47.523075  7.845789      941.0      13.0   \n",
       "3                   4  ...  47.518085  7.603328    12304.0      13.0   \n",
       "4                   4  ...  47.036300  8.177812     7771.0       3.0   \n",
       "\n",
       "  district_id  urban_rural typology_9c typology_25c capital_chf  \\\n",
       "0      1304.0          2.0        21.0        217.0     20000.0   \n",
       "1      1303.0          1.0        11.0        113.0     20000.0   \n",
       "2      1304.0          3.0        23.0        236.0     20000.0   \n",
       "3      1301.0          1.0        11.0        112.0     20000.0   \n",
       "4       312.0          2.0        21.0        216.0     20000.0   \n",
       "\n",
       "                        company_url  \n",
       "0  http://www.arlez-carrosserie.ch/  \n",
       "1         http://vista-coaching.ch/  \n",
       "2              no website available  \n",
       "3              http://wolfregio.ch/  \n",
       "4              no website available  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD WEBSITE URLS TO COMPANY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv(RAW_DATA_DIR / 'company_urls' / 'urls.csv')\n",
    "current_website_stats = pd.read_csv(PROCESSED_DATA_DIR / 'summary_stats' / 'current_website_stats_grouped.csv')\n",
    "current_website_stats.columns = [f'current_{col}' if col != 'ehraid' else col for col in current_website_stats.columns]\n",
    "\n",
    "founding_website_stats = pd.read_csv(PROCESSED_DATA_DIR / 'summary_stats' / 'wayback_website_stats_grouped.csv')\n",
    "founding_website_stats.columns = [f'founding_{col}' if col != 'ehraid' else col for col in founding_website_stats.columns ]\n",
    "\n",
    "websites['ehraid'] = websites['ehraid'].astype(int)\n",
    "\n",
    "assert websites[websites.duplicated(subset='ehraid', keep=False)].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of companies with found website: 48.409195526793205\n"
     ]
    }
   ],
   "source": [
    "df_startups = df_startups.merge(websites[['ehraid', 'company_url']], on='ehraid', how='left')\n",
    "df_startups = df_startups.merge(current_website_stats[['ehraid', 'current_n_pages', 'current_total_text_len', 'current_mean_text_len', 'current_n_internal_links_mean', 'current_n_external_links_mean', 'current_n_languages', 'current_dominant_language']], on='ehraid', how='left')\n",
    "df_startups = df_startups.merge(founding_website_stats[['ehraid', 'founding_n_pages', 'founding_total_text_len', 'founding_mean_text_len', 'founding_n_internal_links_mean', 'founding_n_external_links_mean', 'founding_n_languages', 'founding_dominant_language']], on='ehraid', how='left')\n",
    "\n",
    "df_startups['company_url'] = df_startups['company_url'].fillna('no website available')\n",
    "print(f'Percentage of companies with found website: {len(df_startups[df_startups.company_url != 'no website available']) / len(df_startups) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[df_startups.company_url != 'no website available'].to_csv(RAW_DATA_DIR / 'company_sample' / 'company_sample_website.csv', index=False)\n",
    "df_startups[df_startups.company_url == 'no website available'].to_csv(RAW_DATA_DIR / 'company_sample' / 'company_sample_no_website.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_startups.shape[0]\n",
    "num_website = df_startups[(df_startups.company_url != 'no website available')].shape[0]\n",
    "num_no_website = df_startups[(df_startups.company_url == 'no website available')].shape[0]\n",
    "\n",
    "num_exits = df_startups[(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_exits_website = df_startups[(df_startups.company_url != 'no website available') & (df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_exits_no_website = df_startups[(df_startups.company_url == 'no website available') & (df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_survival = df_startups[~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_survival_website = df_startups[(df_startups.company_url != 'no website available') & ~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "num_survival_no_website = df_startups[(df_startups.company_url == 'no website available') & ~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)].shape[0]\n",
    "print(f'Percentage of firms having website: {num_website / total:.4f}')\n",
    "print(f'Percentage of exited firms having website: {num_exits_website / num_exits:.4f}')\n",
    "print(f'Percentage of survived firms having website: {num_survival_website / num_survival:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ratio of survival to exit: {num_survival / num_exits:.2f}')\n",
    "print(f'Ratio of survival with website to exit with website: {num_survival_website / num_exits_website:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)]\n",
    "df_startups[(df_startups.company_url != 'no website available') & ~(df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)]\n",
    "df_startups[(df_startups.company_url != 'no website available') & (df_startups.liquidation | df_startups.bankruptcy | df_startups.other_exit)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD WEBSITE EMBEDDING-BASED SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_responsibility = pd.read_csv(PROCESSED_DATA_DIR / 'current_websites_responsibility_scores.csv')\n",
    "current_differantiation = pd.read_csv(PROCESSED_DATA_DIR / 'current_strat2vec_differentiation_scores.csv')\n",
    "current_doc2vec = pd.read_csv(PROCESSED_DATA_DIR / 'current_doc2vec_differentiation_scores.csv')\n",
    "\n",
    "founding_responsibility = pd.read_csv(PROCESSED_DATA_DIR / 'wayback_websites_responsibility_scores.csv')\n",
    "founding_differentiation = pd.read_csv(PROCESSED_DATA_DIR / 'wayback_strat2vec_differentiation_scores.csv')\n",
    "founding_doc2vec = pd.read_csv(PROCESSED_DATA_DIR / 'wayback_doc2vec_differentiation_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare responsibility scores\n",
    "current_responsibility.drop(columns=['date'], inplace=True)\n",
    "current_responsibility.columns = [f'current_{col}' if col != 'ehraid' else 'ehraid' for col in current_responsibility.columns]\n",
    "\n",
    "founding_responsibility = founding_responsibility.drop(columns=['date'])\n",
    "founding_responsibility.columns = [f'founding_{col}' if col != 'ehraid' else 'ehraid' for col in founding_responsibility.columns]\n",
    "\n",
    "# Prepare doc2vec differentiation scores\n",
    "current_doc2vec.drop(columns=['competitors', 'score_type', 'field'], inplace=True)\n",
    "current_doc2vec = current_doc2vec.rename(columns={'score': 'current_doc2vec_diff'})\n",
    "\n",
    "founding_doc2vec.drop(columns=['competitors', 'score_type', 'field'], inplace=True)\n",
    "founding_doc2vec = founding_doc2vec.rename(columns={'score': 'founding_doc2vec_diff'})\n",
    "\n",
    "# Prepare contextual differentiation scores\n",
    "current_differantiation = current_differantiation.pivot(index=['ehraid'], columns='field', values='score').reset_index()\n",
    "current_differantiation.columns = [f'current_{col}' if col != 'ehraid' else 'ehraid' for col in current_differantiation.columns]\n",
    "\n",
    "founding_differentiation = founding_differentiation.pivot(index=['ehraid'], columns='field', values='score').reset_index()\n",
    "founding_differentiation.columns = [f'founding_{col}' if col != 'ehraid' else 'ehraid' for col in founding_differentiation.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [current_responsibility, founding_responsibility, current_doc2vec, founding_doc2vec, current_differantiation, founding_differentiation]:\n",
    "    df_startups = df_startups.merge(df, on='ehraid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FEATURE ENCODING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/6qys5jn501gbqmkkkgmvpf0h0000gn/T/ipykernel_580/424907860.py:1: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_startups = pd.read_csv(RAW_DATA_DIR / 'company_sample' / 'until_2020' / '2020_sample_base_data.csv')\n"
     ]
    }
   ],
   "source": [
    "df_startups = pd.read_csv(RAW_DATA_DIR / 'company_sample' / 'until_2020' / '2020_sample_base_data.csv')\n",
    "df_startups['founding_date'] = pd.to_datetime(df_startups['founding_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>uid</th>\n",
       "      <th>current_name</th>\n",
       "      <th>founding_name</th>\n",
       "      <th>current_legal_form</th>\n",
       "      <th>founding_legal_form</th>\n",
       "      <th>current_purpose</th>\n",
       "      <th>founding_purpose</th>\n",
       "      <th>current_street</th>\n",
       "      <th>current_town</th>\n",
       "      <th>...</th>\n",
       "      <th>dominant_language</th>\n",
       "      <th>class_1_label</th>\n",
       "      <th>section_1_label</th>\n",
       "      <th>prediction_1_score</th>\n",
       "      <th>class_2_label</th>\n",
       "      <th>section_2_label</th>\n",
       "      <th>prediction_2_score</th>\n",
       "      <th>class_3_label</th>\n",
       "      <th>section_3_label</th>\n",
       "      <th>prediction_3_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1251327</td>\n",
       "      <td>CHE473646370</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>Wissler Consulting GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Die Gesellschaft bezweckt die Erbringung von B...</td>\n",
       "      <td>Die Gesellschaft bezweckt die Erbringung von B...</td>\n",
       "      <td>Wintersingerstrasse 18a</td>\n",
       "      <td>Maisprach</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7022.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.318912</td>\n",
       "      <td>7021.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.096343</td>\n",
       "      <td>7490.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.082247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1251329</td>\n",
       "      <td>CHE190527339</td>\n",
       "      <td>AHAS GmbH in Liquidation</td>\n",
       "      <td>AHAS GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Führung einer Baufirma, speziell Ausführung sä...</td>\n",
       "      <td>Führung einer Baufirma , speziell Ausführung s...</td>\n",
       "      <td>Bösch 21</td>\n",
       "      <td>Hünenberg</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4333.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.186411</td>\n",
       "      <td>4399.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.125374</td>\n",
       "      <td>4391.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.070668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1251336</td>\n",
       "      <td>CHE350451441</td>\n",
       "      <td>Roof &amp; Terrace AG</td>\n",
       "      <td>Roof &amp; Terrace AG</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Führung von Hotels, Restaurants und Barbetrieb...</td>\n",
       "      <td>Führung von Hotels , Restaurants und Barbetrie...</td>\n",
       "      <td>Pilatusstrasse 1</td>\n",
       "      <td>Luzern</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7022.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.267851</td>\n",
       "      <td>8299.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.093203</td>\n",
       "      <td>7021.0</td>\n",
       "      <td>M</td>\n",
       "      <td>0.091909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1251338</td>\n",
       "      <td>CHE497156719</td>\n",
       "      <td>SP Ventures GmbH</td>\n",
       "      <td>SP Ventures GmbH</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Die Gesellschaft übt die Funktion einer Holdin...</td>\n",
       "      <td>Die Gesellschaft übt die Funktion einer Holdin...</td>\n",
       "      <td>Zürichstrasse 5</td>\n",
       "      <td>Luzern</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6420.0</td>\n",
       "      <td>K</td>\n",
       "      <td>0.131271</td>\n",
       "      <td>6820.0</td>\n",
       "      <td>L</td>\n",
       "      <td>0.109448</td>\n",
       "      <td>6430.0</td>\n",
       "      <td>K</td>\n",
       "      <td>0.103592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1251341</td>\n",
       "      <td>CHE256940465</td>\n",
       "      <td>The Food Bus, Yanev &amp; Co</td>\n",
       "      <td>The Food Bus , Yanev &amp; Co</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>exploitation d'un Food Truck, cuisine ambulante.</td>\n",
       "      <td>exploitation d'un Food Truck , cuisine ambulante</td>\n",
       "      <td>Route de Bourguillon 19</td>\n",
       "      <td>Marly</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5610.0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.301125</td>\n",
       "      <td>5629.0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.130845</td>\n",
       "      <td>5621.0</td>\n",
       "      <td>I</td>\n",
       "      <td>0.082071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ehraid           uid              current_name              founding_name  \\\n",
       "0  1251327  CHE473646370   Wissler Consulting GmbH    Wissler Consulting GmbH   \n",
       "1  1251329  CHE190527339  AHAS GmbH in Liquidation                  AHAS GmbH   \n",
       "2  1251336  CHE350451441         Roof & Terrace AG          Roof & Terrace AG   \n",
       "3  1251338  CHE497156719          SP Ventures GmbH           SP Ventures GmbH   \n",
       "4  1251341  CHE256940465  The Food Bus, Yanev & Co  The Food Bus , Yanev & Co   \n",
       "\n",
       "   current_legal_form  founding_legal_form  \\\n",
       "0                   4                  4.0   \n",
       "1                   4                  4.0   \n",
       "2                   3                  3.0   \n",
       "3                   4                  4.0   \n",
       "4                   2                  2.0   \n",
       "\n",
       "                                     current_purpose  \\\n",
       "0  Die Gesellschaft bezweckt die Erbringung von B...   \n",
       "1  Führung einer Baufirma, speziell Ausführung sä...   \n",
       "2  Führung von Hotels, Restaurants und Barbetrieb...   \n",
       "3  Die Gesellschaft übt die Funktion einer Holdin...   \n",
       "4   exploitation d'un Food Truck, cuisine ambulante.   \n",
       "\n",
       "                                    founding_purpose           current_street  \\\n",
       "0  Die Gesellschaft bezweckt die Erbringung von B...  Wintersingerstrasse 18a   \n",
       "1  Führung einer Baufirma , speziell Ausführung s...                 Bösch 21   \n",
       "2  Führung von Hotels , Restaurants und Barbetrie...         Pilatusstrasse 1   \n",
       "3  Die Gesellschaft übt die Funktion einer Holdin...          Zürichstrasse 5   \n",
       "4   exploitation d'un Food Truck , cuisine ambulante  Route de Bourguillon 19   \n",
       "\n",
       "  current_town  ...  dominant_language class_1_label section_1_label  \\\n",
       "0    Maisprach  ...                NaN        7022.0               M   \n",
       "1    Hünenberg  ...                NaN        4333.0               F   \n",
       "2       Luzern  ...                NaN        7022.0               M   \n",
       "3       Luzern  ...                NaN        6420.0               K   \n",
       "4        Marly  ...                NaN        5610.0               I   \n",
       "\n",
       "  prediction_1_score  class_2_label  section_2_label  prediction_2_score  \\\n",
       "0           0.318912         7021.0                M            0.096343   \n",
       "1           0.186411         4399.0                F            0.125374   \n",
       "2           0.267851         8299.0                N            0.093203   \n",
       "3           0.131271         6820.0                L            0.109448   \n",
       "4           0.301125         5629.0                I            0.130845   \n",
       "\n",
       "  class_3_label section_3_label prediction_3_score  \n",
       "0        7490.0               M           0.082247  \n",
       "1        4391.0               F           0.070668  \n",
       "2        7021.0               M           0.091909  \n",
       "3        6430.0               K           0.103592  \n",
       "4        5621.0               I           0.082071  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_startups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == df_startups.ehraid.nunique()\n",
    "original_len = len(df_startups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS OUTPUT FEATURES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INVOLUNTARY EXIT TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    all_exits = read_from_database(con, \"SELECT ehraid, shab_date AS exit_date FROM zefix.shab WHERE shab_id IN (SELECT shab_id FROM zefix.shab_mutation WHERE description = 'status.aufl' OR description = 'status.loeschung')\")\n",
    "    bankruptcies = read_from_database(con, \"SELECT ehraid, shab_date AS exit_date FROM zefix.shab WHERE shab_id IN (SELECT shab_id FROM zefix.shab_mutation WHERE description = 'status.aufl.konk')\")\n",
    "    liquidations = read_from_database(con, \"SELECT ehraid, shab_date AS exit_date FROM zefix.shab WHERE shab_id IN (SELECT shab_id FROM zefix.shab_mutation WHERE description = 'status.aufl.liq')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates, keep earliest\n",
    "\n",
    "all_exits['exit_date'] = pd.to_datetime(all_exits['exit_date'])\n",
    "bankruptcies['exit_date'] = pd.to_datetime(bankruptcies['exit_date'])\n",
    "liquidations['exit_date'] = pd.to_datetime(liquidations['exit_date'])\n",
    "\n",
    "all_exits = all_exits.sort_values('exit_date').drop_duplicates(subset=['ehraid'], keep='first')\n",
    "bankruptcies = bankruptcies.sort_values('exit_date').drop_duplicates(subset=['ehraid'], keep='first')\n",
    "liquidations = liquidations.sort_values('exit_date').drop_duplicates(subset=['ehraid'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add exit date\n",
    "df_startups = df_startups.merge(all_exits, on='ehraid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['target_inv_exit'] = 0\n",
    "\n",
    "# Set target_inv_exit to 1 for all bankruptcies and liquidations that happened in within 5 years after founding\n",
    "condition_a = (df_startups['ehraid'].isin(bankruptcies.ehraid) | df_startups['ehraid'].isin(liquidations.ehraid))  # Bankrupt or liquidated\n",
    "condition_b = (df_startups['exit_date'] - df_startups['founding_date']) <= pd.Timedelta(days=5*365)  # Event within the first 5 years after founding\n",
    "df_startups.loc[condition_a & condition_b, 'target_inv_exit'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14187719440758892"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_startups[df_startups['target_inv_exit'] == 1]) / len(df_startups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACQUISITION TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidation mergers are determined statistically via a name similarity index\n",
    "# The cutoff is set to 0.7\n",
    "\n",
    "query_merger = \"\"\" \n",
    "    SELECT ehraid_acquiree AS ehraid, merger_date FROM zefix.merger_relation WHERE merger_date > '2016-01-01' AND name_similarity < 0.7;\n",
    "\"\"\"\n",
    "\n",
    "with connect_database() as con:\n",
    "    df_merger = read_from_database(connection=con, query=query_merger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates, keep earliest\n",
    "\n",
    "df_merger['merger_date'] = pd.to_datetime(df_merger['merger_date'])\n",
    "df_merger = df_merger.sort_values('merger_date').drop_duplicates(subset=['ehraid'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add merger date\n",
    "df_startups = df_startups.merge(df_merger, on='ehraid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['target_acquisition'] = 0\n",
    "\n",
    "condition_a = df_startups['ehraid'].isin(df_merger.ehraid)  # Merged\n",
    "condition_b = (df_startups['merger_date'] - df_startups['founding_date']) <= pd.Timedelta(days=5*365)  # Event within the first 5 years after founding\n",
    "df_startups.loc[condition_a & condition_b, 'target_acquisition'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006444450461671766"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_startups[df_startups['target_acquisition'] == 1]) / len(df_startups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NON-GOV. INVESTMENT TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funding = pd.read_csv(PROCESSED_DATA_DIR / 'funding_data' / 'startup-ch_funding.csv', usecols=['ehraid', 'date'])\n",
    "df_funding = df_funding[~df_funding.ehraid.isna()].copy()\n",
    "df_funding.rename(columns={'date': 'investment_date'}, inplace=True)\n",
    "df_funding['ehraid'] = df_funding['ehraid'].astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates, keep earliest\n",
    "\n",
    "df_funding['investment_date'] = pd.to_datetime(df_funding['investment_date'])\n",
    "df_funding = df_funding.sort_values('investment_date').drop_duplicates(subset=['ehraid'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add investment date\n",
    "df_startups = df_startups.merge(df_funding, on='ehraid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['target_non_gov_investment'] = 0\n",
    "\n",
    "condition_a = df_startups['ehraid'].isin(df_funding.ehraid)  # Received funding\n",
    "condition_b = (df_startups['investment_date'] - df_startups['founding_date']) <= pd.Timedelta(days=5*365)  # Event within the first 5 years after founding\n",
    "df_startups.loc[condition_a & condition_b, 'target_non_gov_investment'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005126677678192666"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_startups[df_startups['target_non_gov_investment'] == 1]) / len(df_startups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INNOVATION SUBSIDY TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inno = pd.read_csv(PROCESSED_DATA_DIR / 'funding_data' / 'innosuisse_grants.csv', usecols=['ehraid', 'start_date'])\n",
    "df_inno = df_inno[~df_inno.ehraid.isna()].copy()\n",
    "df_inno.rename(columns={'start_date': 'subsidy_date'}, inplace=True)\n",
    "df_inno['ehraid'] = df_inno['ehraid'].astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates, keep earliest\n",
    "\n",
    "df_inno['subsidy_date'] = pd.to_datetime(df_inno['subsidy_date'])\n",
    "df_inno = df_inno.sort_values('subsidy_date').drop_duplicates(subset=['ehraid'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add investment date\n",
    "df_startups = df_startups.merge(df_inno, on='ehraid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['target_inno_subsidy'] = 0\n",
    "\n",
    "condition_a = df_startups['ehraid'].isin(df_inno.ehraid)  # Received innovation subsidy\n",
    "condition_b = (df_startups['subsidy_date'] - df_startups['founding_date']) <= pd.Timedelta(days=5*365)  # Event within the first 5 years after founding\n",
    "df_startups.loc[condition_a & condition_b, 'target_inno_subsidy'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00817741193035661"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_startups[df_startups['target_inno_subsidy'] == 1]) / len(df_startups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == original_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS INPUT FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE BASIC FIRM FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encode NOGA hierarchy levels\n",
    "for i, col in enumerate(['class_1_label', 'class_2_label', 'class_3_label'], start=1):\n",
    "    # Ensure all values are 4-digit strings with leading zeros if needed\n",
    "    padded_str = df_startups[col].astype(int).astype(str).str.zfill(4)\n",
    "    \n",
    "    # Extract division (first 2 digits) and group (first 3 digits), then convert back to float\n",
    "    df_startups[f'division_{i}_label'] = padded_str.str[:2].astype(float)\n",
    "    df_startups[f'group_{i}_label'] = padded_str.str[:3].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == df_startups.ehraid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE ADDRESS FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_spatial_features(df, lat_col='latitude', lon_col='longitude', founding_col='founding_date', exit_col='exit_date'):\n",
    "    # Ensure date columns are datetime\n",
    "    df = df.copy()\n",
    "    df[founding_col] = pd.to_datetime(df[founding_col])\n",
    "    df[exit_col] = pd.to_datetime(df[exit_col])\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[lon_col], df[lat_col]),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    gdf = gdf.to_crs(epsg=3857)\n",
    "    sindex = gdf.sindex\n",
    "\n",
    "    same_address_counts = []\n",
    "    firms_within_1km = []\n",
    "    firms_within_2_5km = []\n",
    "    firms_within_10km = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        founding_date_i = row[founding_col]\n",
    "\n",
    "        # -- Same address (within 10m)\n",
    "        for radius, store in [\n",
    "            (10, same_address_counts),\n",
    "            (1000, firms_within_1km),\n",
    "            (2500, firms_within_2_5km),\n",
    "            (10_000, firms_within_10km)\n",
    "        ]:\n",
    "            possible_matches_idx = list(sindex.intersection(row.geometry.buffer(radius).bounds))\n",
    "            nearby = gdf.iloc[possible_matches_idx]\n",
    "            candidates = nearby[\n",
    "                (nearby[founding_col] <= founding_date_i) &\n",
    "                (\n",
    "                    nearby[exit_col].isna() |\n",
    "                    (nearby[exit_col] > founding_date_i)\n",
    "                )\n",
    "            ]\n",
    "            n_firms = candidates[candidates.geometry.distance(row.geometry) <= radius].shape[0] - 1  # exclude self\n",
    "            store.append(n_firms)\n",
    "\n",
    "    gdf['n_firms_within_10m'] = same_address_counts\n",
    "    gdf['n_firms_within_1km'] = firms_within_1km\n",
    "    gdf['n_firms_within_2.5km'] = firms_within_2_5km\n",
    "    gdf['n_firms_within_10km'] = firms_within_10km\n",
    "\n",
    "    return gdf.drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two spacial variables for\n",
    "# 1. Number of firms (including the firm itself) at the same location (within 10 meters)\n",
    "# 2. Number of firms (including the firm itself) within 1km distance\n",
    "# Both are calculated only considering existing firms at the time of founding!\n",
    "\n",
    "df_startups = encode_spatial_features(df_startups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == df_startups.ehraid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE FIRM NAME FEATUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "token_classifier = pipeline(\n",
    "  model=\"ZurichNLP/swissbert-ner\",\n",
    "  aggregation_strategy=\"simple\",\n",
    "  device='mps'\n",
    ")\n",
    "\n",
    "UMLAUT_REPLACEMENTS = {\n",
    "    'ä': 'ae',\n",
    "    'ö': 'oe',\n",
    "    'ü': 'ue',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_words(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    for char, replacement in UMLAUT_REPLACEMENTS.items():\n",
    "        text = text.replace(char, replacement)\n",
    "    return unidecode(text.lower())\n",
    "\n",
    "def get_language(text: str) -> str:\n",
    "    language = detect(text)\n",
    "    return language.get('lang', 'de')\n",
    "\n",
    "def stem_text(text: str, lang_code: str) -> str:\n",
    "    code2lang = {\n",
    "        'de': 'german',\n",
    "        'en': 'english',\n",
    "        'fr': 'french',\n",
    "        'it': 'italian'\n",
    "    }\n",
    "    language = code2lang.get(lang_code, 'german')\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    return [porter_stemmer.stem(w) for w in tokens if not w.lower() in stop_words]\n",
    "\n",
    "def batch_ner_tag_bps(texts: list[str], token_classifier: pipeline):\n",
    "    batch_outputs = token_classifier(texts, batch_size=32)\n",
    "    results = []\n",
    "    for output in batch_outputs:\n",
    "        people = [entry['word'] for entry in output if entry['entity_group'] == 'PER']\n",
    "        locations = [entry['word'] for entry in output if entry['entity_group'] == 'LOC']\n",
    "        results.append((people, locations))\n",
    "    return results\n",
    "\n",
    "def process_df_with_ner(df_lang: pd.DataFrame, token_classifier: pipeline, batch_size: int):\n",
    "    texts = df_lang['founding_purpose'].fillna(\"\").tolist()\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        results.extend(batch_ner_tag_bps(batch, token_classifier))\n",
    "\n",
    "    df_lang[['people', 'locations']] = pd.DataFrame(results, index=df_lang.index)\n",
    "    return df_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "raumgliederungen = pd.read_excel(EXTERNAL_DATA_DIR / 'geo_data' / 'Raumgliederungen.xlsx')\n",
    "municipality_names = [re.sub(r'\\(.*?\\)', '', normalize_words(name)).strip() for name in raumgliederungen['Gemeindename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    gendered_names = read_from_database(con, \"SELECT * FROM zefix.founders_gender_mapping WHERE gender != 'u' AND request_type = 'first_name' AND probability >= 0.95\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_names['split_names'] = gendered_names['name'].str.split()\n",
    "\n",
    "certain_gender = gendered_names[gendered_names['split_names'].apply(len) == 1]\n",
    "certain_gender = certain_gender[certain_gender['name'].str.isalpha() & (certain_gender['name'].str.len() >= 4)].copy()\n",
    "\n",
    "female_names = set(certain_gender[certain_gender['gender'] == 'f']['name'].apply(normalize_words))\n",
    "male_names = set(certain_gender[certain_gender['gender'] == 'm']['name'].apply(normalize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get external data to create firm name and BPS features (e.g. includes female name)\n",
    "official_male_names = pd.read_csv(EXTERNAL_DATA_DIR / 'newborn_names' / 'maennliche_vornamen.csv', encoding='ISO-8859-15', usecols=['Vorname'])\n",
    "official_female_names = pd.read_csv(EXTERNAL_DATA_DIR / 'newborn_names' / 'weibliche_vornamen.csv', encoding='ISO-8859-15', usecols=['Vorname'])\n",
    "\n",
    "official_male_names = set([normalize_words(name) for name in official_male_names['Vorname']])\n",
    "official_female_names = set([normalize_words(name) for name in official_female_names['Vorname']])\n",
    "\n",
    "female_names = female_names.union(official_female_names)\n",
    "male_names = male_names.union(official_male_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7217\n",
      "10473\n"
     ]
    }
   ],
   "source": [
    "print(len(female_names))\n",
    "print(len(male_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Firm name length\n",
    "df_startups['firm_name_length'] = df_startups['founding_name'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['founding_name_norm'] = df_startups['founding_name'].apply(normalize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Contains swiss reference\n",
    "swiss_terms = [\n",
    "    \"switzerland\",\n",
    "    \"swiss\", \n",
    "    \"schweiz\",  # Covers \"schweiz\", \"schweizer\", \"schweizerische\", etc.\n",
    "    \"swi\",  # e.g. swica\n",
    "    \"sui\",\n",
    "    \"suisse\",\n",
    "    \"helvet\",  # Covers Helvetia, Helvetica, etc.\n",
    "    \"confed\",  # Covers Confederation, \"confédération\", \"confederazione\"\n",
    "    \"sviz\",  # Covers \"Svizzera\", \"Svizzero\", \"Svizzere\", \"Svizzeri\", \"svizra\", etc.\n",
    "    \"eidgen\",  # Covers \"eidgenossenschaft\", \"eidgenössisch\",\n",
    "]\n",
    "\n",
    "df_startups['firm_name_swiss_ref'] = df_startups['founding_name_norm'].str.contains('|'.join(swiss_terms)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Contains holding reference\n",
    "holding_terms = [\n",
    "    \"holding\",\n",
    "    \"beteiligung\",\n",
    "    \"participation\",\n",
    "    \"partecipazion\",\n",
    "    \"anteil\",\n",
    "    \"capital\",\n",
    "    \"kapital\",\n",
    "    \"invest\",\n",
    "    \"share\",\n",
    "    \"aktie\",\n",
    "    \"action\",\n",
    "    \"azion\"\n",
    "]\n",
    "\n",
    "df_startups['firm_name_holding_ref'] = df_startups['founding_name_norm'].str.contains('|'.join(holding_terms)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Contains geographic term (municipality name)\n",
    "municipality_set = set(municipality_names)\n",
    "def has_geographic_term(firm_name, geo_terms):\n",
    "    tokens = [t for t in firm_name.split() if len(t) > 2]\n",
    "    return int(any(tok in geo_terms for tok in tokens))\n",
    "    \n",
    "df_startups['firm_name_geog_ref'] = df_startups['founding_name_norm'].apply(lambda x: has_geographic_term(x, municipality_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Contains founder names\n",
    "def has_founder_name(firm_name, founders):\n",
    "    if founders:\n",
    "        founder_name_list = []\n",
    "        for fn, ln in founders.get('founder_names', []):\n",
    "            fn = [n for n in re.split(r'[- ]', fn) if len(n) > 2]  # Avoid really short names to decrease likelihood of false positive\n",
    "            ln = [n for n in re.split(r'[- ]', ln) if len(n) > 2]\n",
    "            founder_name_list.extend(fn + ln)\n",
    "        return int(any(name in firm_name for name in founder_name_list))\n",
    "    return 0\n",
    "\n",
    "df_startups['firm_name_founder_match'] = df_startups.apply(lambda row: has_founder_name(row['founding_name_norm'], founder_lists.get(row['ehraid'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Contains gendered name\n",
    "def contains_male_or_female_name(names: list[str] | str, gendered_first_names):\n",
    "    if isinstance(names, list):\n",
    "        names = [n for name in names for n in name.split() if len(n) > 2]\n",
    "    elif isinstance(names, str):\n",
    "        names = names.split()\n",
    "    for name in names:\n",
    "        gender = name in gendered_first_names\n",
    "        if gender:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "df_startups['firm_name_male_match'] = df_startups['founding_name_norm'].apply(lambda x: contains_male_or_female_name(x, male_names))\n",
    "df_startups['firm_name_female_match'] = df_startups['founding_name_norm'].apply(lambda x: contains_male_or_female_name(x, female_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == df_startups.ehraid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE FOUNDER FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/6qys5jn501gbqmkkkgmvpf0h0000gn/T/ipykernel_580/1777951682.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_startups['n_inscribed_firms'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. Encode number of founders\n",
    "df_startups['n_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_founders', 1))\n",
    "df_startups.loc[(df_startups['n_founders'] == 0) | (df_startups['n_founders'].isna()), 'n_founders'] = 1  # Firm must have at least one founder\n",
    "\n",
    "# 2. Encode number of inscribed firms\n",
    "df_startups = df_startups.merge(grouped_insc_firms, on='ehraid', how='left')\n",
    "df_startups['founding_date'] = pd.to_datetime(df_startups['founding_date'])\n",
    "df_startups['firm_inscription_date'] = pd.to_datetime(df_startups['firm_inscription_date'])\n",
    "\n",
    "df_startups.loc[df_startups['firm_inscription_date'] > df_startups['founding_date'], 'n_inscribed_firms'] = 0\n",
    "df_startups.drop(columns=['firm_inscription_date'], inplace=True)\n",
    "df_startups['n_inscribed_firms'].fillna(0, inplace=True)\n",
    "\n",
    "# 3. Percentage of female founders\n",
    "df_startups['n_female_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_female_founders', np.nan))\n",
    "\n",
    "# 4. Number of distinct nationalities\n",
    "def get_distinct_nationalities(founder_data):\n",
    "    if not founder_data:\n",
    "        return 0\n",
    "    return len(set([nat for nationalities in founder_data.get('founder_nationalities', []) for nat in nationalities]))\n",
    "\n",
    "df_startups['n_distinct_nationalities'] = df_startups['ehraid'].apply(lambda ehraid: get_distinct_nationalities(founder_lists.get(ehraid)))\n",
    "\n",
    "# 5. Number of Swiss founders\n",
    "df_startups['n_swiss_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_swiss_founders', np.nan))\n",
    "\n",
    "# 6. Number of foreign founders\n",
    "df_startups['n_foreign_founders'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_foreign_founders', np.nan))\n",
    "\n",
    "# 7. Number of founders with Dr. PhD. Prof. in name\n",
    "df_startups['n_dr_titles'] = df_startups['ehraid'].apply(lambda x: count_stats.get(x, {}).get('n_dr_titles', np.nan))\n",
    "\n",
    "# 8. Founders with same municipality than firm\n",
    "def get_residence_match(firm_bfs_code, founder_data):\n",
    "    if not founder_data or pd.isna(firm_bfs_code):\n",
    "        return 0\n",
    "    count = 0\n",
    "    for residencies in founder_data.get('founder_residencies', []):\n",
    "        count += int(firm_bfs_code in residencies)\n",
    "    return count\n",
    "\n",
    "df_startups['n_founders_same_residence'] = df_startups.apply(lambda row: get_residence_match(row['founding_bfs_code'], founder_lists.get(row['ehraid'])), axis=1)\n",
    "\n",
    "df_startups.fillna({\n",
    "    'n_inscribed_firms': 0,\n",
    "    'n_female_founders': 0,\n",
    "    'n_swiss_founders': 0,\n",
    "    'n_foreign_founders': 0,\n",
    "    'n_dr_titles': 0,\n",
    "    'n_founders_same_residence': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df_startups['pct_female_founders'] = df_startups['n_female_founders'] / df_startups['n_founders']\n",
    "df_startups['pct_swiss_founders'] = df_startups['n_swiss_founders'] / df_startups['n_founders']\n",
    "df_startups['pct_foreign_founders'] = df_startups['n_foreign_founders'] / df_startups['n_founders']\n",
    "df_startups['pct_dr_titles'] = df_startups['n_dr_titles'] / df_startups['n_founders']\n",
    "df_startups['pct_founders_same_residence'] = df_startups['n_founders_same_residence'] / df_startups['n_founders']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110793/110793 [02:33<00:00, 719.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# 9. Encode prior founding experience\n",
    "df_startups['founder_fids'] = df_startups['ehraid'].apply(lambda x: founder_lists.get(x, {}).get('founder_fids', []))\n",
    "\n",
    "prior_foundings = {}\n",
    "for ehraid, data in founder_lists.items():\n",
    "    founders = data.get('founder_fids', [])\n",
    "    for fid in founders:\n",
    "        if fid not in prior_foundings:\n",
    "            prior_foundings[fid] = [ehraid]\n",
    "        else:\n",
    "            prior_foundings[fid].append(ehraid)\n",
    "\n",
    "prior_founding = []\n",
    "prior_failed = []\n",
    "prior_existing = []\n",
    "for i, row in tqdm(df_startups.iterrows(), total=df_startups.shape[0]):\n",
    "    foundings = 0\n",
    "    failed = set()\n",
    "    existing = set()\n",
    "    for fid in row['founder_fids']:\n",
    "        matches = df_startups[\n",
    "            df_startups['ehraid'].isin(prior_foundings[fid])\n",
    "            & (df_startups['ehraid'] != row['ehraid'])\n",
    "            & (df_startups['founding_date'] <= row['founding_date'])]\n",
    "        if matches.empty:\n",
    "            continue\n",
    "        foundings += 1\n",
    "        failed_ehraids = matches[matches['exit_date'] < row['founding_date']]['ehraid'].tolist()\n",
    "        for eid in failed_ehraids:\n",
    "            failed.add(eid)\n",
    "        existing_ehraids = [ehraid for ehraid in matches['ehraid'] if ehraid not in failed_ehraids]\n",
    "        for eid in existing_ehraids:\n",
    "            existing.add(eid)\n",
    "\n",
    "    prior_founding.append(foundings)\n",
    "    prior_failed.append(len(failed))\n",
    "    prior_existing.append(len(existing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups['n_founders_with_prior_founding'] = prior_founding\n",
    "df_startups['pct_founders_with_prior_founding'] = df_startups['n_founders_with_prior_founding'] / df_startups['n_founders']\n",
    "df_startups['n_dissolved_firms'] = prior_failed\n",
    "df_startups['n_existing_firms'] = prior_existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == df_startups.ehraid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE BPS FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups.loc[df_startups.founding_purpose.isna(), 'founding_purpose'] = df_startups['current_purpose']  # Use current purpose if founding purpose is missing\n",
    "df_startups['bps_language'] = df_startups['founding_purpose'].apply(get_language)\n",
    "\n",
    "corrections = {\n",
    "    'cs': 'de',\n",
    "    'en': 'de',\n",
    "    'sv': 'de',\n",
    "    'es': 'fr',\n",
    "    'pt': 'de',\n",
    "    'pl': 'de',\n",
    "    'nl': 'de',\n",
    "    'ca': 'it',\n",
    "    'et': 'de'\n",
    "}\n",
    "\n",
    "df_startups['bps_language'] = df_startups['bps_language'].replace(corrections)\n",
    "df_startups.loc[~df_startups['bps_language'].isin(['de', 'fr', 'it']), 'bps_language'] = 'de'  # If not de, fr, it assume it as the majority language (German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stop-word removal and stemming\n",
    "df_startups['bps_normalized'] = df_startups.apply(lambda row: stem_text(row['founding_purpose'], lang_code=row['bps_language']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Encode bps length\n",
    "df_startups['bps_length'] = df_startups['bps_normalized'].apply(lambda x: len(' '.join(x)))  # bps length\n",
    "\n",
    "# 2. Encode mean word length\n",
    "df_startups['bps_mean_word_length'] = [(1 / len(word_list)) * np.array([len(w) for w in word_list]).sum() for word_list in df_startups['bps_normalized']]  # average word length per bps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get quantiles of length metrics\n",
    "def get_quantiles(df: pd.DataFrame, column: str, q: list[float] = [.2, .4, .6, .8]) -> int:\n",
    "    quantiles = df[column].quantile(q)\n",
    "    def get_numeric_quintile(target: float, quantiles: pd.Series):\n",
    "        for i, tau in enumerate(quantiles, start=1):\n",
    "            if target <= tau:\n",
    "                return i\n",
    "        return len(quantiles) + 1\n",
    "    df[f'{column}_quantiles_{len(quantiles) + 1}'] = df[column].apply(lambda x: get_numeric_quintile(x, quantiles))\n",
    "    return df\n",
    "\n",
    "\n",
    "df_startups = get_quantiles(df_startups, 'bps_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate LIX\n",
    "def calculate_lix(word_list: list[str]):\n",
    "    pct_above_six = len([w for w in word_list if len(w) > 6])  / len(word_list) * 100\n",
    "    return len(word_list) + pct_above_six\n",
    "\n",
    "df_startups['bps_lix'] = df_startups['bps_normalized'].apply(calculate_lix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Word-frequency Features\n",
    "word_dictionary_de = [token for sublist in df_startups[df_startups['bps_language'] == 'de']['bps_normalized'] for token in sublist if token.isalpha() and len(token) > 4]\n",
    "word_freqencies_de = Counter(word_dictionary_de)\n",
    "\n",
    "word_dictionary_fr = [token for sublist in df_startups[df_startups['bps_language'] == 'fr']['bps_normalized'] for token in sublist if token.isalpha() and len(token) > 4]\n",
    "word_freqencies_fr = Counter(word_dictionary_fr)\n",
    "\n",
    "word_dictionary_it = [token for sublist in df_startups[df_startups['bps_language'] == 'it']['bps_normalized'] for token in sublist if token.isalpha() and len(token) > 4]\n",
    "word_freqencies_it = Counter(word_dictionary_it)\n",
    "\n",
    "def compute_specificity_features(bps_tokens: list[str], lang_code: str) -> tuple[float, float, float]:\n",
    "    if not isinstance(bps_tokens, list):\n",
    "        print(bps_tokens)\n",
    "    if lang_code == 'fr':\n",
    "        word_freqencies = word_freqencies_fr\n",
    "    elif lang_code == 'it':\n",
    "        word_freqencies = word_freqencies_it\n",
    "    else:\n",
    "        word_freqencies = word_freqencies_de\n",
    "    token_freqs = [word_freqencies[token] for token in bps_tokens if token in word_freqencies]\n",
    "    if not token_freqs:\n",
    "        return (0, 0, 0.0)\n",
    "\n",
    "    total_tokens = sum(word_freqencies.values())\n",
    "    # Normalize it by total number of tokens to account for differences between languages\n",
    "    min_freq_norm, max_freq_norm = min(token_freqs) / total_tokens, max(token_freqs) / total_tokens\n",
    "\n",
    "    ratio = min_freq_norm / max_freq_norm if max_freq_norm > 0 else 0.0\n",
    "    return (min_freq_norm, max_freq_norm, ratio)\n",
    "\n",
    "df_startups[['bps_min_word_freq_norm', 'bps_max_word_freq_norm', 'bps_freq_ratio_norm']] = df_startups.apply(lambda row: compute_specificity_features(row['bps_normalized'], row['bps_language']), axis=1).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 580/580 [12:56<00:00,  1.34s/it]\n",
      "100%|██████████| 233/233 [05:05<00:00,  1.31s/it]\n",
      "100%|██████████| 54/54 [02:22<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# 5. Encode geographic and name features\n",
    "dfs = []\n",
    "for language in ['de', 'fr', 'it']:\n",
    "    token_classifier.model.set_default_language(f\"{language}_CH\")\n",
    "    df_lang = df_startups[df_startups['bps_language'] == language].copy()\n",
    "    df_lang = process_df_with_ner(df_lang, token_classifier, batch_size=128)\n",
    "\n",
    "    df_lang['bps_geographic_term'] = df_lang['locations'].apply(lambda x: int(len(x) > 0))\n",
    "    df_lang['people'] = df_lang['people'].apply(lambda names: [normalize_words(name) for name in names])\n",
    "    df_lang['bps_male_name'] = df_lang['people'].apply(lambda x: contains_male_or_female_name(x, male_names))\n",
    "    df_lang['bps_female_name'] = df_lang['people'].apply(lambda x: contains_male_or_female_name(x, female_names))\n",
    "\n",
    "    dfs.append(df_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups[['bps_geographic_term', 'bps_male_name', 'bps_female_name']] = df_startups[[\n",
    "    'bps_geographic_term', 'bps_male_name', 'bps_female_name'\n",
    "]].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == df_startups.ehraid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD CONTROL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of days that we have a history to inform the model\n",
    "# about missing history for variables depending on the history (e.g. n-firms at address, prior founding experiance)\n",
    "\n",
    "df_startups['founding_year'] = df_startups['founding_date'].dt.year\n",
    "\n",
    "min_date = df_startups['founding_date'].agg('min')\n",
    "df_startups['days_of_prior_observations'] = (df_startups['founding_date'] - min_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_startups) == df_startups.ehraid.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_startups.to_csv(RAW_DATA_DIR / 'company_sample' / 'until_2020' / '2020_sample_encoded_features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
