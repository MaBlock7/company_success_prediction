{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from unidecode import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pocketknife.database import (connect_database, read_from_database)\n",
    "\n",
    "from success_prediction.config import RAW_DATA_DIR, EXTERNAL_DATA_DIR, PROCESSED_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA FROM ZEFIX TO ATTACH THE EHRAID AND UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    df = read_from_database(\n",
    "        con,\n",
    "        query=\"\"\"\n",
    "            SELECT * FROM zefix.base base WHERE\n",
    "                NOT base.is_branch\n",
    "                AND NOT base.legal_form_id IN (9, 11, 18, 19)\n",
    "                AND LOWER(base.name) NOT LIKE '%zweigniederlassung%'\n",
    "                AND LOWER(base.name) NOT LIKE '%succursale%'\n",
    "                AND (base.delete_date >= '2016-01-01' OR base.delete_date IS NULL);\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "with connect_database() as con:\n",
    "    df_previous_names = read_from_database(con, \"SELECT * FROM zefix.old_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine base data with previous names for the matching\n",
    "\n",
    "df_grouped = df_previous_names.groupby('ehraid')['name'].apply(lambda x: [el for el in x]).reset_index()\n",
    "df_grouped = df_grouped.rename(columns={'name': 'old_names'})\n",
    "\n",
    "df = df.merge(df_grouped, on=['ehraid'], how='left')\n",
    "df = df.rename(columns={'legal_seat': 'city'})\n",
    "\n",
    "df['all_names'] = [old_names + [current_name] if isinstance(old_names, list) else [current_name] for old_names, current_name in zip(df['old_names'], df['name'])]\n",
    "df = df.explode(column='all_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the firm names before matching\n",
    "\n",
    "df['firm_name_norm'] = (df['all_names']\n",
    "    .str.lower()\n",
    "    .str.replace('in liquidation', '')\n",
    "    .str.replace('en liquidation', '')\n",
    "    .str.replace('in liquidazione', '')\n",
    "    .str.strip()\n",
    ")\n",
    "df['firm_name_norm'] = df['firm_name_norm'].apply(unidecode).apply(lambda x: ' '.join([el for el in x.split()]))\n",
    "df['firm_name_clean'] = (df['firm_name_norm']\n",
    "    .str.replace('.', '', regex=False)\n",
    "    .str.replace(r' ag$', '', regex=True)\n",
    "    .str.replace(r' sa$', '', regex=True)\n",
    "    .str.replace(r' gmbh$', '', regex=True)\n",
    "    .str.replace(r' sarl$', '', regex=True)\n",
    "    .str.replace(r' ltd$', '', regex=True)\n",
    "    .str.replace(r' llc$', '', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "df['firm_name_clean'] = df['firm_name_clean'].str.replace(' ', '', regex=False)\n",
    "\n",
    "df = df.drop_duplicates(subset=['firm_name_norm', 'uid'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET FUNDING DATA FROM STARTUP.CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "cookies = {\n",
    "    \"CFID\": \"22868597\",\n",
    "    \"CFTOKEN\": \"c03f47d2f643a0ec-FFC1597B-0A9F-6C03-1550C3A2498E7C58\",\n",
    "    \"CFGLOBALS\": \"urltoken%3DCFID%23%3D22868597%26CFTOKEN%23%3Dc03f47d2f643a0ec%2DFFC1597B%2D0A9F%2D6C03%2D1550C3A2498E7C58%23lastvisit%3D%7Bts%20%272025%2D04%2D14%2009%3A54%3A26%27%7D%23hitcount%3D114%23timecreated%3D%7Bts%20%272025%2D01%2D13%2016%3A23%3A03%27%7D%23cftoken%3Dc03f47d2f643a0ec%2DFFC1597B%2D0A9F%2D6C03%2D1550C3A2498E7C58%23cfid%3D22868597%23\"\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for list_idx, page_idx in zip(range(1, 100_000, 10), range(1, 10_000)):\n",
    "    url = f\"https://www.startup.ch/index.cfm?cfid=22868597&cftoken=c03f47d2f643a0ec-FFC1597B-0A9F-6C03-1550C3A2498E7C58&bericht_id=10000&start_liste_10000={list_idx}&bericht_seite_10000={page_idx}&page=137906#fgBerichtAnker_10000\"\n",
    "    response = requests.get(url, headers=headers, cookies=cookies)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        table_div = soup.find(\"div\", class_=\"founding-table\")\n",
    "        table = table_div.find(\"table\") if table_div else None\n",
    "\n",
    "        if table:\n",
    "            dfs.append(pd.read_html(StringIO(str(table)))[0])\n",
    "        else:\n",
    "            print(f\"Table not found or not accessible for {url}\")\n",
    "    else:\n",
    "        print(f\"Page not found: {url}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "combined['DATE'] = combined['DATE'].str.replace('1900', '2019')\n",
    "combined['DATE'] = pd.to_datetime(combined['DATE'], format='%d.%m.%Y')\n",
    "combined = combined.dropna(subset='DATE')\n",
    "combined = combined.set_index('DATE')\n",
    "combined = combined.sort_index()\n",
    "filtered = combined['2016-01-01':]\n",
    "filtered = filtered.drop_duplicates().reset_index()\n",
    "filtered.columns = ['date', 'firm_name', 'amount_chf', 'type', 'location']\n",
    "filtered.to_csv(EXTERNAL_DATA_DIR / 'startup-ch_funding.csv', index=False)\n",
    "\n",
    "filtered['firm_name_norm'] = filtered['firm_name'].str.lower().apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "filtered['firm_name_clean'] = filtered['firm_name_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge startup.ch data with zefix data\n",
    "merged = filtered.merge(df[['firm_name_norm', 'ehraid', 'uid', 'legal_seat']], on=['firm_name_norm'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = merged[~merged.ehraid.isna()]\n",
    "missing = merged[merged.ehraid.isna()].drop(columns=['ehraid', 'uid'])\n",
    "missing = missing.merge(df[['firm_name_clean', 'ehraid', 'uid']], on=['firm_name_clean'], how='left')\n",
    "\n",
    "matched = pd.concat([matched, missing[~missing.ehraid.isna()]])\n",
    "missing = missing[missing.ehraid.isna()].sort_values(['firm_name'])\n",
    "\n",
    "duplicates = matched[matched.duplicated(subset=['date', 'firm_name', 'type', 'location'], keep=False)].sort_values(['firm_name', 'ehraid'])\n",
    "\n",
    "unique_matched = matched[~matched.duplicated(subset=['date', 'firm_name', 'type', 'location'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_matched.to_csv(EXTERNAL_DATA_DIR / 'uniquely_matched.csv', index=False)\n",
    "duplicates.to_csv(EXTERNAL_DATA_DIR / 'duplicates.csv', index=False)\n",
    "missing.to_csv(EXTERNAL_DATA_DIR / 'missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD SCRAPED INNOSUISSE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inno_df = pd.read_csv(RAW_DATA_DIR / 'funding_data' / 'innosuisse_grants.csv')\n",
    "\n",
    "for col in ['contact_person', 'scientific_management', 'implementation_partner']:\n",
    "    inno_df[col] = inno_df[col].apply(ast.literal_eval)\n",
    "\n",
    "for col in ['start_date', 'end_date']:\n",
    "    inno_df[col] = pd.to_datetime(inno_df[col])\n",
    "\n",
    "inno_df = inno_df[(inno_df['implementation_partner'].apply(bool)) & (inno_df['start_date'] >= datetime(2016, 1, 1))]\n",
    "\n",
    "inno_df = inno_df.explode(column=['implementation_partner'])\n",
    "\n",
    "inno_df['uid'] = inno_df['implementation_partner'].apply(lambda x: x.get('uid_1'))\n",
    "inno_df['canton'] = inno_df['implementation_partner'].apply(lambda x: x.get('canton'))\n",
    "inno_df['city'] = inno_df['implementation_partner'].apply(lambda x: x.get('city'))\n",
    "inno_df['zip_code'] = inno_df['implementation_partner'].apply(lambda x: x.get('zip_code'))\n",
    "inno_df['street'] = inno_df['implementation_partner'].apply(lambda x: x.get('street'))\n",
    "\n",
    "inno_df['firm_name_original'] = inno_df['implementation_partner'].apply(lambda x: x.get('company_1'))\n",
    "\n",
    "inno_df['firm_name_norm'] = inno_df['implementation_partner'].apply(lambda x: x.get('company_1'))\n",
    "inno_df['firm_name_combined'] = inno_df['implementation_partner'].apply(lambda x: ' '.join([x.get(col) for col in ['company_1', 'company_2', 'company_3', 'company_4'] if isinstance(x.get(col), str)]))\n",
    "\n",
    "inno_df['firm_name_norm'] = inno_df['firm_name_norm'].str.lower().apply(lambda x: ' '.join([el for el in x.split()]))\n",
    "inno_df['firm_name_norm'] = inno_df['firm_name_norm'].apply(unidecode)\n",
    "\n",
    "inno_df['firm_name_combined'] = inno_df['firm_name_combined'].str.lower().apply(lambda x: ' '.join([el for el in x.split()]))\n",
    "\n",
    "inno_df['firm_name_clean'] = (inno_df['firm_name_norm']\n",
    "    .str.replace('.', '', regex=False)\n",
    "    .str.replace(r' ag$', '', regex=True)\n",
    "    .str.replace(r' sa$', '', regex=True)\n",
    "    .str.replace(r' gmbh$', '', regex=True)\n",
    "    .str.replace(r' sarl$', '', regex=True)\n",
    "    .str.replace(r' ltd$', '', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "inno_df['firm_name_clean'] = inno_df['firm_name_clean'].str.replace(' ', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into entries with UID and entries without\n",
    "inno_df_matched = inno_df[~inno_df.uid.isna()].copy()\n",
    "inno_df_missing = inno_df[inno_df.uid.isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First match on firm_name and city to reduce duplicates\n",
    "inno_df_missing = inno_df_missing.drop(columns=['uid'])\n",
    "inno_df_missing = inno_df_missing.merge(df[['firm_name_norm', 'city', 'uid']], on=['firm_name_norm', 'city'], how='left')\n",
    "\n",
    "inno_df_matched = pd.concat([inno_df_matched, inno_df_missing[~inno_df_missing.uid.isna()]])\n",
    "inno_df_missing = inno_df_missing[inno_df_missing.uid.isna()].sort_values(['firm_name_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then only match remaining only on firm_name\n",
    "inno_df_missing = inno_df_missing.drop(columns=['uid'])\n",
    "inno_df_missing = inno_df_missing.merge(df[['firm_name_norm', 'uid']], on=['firm_name_norm'], how='left')\n",
    "\n",
    "inno_df_matched = pd.concat([inno_df_matched, inno_df_missing[~inno_df_missing.uid.isna()]])\n",
    "inno_df_missing = inno_df_missing[inno_df_missing.uid.isna()].sort_values(['firm_name_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then only match remaining on firm name without common company type indicators (e.g. AG, GmbH, SA, Ltd, etc.)\n",
    "inno_df_missing = inno_df_missing.drop(columns=['uid'])\n",
    "inno_df_missing = inno_df_missing.merge(df[['firm_name_clean', 'uid']], on=['firm_name_clean'], how='left')\n",
    "\n",
    "inno_df_matched = pd.concat([inno_df_matched, inno_df_missing[~inno_df_missing.uid.isna()]])\n",
    "inno_df_missing = inno_df_missing[inno_df_missing.uid.isna()].sort_values(['firm_name_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inno_df_missing.to_csv(EXTERNAL_DATA_DIR / 'innosuisse_missing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the manually matched entries and combine them with the previous entries\n",
    "manuell = pd.read_csv(EXTERNAL_DATA_DIR / 'innosuisse_missing copy.csv')\n",
    "\n",
    "for col in ['start_date', 'end_date']:\n",
    "    manuell[col] = pd.to_datetime(manuell[col])\n",
    "\n",
    "manuell['uid'] = manuell['uid'].str.replace('-', '').str.replace('.', '')\n",
    "inno_df_matched = pd.concat([inno_df_matched, manuell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into uniquely matched and entries with multiple name+uid matches to check them manually\n",
    "inno_duplicates = inno_df_matched[inno_df_matched.duplicated(subset=['project_id', 'firm_name_norm'], keep=False)].sort_values(['project_id', 'firm_name_norm', 'uid'])\n",
    "inno_duplicates = inno_duplicates.drop_duplicates(subset=['project_id', 'uid'], keep='first')\n",
    "\n",
    "inno_df_matched_unique = inno_df_matched[~inno_df_matched.duplicated(subset=['project_id', 'firm_name_norm'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inno_df_matched_unique.to_csv(EXTERNAL_DATA_DIR / 'innosuisse_unique_matches.csv', index=False)\n",
    "inno_duplicates.to_csv(EXTERNAL_DATA_DIR / 'innosuisse_duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined data with attached EHRAID and UID\n",
    "df_final = pd.concat([inno_df_matched_unique, inno_duplicates]).drop_duplicates(subset=['project_id', 'uid'])\n",
    "df_final = df_final.merge(df[~df['uid'].isna()][['uid', 'ehraid']].drop_duplicates(subset=['uid', 'ehraid']), on=['uid'], how='left')\n",
    "\n",
    "cols = [col for col in df_final.columns if col not in ['uid', 'ehraid']] + ['ehraid', 'uid']\n",
    "df_final = df_final[cols].sort_values('project_id')\n",
    "\n",
    "df_final = df_final.drop(columns=['firm_name_norm', 'firm_name_combined', 'firm_name_clean'])\n",
    "\n",
    "df_final.to_csv(PROCESSED_DATA_DIR / 'funding_data' / 'innosuisse_grants.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
