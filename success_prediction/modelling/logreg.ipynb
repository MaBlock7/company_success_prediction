{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from success_prediction.config import RAW_DATA_DIR, PROCESSED_DATA_DIR, MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ly/6qys5jn501gbqmkkkgmvpf0h0000gn/T/ipykernel_59478/3524526548.py:1: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(RAW_DATA_DIR / 'company_sample' / 'until_2020' / '2020_sample_encoded_features.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(RAW_DATA_DIR / 'company_sample' / 'until_2020' / '2020_sample_encoded_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   []\n",
       "1                   []\n",
       "2                   []\n",
       "3                   []\n",
       "4                   []\n",
       "              ...     \n",
       "129561    ['Svizzera']\n",
       "129562              []\n",
       "129563    ['Svizzera']\n",
       "129564    ['Svizzera']\n",
       "129565              []\n",
       "Name: locations, Length: 129566, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['locations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'uid', 'current_name', 'founding_name', 'current_legal_form', 'current_street', 'current_town', 'current_zip_code',\n",
    "    'current_country', 'founding_street', 'founding_town', 'founding_zip_code', 'shab_id', 'founding_message',\n",
    "    'combined_address', 'geocoded_address', 'company_url', 'founding_name_norm', 'people', 'locations', 'current_purpose', 'founding_purpose',\n",
    "    'exit_date', 'merger_date', 'investment_date', 'subsidy_date', 'n_female_founders'\n",
    "]\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\n",
    "\n",
    "    'ehraid',  # Company ID only for identification\n",
    "    \n",
    "    # Targets\n",
    "    'target_inv_exit',\n",
    "    'target_acquisition',\n",
    "    'target_non_gov_investment',\n",
    "    'target_inno_subsidy'\n",
    "\n",
    "    # Basic firm features\n",
    "    'founding_legal_form',  # Cat\n",
    "\n",
    "    'legal_form_binary_3.0',  # Binary\n",
    "    'legal_form_binary_4.0',\n",
    "    'legal_form_binary_5.0',\n",
    "    'legal_form_binary_6.0',\n",
    "    'legal_form_binary_8.0',\n",
    "    'legal_form_binary_10.0',\n",
    "    'legal_form_binary_12.0',\n",
    "    'legal_form_binary_13.0',\n",
    "    'legal_form_binary_15.0',\n",
    "    'legal_form_binary_16.0',\n",
    "    'legal_form_binary_17.0',\n",
    "    'legal_form_binary_19.0',\n",
    "\n",
    "    'section_1_label',  # Top 3 NOGA Levels\n",
    "    'division_1_label',\n",
    "    'group_1_label',\n",
    "    'class_1_label',\n",
    "\n",
    "    'section_2_label',\n",
    "    'division_2_label',\n",
    "    'group_2_label',\n",
    "    'class_2_label',\n",
    "\n",
    "    'section_3_label',\n",
    "    'division_3_label',\n",
    "    'group_3_label',\n",
    "    'class_3_label',\n",
    "\n",
    "    'capital_chf',  # registered capital\n",
    "\n",
    "    # Firm name features\n",
    "    'firm_name_length',  \n",
    "    'firm_name_swiss_ref',\n",
    "    'firm_name_geog_ref',\n",
    "    'firm_name_founder_match',\n",
    "    'firm_name_male_match',\n",
    "    'firm_name_female_match',\n",
    "\n",
    "    # Address features\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'founding_bfs_code',\n",
    "    'district_id',\n",
    "    'canton_id', \n",
    "    'urban_rural',  # Cat\n",
    "    'typology_9c',  # Cat\n",
    "    'typology_25c',  # Cat\n",
    "\n",
    "    'urban_rural_2.0',  # Binary\n",
    "    'urban_rural_3.0',\n",
    "\n",
    "    'typology_9c_12.0',  # Binary\n",
    "    'typology_9c_13.0',\n",
    "    'typology_9c_21.0',\n",
    "    'typology_9c_22.0',\n",
    "    'typology_9c_23.0',\n",
    "    'typology_9c_31.0',\n",
    "    'typology_9c_32.0',\n",
    "    'typology_9c_33.0',\n",
    "\n",
    "    'typology_25c_112.0',  # Binary\n",
    "    'typology_25c_113.0',\n",
    "    'typology_25c_121.0',\n",
    "    'typology_25c_122.0',\n",
    "    'typology_25c_123.0',\n",
    "    'typology_25c_134.0',\n",
    "    'typology_25c_136.0',\n",
    "    'typology_25c_137.0',\n",
    "    'typology_25c_216.0',\n",
    "    'typology_25c_217.0',\n",
    "    'typology_25c_226.0',\n",
    "    'typology_25c_227.0',\n",
    "    'typology_25c_235.0',\n",
    "    'typology_25c_236.0',\n",
    "    'typology_25c_237.0',\n",
    "    'typology_25c_314.0',\n",
    "    'typology_25c_316.0',\n",
    "    'typology_25c_317.0',\n",
    "    'typology_25c_325.0',\n",
    "    'typology_25c_326.0',\n",
    "    'typology_25c_327.0',\n",
    "    'typology_25c_334.0',\n",
    "    'typology_25c_335.0',\n",
    "    'typology_25c_338.0',\n",
    "\n",
    "    'population',\n",
    "\n",
    "    'n_firms_same_address',\n",
    "    'n_firms_within_1km',\n",
    "    'n_firms_within_2.5km',\n",
    "    'n_firms_within_10km',\n",
    "\n",
    "    # Founder features\n",
    "    'n_founders',\n",
    "    'n_inscribed_firms',\n",
    "    'pct_female_founders',\n",
    "    'n_distinct_nationalities',\n",
    "    'n_swiss_founders',\n",
    "    'n_foreign_founders',\n",
    "    'n_dr_titles',\n",
    "    'n_founders_same_residence',\n",
    "    'founder_fids',\n",
    "    'n_founders_with_experience',\n",
    "    'pct_founders_with_experience',\n",
    "    'prior_failed',\n",
    "    'prior_existing',\n",
    "\n",
    "    # BPS features\n",
    "    'bps_language',\n",
    "    'bps_normalized',\n",
    "    'bps_length',\n",
    "    'bps_mean_word_length',\n",
    "    'bps_length_quantiles_5',\n",
    "    'bps_lix',\n",
    "    'bps_min_word_freq_norm',\n",
    "    'bps_max_word_freq_norm',\n",
    "    'bps_freq_ratio_norm',\n",
    "    'has_location',\n",
    "    'has_male_name',\n",
    "    'has_female_name',\n",
    "\n",
    "    # Website features\n",
    "    'n_pages',\n",
    "    'total_text_len',\n",
    "    'mean_text_len',\n",
    "    'n_internal_links_mean',\n",
    "    'n_external_links_mean',\n",
    "    'n_languages',\n",
    "    'dominant_language',\n",
    "\n",
    "    # Control variables\n",
    "    'days_of_prior_observations'\n",
    "    'founding_date',\n",
    "    'prediction_1_score',\n",
    "    'prediction_2_score',\n",
    "    'prediction_3_score',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ehraid',\n",
       " 'founding_legal_form',\n",
       " 'current_purpose',\n",
       " 'founding_purpose',\n",
       " 'founding_bfs_code',\n",
       " 'founding_date',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'population',\n",
       " 'canton_id',\n",
       " 'district_id',\n",
       " 'urban_rural',\n",
       " 'typology_9c',\n",
       " 'typology_25c',\n",
       " 'capital_chf',\n",
       " 'n_pages',\n",
       " 'total_text_len',\n",
       " 'mean_text_len',\n",
       " 'n_internal_links_mean',\n",
       " 'n_external_links_mean',\n",
       " 'n_languages',\n",
       " 'dominant_language',\n",
       " 'class_1_label',\n",
       " 'section_1_label',\n",
       " 'prediction_1_score',\n",
       " 'class_2_label',\n",
       " 'section_2_label',\n",
       " 'prediction_2_score',\n",
       " 'class_3_label',\n",
       " 'section_3_label',\n",
       " 'prediction_3_score',\n",
       " 'exit_date',\n",
       " 'target_inv_exit',\n",
       " 'merger_date',\n",
       " 'target_acquisition',\n",
       " 'investment_date',\n",
       " 'target_non_gov_investment',\n",
       " 'subsidy_date',\n",
       " 'target_inno_subsidy',\n",
       " 'legal_form_binary_3.0',\n",
       " 'legal_form_binary_4.0',\n",
       " 'legal_form_binary_5.0',\n",
       " 'legal_form_binary_6.0',\n",
       " 'legal_form_binary_8.0',\n",
       " 'legal_form_binary_10.0',\n",
       " 'legal_form_binary_12.0',\n",
       " 'legal_form_binary_13.0',\n",
       " 'legal_form_binary_15.0',\n",
       " 'legal_form_binary_16.0',\n",
       " 'legal_form_binary_17.0',\n",
       " 'legal_form_binary_19.0',\n",
       " 'typology_25c_112.0',\n",
       " 'typology_25c_113.0',\n",
       " 'typology_25c_121.0',\n",
       " 'typology_25c_122.0',\n",
       " 'typology_25c_123.0',\n",
       " 'typology_25c_134.0',\n",
       " 'typology_25c_136.0',\n",
       " 'typology_25c_137.0',\n",
       " 'typology_25c_216.0',\n",
       " 'typology_25c_217.0',\n",
       " 'typology_25c_226.0',\n",
       " 'typology_25c_227.0',\n",
       " 'typology_25c_235.0',\n",
       " 'typology_25c_236.0',\n",
       " 'typology_25c_237.0',\n",
       " 'typology_25c_314.0',\n",
       " 'typology_25c_316.0',\n",
       " 'typology_25c_317.0',\n",
       " 'typology_25c_325.0',\n",
       " 'typology_25c_326.0',\n",
       " 'typology_25c_327.0',\n",
       " 'typology_25c_334.0',\n",
       " 'typology_25c_335.0',\n",
       " 'typology_25c_338.0',\n",
       " 'typology_9c_12.0',\n",
       " 'typology_9c_13.0',\n",
       " 'typology_9c_21.0',\n",
       " 'typology_9c_22.0',\n",
       " 'typology_9c_23.0',\n",
       " 'typology_9c_31.0',\n",
       " 'typology_9c_32.0',\n",
       " 'typology_9c_33.0',\n",
       " 'urban_rural_2.0',\n",
       " 'urban_rural_3.0',\n",
       " 'group_1_label',\n",
       " 'division_1_label',\n",
       " 'group_2_label',\n",
       " 'division_2_label',\n",
       " 'group_3_label',\n",
       " 'division_3_label',\n",
       " 'n_firms_same_address',\n",
       " 'n_firms_within_1km',\n",
       " 'n_firms_within_2.5km',\n",
       " 'n_firms_within_10km',\n",
       " 'firm_name_length',\n",
       " 'firm_name_swiss_ref',\n",
       " 'firm_name_geog_ref',\n",
       " 'firm_name_founder_match',\n",
       " 'firm_name_male_match',\n",
       " 'firm_name_female_match',\n",
       " 'bps_language',\n",
       " 'bps_normalized',\n",
       " 'bps_length',\n",
       " 'bps_mean_word_length',\n",
       " 'bps_length_quantiles_5',\n",
       " 'bps_lix',\n",
       " 'bps_min_word_freq_norm',\n",
       " 'bps_max_word_freq_norm',\n",
       " 'bps_freq_ratio_norm',\n",
       " 'has_location',\n",
       " 'has_male_name',\n",
       " 'has_female_name',\n",
       " 'n_founders',\n",
       " 'n_inscribed_firms',\n",
       " 'n_female_founders',\n",
       " 'pct_female_founders',\n",
       " 'n_distinct_nationalities',\n",
       " 'n_swiss_founders',\n",
       " 'n_foreign_founders',\n",
       " 'n_dr_titles',\n",
       " 'n_founders_same_residence',\n",
       " 'founder_fids',\n",
       " 'n_founders_with_experience',\n",
       " 'pct_founders_with_experience',\n",
       " 'prior_failed',\n",
       " 'prior_existing',\n",
       " 'days_of_prior_observations']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "\n",
    "    def __init__(self, target_vars: list[str], model_specs: dict, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        target_vars: list of target column names\n",
    "        model_specs: dict with keys as model names, values as dicts with:\n",
    "                     - 'model': a scikit-learn estimator class or a callable returning an instance\n",
    "                     - 'param_grid': dict of hyperparameter grid for grid search\n",
    "        Example:\n",
    "        model_specs = {\n",
    "            'logreg': {'model': LogisticRegression, 'param_grid': {...}},\n",
    "            'rf': {'model': RandomForestClassifier, 'param_grid': {...}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.target_vars = target_vars\n",
    "        self.model_specs = model_specs\n",
    "        self.random_state = random_state\n",
    "        self.best_params = {m: {t: None for t in target_vars} for m in model_specs}\n",
    "        self.selected_features = {m: {t: None for t in target_vars} for m in model_specs}\n",
    "        self.best_models = {m: {t: None for t in target_vars} for m in model_specs}\n",
    "        self.production_models = {m: {t: None for t in target_vars} for m in model_specs}\n",
    "        self.metrics_report = {m: {t: None for t in target_vars} for m in model_specs}\n",
    "\n",
    "    def load_data(self, df: pd.DataFrame, feature_cols: list[str]):\n",
    "        self.df = df.copy()\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "    def load_best_params(self, file_path: Path, model_names: list[str], target_vars: list[str]):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            all_params = pickle.load(f)\n",
    "        self.best_params = {\n",
    "            m: {t: all_params[m][t] for t in target_vars}\n",
    "            for m in model_names\n",
    "        }\n",
    "\n",
    "    def load_best_features(self, file_path: Path, model_names: list[str], target_vars: list[str], additional_features: list = []):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            all_features = pickle.load(f)\n",
    "        self.selected_features = {\n",
    "            m: {t: all_features[m][t] + additional_features for t in target_vars}\n",
    "            for m in model_names\n",
    "        }\n",
    "\n",
    "    def _get_feature_importances(self, model):\n",
    "        \"\"\"Return feature importances as a pandas Series, sorted descending.\"\"\"\n",
    "        # Tree-based models\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            return model.feature_importances_\n",
    "        # Linear models (coefficients)\n",
    "        elif hasattr(model, \"coef_\"):\n",
    "            return np.abs(model.coef_).flatten()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _add_class_weights_to_fit_params(self, fit_params, ModelClass, model_name, y):\n",
    "        \n",
    "        if not self.model_specs[model_name]['include_class_weights']:\n",
    "            return fit_params\n",
    "\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        if model_name == 'xgb':\n",
    "            n_pos = np.sum(y == classes[1])\n",
    "            n_neg = np.sum(y == classes[0])\n",
    "            fit_params['scale_pos_weight'] = n_neg / n_pos if n_pos > 0 else 1.0\n",
    "\n",
    "        elif hasattr(ModelClass(), 'class_weight'):\n",
    "            class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "            cw_dict = {cls: w for cls, w in zip(classes, class_weights)}\n",
    "            fit_params['class_weight'] = cw_dict\n",
    "\n",
    "        return fit_params\n",
    "\n",
    "    def nested_cv_with_feature_selection(\n",
    "        self,\n",
    "        k_outer: int = 5,\n",
    "        k_inner: int = 5,\n",
    "        min_features_to_select: int = 3,\n",
    "        scoring: str = \"roc_auc\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform feature selection (RFECV) using cross-validation without hyperparameter tuning.\n",
    "\n",
    "        Saves the most frequently selected features across outer folds.\n",
    "        \"\"\"\n",
    "\n",
    "        for model_name, spec in self.model_specs.items():\n",
    "            ModelClass = spec['model']\n",
    "            fit_params = self._add_class_weights_to_fit_params(spec['fit_params'])\n",
    "\n",
    "            for target in self.target_vars:\n",
    "                X = self.df[self.feature_cols].values\n",
    "                y = self.df[target].values\n",
    "\n",
    "                outer_skf = StratifiedKFold(n_splits=k_outer, shuffle=True, random_state=self.random_state)\n",
    "\n",
    "                selected_feature_masks = []\n",
    "\n",
    "                for train_idx, _ in outer_skf.split(X, y):\n",
    "                    X_train, y_train = X[train_idx], y[train_idx]\n",
    "\n",
    "                    model = ModelClass(**fit_params)\n",
    "                    \n",
    "                    inner_skf = StratifiedKFold(n_splits=k_inner, shuffle=True, random_state=self.random_state)\n",
    "                    rfecv = RFECV(\n",
    "                        estimator=model,\n",
    "                        step=1,\n",
    "                        min_features_to_select=min_features_to_select,\n",
    "                        cv=inner_skf,\n",
    "                        scoring=scoring,\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "\n",
    "                    rfecv.fit(X_train, y_train)\n",
    "                    selected_feature_masks.append(rfecv.support_)\n",
    "\n",
    "                # Aggregate selected features across folds\n",
    "                selected_feature_masks = np.array(selected_feature_masks)\n",
    "                mean_mask = selected_feature_masks.mean(axis=0)\n",
    "                threshold = 0.6  # at least 60% of folds must have selected a feature\n",
    "                final_mask = mean_mask >= threshold\n",
    "                final_features = np.array(self.feature_cols)[final_mask].tolist()\n",
    "\n",
    "                self.selected_features[model_name][target] = final_features\n",
    "                print(f\"[{model_name}/{target}] Selected {len(final_features)} features: {final_features}\")\n",
    "\n",
    "    def nested_cv_with_hyperparam_search(self, out_folder: Path, k_outer: int = 5, k_inner: int = 5, best_features: bool = False):\n",
    "        \n",
    "        if best_features and not self.selected_features:\n",
    "            raise ValueError('To use the best features execure find_best_feature_subset first!')\n",
    "        \n",
    "        for model_name, spec in self.model_specs.items():\n",
    "\n",
    "            ModelClass = spec['model']\n",
    "            fit_params = self._add_class_weights_to_fit_params(spec['fit_params'])\n",
    "            param_grid = spec['param_grid']\n",
    "\n",
    "            for target in self.target_vars:\n",
    "                features = self.selected_features[model_name][target] if best_features else self.feature_cols\n",
    "                X = self.df[features].values\n",
    "                y = self.df[target].values\n",
    "\n",
    "                outer_skf = StratifiedKFold(n_splits=k_outer, shuffle=True, random_state=self.random_state)\n",
    "                \n",
    "                outer_aucs = []\n",
    "                feature_importances_folds = []\n",
    "                inner_best_params = []\n",
    "                \n",
    "                # Outer loop over k_outer folds\n",
    "                for train_idx, test_idx in outer_skf.split(X, y):\n",
    "                    \n",
    "                    # Init data of the current outer fold\n",
    "                    X_train, X_test = X[train_idx], X[test_idx]\n",
    "                    y_train, y_test = y[train_idx], y[test_idx]\n",
    "                    \n",
    "                    # Only include relevant fit_params for this model\n",
    "                    model = ModelClass(**fit_params)\n",
    "\n",
    "                    # Set up inner grid search loop for k_inner folds\n",
    "                    inner_skf = StratifiedKFold(n_splits=k_inner, shuffle=True, random_state=self.random_state)\n",
    "                    grid_search = GridSearchCV(\n",
    "                        estimator=model,\n",
    "                        param_grid=param_grid,\n",
    "                        cv=inner_skf,\n",
    "                        scoring='roc_auc',\n",
    "                        n_jobs=-1\n",
    "                    )\n",
    "                    \n",
    "                    # Determine best hyperparameters of this fold using only the training data and not testing\n",
    "                    # training data is then again split into k_inner folds\n",
    "                    grid_search.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Select and store best hyperparam config determined on the training data\n",
    "                    inner_lambda_star = grid_search.best_params_\n",
    "                    inner_best_params.append(inner_lambda_star)\n",
    "                    \n",
    "                    # Refit model with full training data to estimate auc and feature importance of the outer fold\n",
    "                    temp_fit_params = {**fit_params, 'n_jobs': -1}  # For training without CV set to -1\n",
    "                    params = dict(inner_lambda_star, **temp_fit_params)\n",
    "                    best_model = ModelClass(**params)\n",
    "                    best_model.fit(X_train, y_train)\n",
    "                    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "                    \n",
    "                    feature_importances_folds.append(self._get_feature_importances(best_model))\n",
    "                    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "                    outer_aucs.append(auc)\n",
    "\n",
    "                mean_feature_importance = None\n",
    "                if feature_importances_folds:\n",
    "                    mean_importances = np.mean(feature_importances_folds, axis=0)\n",
    "                    mean_feature_importance = {feature: score for feature, score in zip(features, mean_importances)}\n",
    "\n",
    "                # Select hyperparameters from the inner folds that were most frequently picked as the best hyperparameters for the outer model\n",
    "                overall_lambda_star = Counter([frozenset(p.items()) for p in inner_best_params]).most_common(1)[0][0]\n",
    "                overall_lambda_star = dict(overall_lambda_star)\n",
    "\n",
    "                # Retrain final model on all data\n",
    "                temp_fit_params = {**fit_params, 'n_jobs': -1}  # For training without CV set to -1\n",
    "                best_params = dict(overall_lambda_star, **temp_fit_params)\n",
    "                production_model = ModelClass(**best_params)\n",
    "                production_model.fit(X, y)\n",
    "\n",
    "                # Store production model\n",
    "                self.best_models[model_name][target] = production_model\n",
    "\n",
    "                # Store best hyperparameters\n",
    "                self.best_params[model_name][target] = overall_lambda_star\n",
    "\n",
    "                # Store metrics report for the avg performance of the model on the current target\n",
    "                self.metrics_report[model_name][target] = {\n",
    "                    'mean_auc': np.mean(outer_aucs),\n",
    "                    'std_auc': np.std(outer_aucs),\n",
    "                    'all_auc': outer_aucs,\n",
    "                    'best_params': overall_lambda_star,\n",
    "                    'mean_feature_importances': mean_feature_importance\n",
    "                }\n",
    "        self._save_models_and_reports(out_folder)\n",
    "\n",
    "    def _save_models_and_reports(self, out_folder: Path):\n",
    "        \"\"\"Retrains model on the full dataset and stores production ready model with additional performance reports\n",
    "        from the k-fold CV evaluations.\"\"\"\n",
    "        \n",
    "        out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save best models stored in self.best_models[model_name][target]\n",
    "        models_dir = out_folder / 'trained_models'\n",
    "        models_dir.mkdir(exist_ok=True)\n",
    "        for model_name, targets in self.best_models.items():\n",
    "            for target, model in targets.items():\n",
    "                model_path = models_dir / f'{model_name}_{target}.joblib'\n",
    "                joblib.dump(model, model_path)\n",
    "                print(f\"Saved model for {model_name}/{target} to {model_path}\")\n",
    "\n",
    "        # Save metrics report as csv file\n",
    "        summary_rows = []\n",
    "        for model_name, model_results in self.metrics_report.items():\n",
    "            for target, d in model_results.items():\n",
    "                summary_rows.append({\n",
    "                    'model': model_name,\n",
    "                    'target': target,\n",
    "                    'mean_auc': d['mean_auc'],\n",
    "                    'std_auc': d['std_auc'],\n",
    "                    'all_auc': str(d['all_auc']),\n",
    "                    'best_params': str(d['best_params']),\n",
    "                    'mean_feature_importances': str(d['mean_feature_importances'])\n",
    "                })\n",
    "        pd.DataFrame(summary_rows).to_csv(out_folder / 'cv_metrics_report.csv', index=False)\n",
    "        print(f\"Saved metrics summary to {str(out_folder / 'cv_metrics_report.csv')}\")\n",
    "\n",
    "        # Save hyperparameters\n",
    "        best_params_path = out_folder / 'best_hyperparameters.pkl'\n",
    "        with open(best_params_path, 'wb') as f:\n",
    "            pickle.dump(self.best_params, f)\n",
    "        print(f\"Saved best hyperparameters to {best_params_path}\")\n",
    "\n",
    "        # Save best feature set\n",
    "        best_features_path = out_folder / 'best_features.pkl'\n",
    "        with open(best_features_path, 'wb') as f:\n",
    "            pickle.dump(self.selected_features, f)\n",
    "        print(f\"Saved best features to {best_features_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "MODEL_SPECS = {\n",
    "    'logreg': {\n",
    "        'model': LogisticRegression,\n",
    "        'fit_params': {\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': 1,  # Avoid thread thrashing, so model n_jobs should be set to 1 because Grid Search CV and Feature Selection is set to -1\n",
    "            'max_iter': 1000,\n",
    "            'solver': 'saga',  # Fixed for computational efficiency\n",
    "        },\n",
    "        'param_grid': [\n",
    "            {\n",
    "                'penalty': ['none'],  # Test vanilla LogReg\n",
    "            },\n",
    "            {\n",
    "                'penalty': ['l1', 'l2'],  # Test Lasso and Ridge regularization\n",
    "                'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            },\n",
    "        ],\n",
    "        'include_class_weights': True\n",
    "    },\n",
    "    'rf': {\n",
    "        'model': RandomForestClassifier,\n",
    "        'fit_params': {\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': 1,\n",
    "        },\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'max_depth': [None, 10, 25, 50],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', 0.5],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "        },\n",
    "        'include_class_weights': True\n",
    "    },\n",
    "    'xgb': {\n",
    "        'model': XGBClassifier,\n",
    "        'fit_params': {\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': 1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'booster': 'gbtree',\n",
    "            'tree_method': 'hist',\n",
    "            'use_label_encoder': False,\n",
    "            'eval_metric': 'auc',\n",
    "        },\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 300],\n",
    "            'max_depth': [3, 6],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "            'min_child_weight': [1, 5],\n",
    "            'gamma': [0, 1],\n",
    "            'reg_alpha': [0, 1],\n",
    "            'reg_lambda': [1, 2],\n",
    "        },\n",
    "        'include_class_weights': True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment A: Reproducability experiment and model evaluation on the full data set without website data\n",
    "\"\"\"\n",
    "\n",
    "# 1. Load data for experiment A\n",
    "df_a = pd.read_csv(PROCESSED_DATA_DIR / ...)\n",
    "\n",
    "FEATURE_COLS = [col for col in df_a.columns if col not in ['target1', 'target2', 'target3', 'target4']]\n",
    "TARGET_VARS = ['target1', 'target2', 'target3', 'target4']\n",
    "\n",
    "# 2. Initialize model evaluation with targets and model specs\n",
    "meval = ModelEvaluation(TARGET_VARS, MODEL_SPECS, random_state=RANDOM_STATE)\n",
    "meval.load_data(df_a, FEATURE_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Training procedure on all features for the baseline reproduction\n",
    "out_folder = MODELS_DIR / 'experiment_A'\n",
    "meval.nested_cv_with_hyperparam_search(out_folder=out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment B: Performance difference between Doc2vec and my implementation\n",
    "\"\"\"\n",
    "\n",
    "BEST_MODEL = 'xgb'\n",
    "TARGET_VARS = [...]\n",
    "\n",
    "\n",
    "MODELS = {\n",
    "    'logreg': LogisticRegression,\n",
    "    'rf': RandomForestClassifier,\n",
    "    'xgb': XGBClassifier,\n",
    "}\n",
    "\n",
    "# 1. Load data for experiment B\n",
    "df_b = pd.read_csv(PROCESSED_DATA_DIR / ...)\n",
    "FEATURE_COLS = [col for col in df_b.columns if col not in ['target1', 'target2', 'target3', 'target4']]\n",
    "\n",
    "# 2. Initialize model with best best performing model from experiment A\n",
    "model_specs = {\n",
    "    BEST_MODEL: {\n",
    "        'model': MODELS.get(BEST_MODEL),\n",
    "        'fit_params': {\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "            'max_iter': 1000\n",
    "        },\n",
    "        'param_grid': [\n",
    "            {\n",
    "                'penalty': ['none'],\n",
    "                'solver': ['saga'],\n",
    "            },\n",
    "            {\n",
    "                'penalty': ['l1'],\n",
    "                'solver': ['saga'],\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "            },\n",
    "            {\n",
    "                'penalty': ['l2'],\n",
    "                'solver': ['saga'],\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "            },\n",
    "        ],\n",
    "        'include_class_weights': True\n",
    "    }\n",
    "}\n",
    "meval = ModelEvaluation(TARGET_VARS, model_specs)\n",
    "meval.load_data(df_b, FEATURE_COLS)\n",
    "\n",
    "# 3. Use best model params and features from experiment A\n",
    "exp_A_folder = MODELS_DIR / 'experiment_A'\n",
    "\n",
    "meval.load_best_params(exp_A_folder / 'best_hyperparameters.pkl', ['xgb'], TARGET_VARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate with doc2vec scores\n",
    "out_folder = MODELS_DIR / 'experiment_B' / 'doc2vec'\n",
    "\n",
    "meval.nested_cv_with_hyperparam_search(out_folder=out_folder)\n",
    "\n",
    "# 2. Evaluate with dimension scores\n",
    "out_folder = MODELS_DIR / 'experiment_B' / 'dimensions'\n",
    "\n",
    "meval.nested_cv_with_hyperparam_search(out_folder=out_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Experiment C: Performance difference between Doc2vec and my implementation\n",
    "\"\"\"\n",
    "\n",
    "# Add hyperparams to fit_params for the initial feature selection without tuning\n",
    "model_specs = {\n",
    "    BEST_MODEL: {\n",
    "        'model': MODELS.get(BEST_MODEL),\n",
    "        'fit_params': {\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "            'max_iter': 1000\n",
    "        },\n",
    "        'param_grid': [\n",
    "            {\n",
    "                'penalty': ['none'],\n",
    "                'solver': ['saga'],\n",
    "            },\n",
    "            {\n",
    "                'penalty': ['l1'],\n",
    "                'solver': ['saga'],\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "            },\n",
    "            {\n",
    "                'penalty': ['l2'],\n",
    "                'solver': ['saga'],\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "            },\n",
    "        ],\n",
    "        'include_class_weights': True\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# 1. Init model evaluation with same setup as in experiment B\n",
    "meval = ModelEvaluation(TARGET_VARS, model_specs)\n",
    "meval.load_data(df_b, FEATURE_COLS)\n",
    "\n",
    "# 2. Training procedure with feature selection\n",
    "out_folder = MODELS_DIR / 'experiment_C'\n",
    "\n",
    "meval.nested_cv_with_feature_selection()\n",
    "meval.nested_cv_with_hyperparam_search(out_folder=out_folder, best_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
