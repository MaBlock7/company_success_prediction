{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-22 18:03:49.718\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/manuelbolz/Documents/git/for_work/company_success_prediction\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from pocketknife.database import connect_database, read_from_database, save_to_database\n",
    "from config import EXTERNAL_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db queries\n",
    "query_shab = \"\"\"\n",
    "    SELECT\n",
    "        ehraid,\n",
    "        shab_id,\n",
    "        shab_date,\n",
    "        registry_office_canton,\n",
    "        message AS message_raw\n",
    "    FROM zefix.shab\n",
    "\"\"\"\n",
    "\n",
    "query_shab_mutation = \"\"\"\n",
    "    SELECT * \n",
    "    FROM zefix.shab_mutation\n",
    "\"\"\"\n",
    "\n",
    "query_gender_mapping = \"\"\"\n",
    "    SELECT *\n",
    "    FROM zefix.founders_gender_mapping\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    raw_shab = read_from_database(con, query_shab)\n",
    "    raw_shab_mutation = read_from_database(con, query_shab_mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connect_database() as con:\n",
    "    zefix_gender_mapping = read_from_database(con, query_gender_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = 'de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_shab_messages = pd.read_csv(EXTERNAL_DATA_DIR / f'final_{LANGUAGE}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.fillna('{}')\n",
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.str.replace(\"'null'\", \"''\", regex=False)\n",
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.str.replace(\"'n/a'\", \"''\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.apply(ast.literal_eval)\n",
    "parsed_shab_messages['parsed_variables'] = parsed_shab_messages.parsed_variables.fillna({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shab_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_slices</th>\n",
       "      <th>parsed_variables</th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2636073</td>\n",
       "      <td>BEGINNING</td>\n",
       "      <td>[Rubigenhof Fischzucht AG , in Rubigen , CHE-2...</td>\n",
       "      <td>{}</td>\n",
       "      <td>undetermined</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2636073</td>\n",
       "      <td>adresse neu</td>\n",
       "      <td>[Alte Belpstrasse 5 , 3113 Rubigen]</td>\n",
       "      <td>{'addresses_until_now': [], 'addresses_deleted...</td>\n",
       "      <td>firm and address changes</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2636075</td>\n",
       "      <td>BEGINNING</td>\n",
       "      <td>[Käsereigenossenschaft Rüegsbach , in Rüegsau ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>undetermined</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2636075</td>\n",
       "      <td>adresse neu</td>\n",
       "      <td>[c/o Peter Stalder , Lehn 265 , 3418 Rüegsbach]</td>\n",
       "      <td>{'addresses_until_now': [], 'addresses_deleted...</td>\n",
       "      <td>firm and address changes</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2636075</td>\n",
       "      <td>mitteilungen</td>\n",
       "      <td>[Mitteilungen an die Genossenschafter : schrif...</td>\n",
       "      <td>{}</td>\n",
       "      <td>undetermined</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shab_id       keyword                                        text_slices  \\\n",
       "0  2636073     BEGINNING  [Rubigenhof Fischzucht AG , in Rubigen , CHE-2...   \n",
       "1  2636073   adresse neu                [Alte Belpstrasse 5 , 3113 Rubigen]   \n",
       "2  2636075     BEGINNING  [Käsereigenossenschaft Rüegsbach , in Rüegsau ...   \n",
       "3  2636075   adresse neu    [c/o Peter Stalder , Lehn 265 , 3418 Rüegsbach]   \n",
       "4  2636075  mitteilungen  [Mitteilungen an die Genossenschafter : schrif...   \n",
       "\n",
       "                                    parsed_variables  \\\n",
       "0                                                 {}   \n",
       "1  {'addresses_until_now': [], 'addresses_deleted...   \n",
       "2                                                 {}   \n",
       "3  {'addresses_until_now': [], 'addresses_deleted...   \n",
       "4                                                 {}   \n",
       "\n",
       "                   category language  \n",
       "0              undetermined       de  \n",
       "1  firm and address changes       de  \n",
       "2              undetermined       de  \n",
       "3  firm and address changes       de  \n",
       "4              undetermined       de  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_shab_messages['text_slices'] = parsed_shab_messages['text_slices'].fillna('[]')\n",
    "parsed_shab_messages['text_slices'] = parsed_shab_messages['text_slices'].apply(ast.literal_eval)\n",
    "parsed_shab_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636019\n",
      "2476325\n"
     ]
    }
   ],
   "source": [
    "print(parsed_shab_messages['shab_id'].nunique())\n",
    "print(raw_shab['shab_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the companies where the shab entries have been parsed\n",
    "raw_shab_sub = raw_shab[raw_shab.shab_id.isin(parsed_shab_messages['shab_id'].unique())].copy()\n",
    "raw_shab_mutation = raw_shab_mutation[raw_shab_mutation.shab_id.isin(parsed_shab_messages['shab_id'].unique())].copy()\n",
    "raw_shab_mutation_grouped = (\n",
    "    raw_shab_mutation\n",
    "    .groupby('shab_id')\n",
    "    .agg(codes=pd.NamedAgg(column='description', aggfunc=lambda x: [v for v in x]))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the dataframes\n",
    "shab_merged = (\n",
    "    raw_shab_sub\n",
    "    .merge(raw_shab_mutation_grouped, on='shab_id', how='left')\n",
    "    .merge(parsed_shab_messages, on='shab_id', how='left')\n",
    ")\n",
    "\n",
    "# Sort values in the correct temporal order\n",
    "shab_merged = shab_merged.sort_values(['ehraid', 'shab_date', 'shab_id'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "shab_merged = shab_merged.drop(columns=['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shab_merged_temp = shab_merged[shab_merged.ehraid.isin([905876, 905843, 905844])]\n",
    "# shab_merged_temp = shab_merged_temp.sort_values(['ehraid', 'shab_date', 'shab_id'], ascending=[True, True, True]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_json_history(df: pd.DataFrame) -> dict:\n",
    "    json_structure = defaultdict(lambda: {'history': []})\n",
    "    for _, row in df.iterrows():\n",
    "        # Extract the fields to identify a company and its entries\n",
    "        ehraid = row['ehraid']\n",
    "        shab_date = row['shab_date'] if isinstance(row['shab_date'], str) else row['shab_date'].strftime('%Y-%d-%m')\n",
    "        shab_id = row['shab_id']\n",
    "        category = row['category']\n",
    "        keyword = row['keyword']\n",
    "\n",
    "        # Extract main information\n",
    "        message_info = {\n",
    "            'registry_office_canton': row['registry_office_canton'],\n",
    "            'codes': row['codes'],\n",
    "            'message_raw': row['message_raw'],\n",
    "            'extracted_content': {\n",
    "                category: {\n",
    "                    keyword: {\n",
    "                        'text_slices': row['text_slices'],\n",
    "                        'variables': row['parsed_variables']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Search if shab_date already exists in the history\n",
    "        date_entry = next((entry for entry in json_structure[ehraid]['history'] if shab_date in entry), None)\n",
    "\n",
    "        if date_entry is None:\n",
    "            # If the date does not exist, create a new entry\n",
    "            date_entry = {shab_date: {shab_id: message_info}}\n",
    "            json_structure[ehraid]['history'].append(date_entry)\n",
    "        else:\n",
    "            # If the date exists, check if the shab_id already exists\n",
    "            id_entry = date_entry[shab_date].get(shab_id, None)\n",
    "            if id_entry is None:\n",
    "                # If the shab_id does not exists, we can simply add it to the shab_date\n",
    "                date_entry[shab_date][shab_id] = message_info\n",
    "            else:\n",
    "                # If the shab_id exists, we need to check if the category already exists\n",
    "                category_entry = date_entry[shab_date][shab_id]['extracted_content'].get(category, None)\n",
    "                if category_entry is None:\n",
    "                    # If it does not exist, we add it to the extracted content\n",
    "                    date_entry[shab_date][shab_id]['extracted_content'][category] = message_info['extracted_content'][category]\n",
    "                else:\n",
    "                    # If it does, we add the keyword to the category, since a keyword can only appear once within the category\n",
    "                    date_entry[shab_date][shab_id]['extracted_content'][category][keyword] = message_info['extracted_content'][category][keyword]      \n",
    "\n",
    "    return dict(json_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_json = create_raw_json_history(shab_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE REGISTERED PEOPLE AND FIRMS TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people = shab_merged[shab_merged.category == 'natural persons and legal entities'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure only the expected fields are there\n",
    "def validate_person_and_firms(x: dict):    \n",
    "    schema = {'firms': [], 'people': []}\n",
    "    for firm in x.get('firms', []):\n",
    "        if isinstance(firm, dict):\n",
    "            schema['firms'].append({\n",
    "                'firm_name': firm.get('firm_name'),\n",
    "                'firm_uid': firm.get('id'),\n",
    "                'firm_seat': firm.get('location'),\n",
    "                'firm_type': firm.get('type'),\n",
    "                'firm_shares': firm.get('shares')\n",
    "            })\n",
    "    for person in x.get('people', []):\n",
    "        if isinstance(person, dict):\n",
    "            schema['people'].append({\n",
    "                'first_name': person.get('first_name'),\n",
    "                'last_name': person.get('last_name'),\n",
    "                'hometown': person.get('hometown'),\n",
    "                'place_of_residence': person.get('place_of_residence'),\n",
    "                'nationality': person.get('nationality'),\n",
    "                'job_title': person.get('job_title'),\n",
    "                'signing_rights': person.get('signing_rights'),\n",
    "                'shares': person.get('shares')\n",
    "            })\n",
    "    return schema\n",
    "\n",
    "df_people['validated_variables'] = df_people['parsed_variables'].apply(validate_person_and_firms)  \n",
    "df_people['firms'] = df_people['validated_variables'].apply(lambda x: x.get('firms', []))\n",
    "df_people['people'] = df_people['validated_variables'].apply(lambda x: x.get('people', []))\n",
    "\n",
    "# Split individual firm dictionaries into individual rows\n",
    "df_firms_exploded = df_people.explode(column=['firms']).dropna()\n",
    "df_people_exploded = df_people.explode(column=['people']).dropna()\n",
    "\n",
    "# Create individual columns from the dictionary\n",
    "df_firms_norm = pd.json_normalize(\n",
    "    df_firms_exploded['firms'],\n",
    "    errors='raise'\n",
    ")\n",
    "df_firms_concat = pd.concat([df_firms_exploded[['ehraid', 'message_raw', 'shab_date', 'shab_id', 'codes', 'keyword']].reset_index(drop=True), df_firms_norm], axis=1)\n",
    "\n",
    "df_people_norm = pd.json_normalize(\n",
    "    df_people_exploded['people'],\n",
    "    errors='raise'\n",
    ")\n",
    "df_people_concat = pd.concat([df_people_exploded[['ehraid', 'message_raw', 'shab_date', 'shab_id', 'codes', 'keyword']].reset_index(drop=True), df_people_norm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Gender and Nationality to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from unidecode import unidecode\n",
    "from zefix_processing.country_mapping import country_names_to_alpha2\n",
    "from zefix_processing.gender_mapping import german2gender, french2gender, italian2gender\n",
    "\n",
    "nlp_models = {\n",
    "    \"de\": spacy.load(\"de_core_news_sm\"),\n",
    "    \"fr\": spacy.load(\"fr_core_news_sm\"),\n",
    "    \"it\": spacy.load(\"it_core_news_sm\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_words(string: str) -> str:\n",
    "    replacements = {\n",
    "        'ä': 'ae',\n",
    "        'ö': 'oe',\n",
    "        'ü': 'ue'\n",
    "    }\n",
    "    for char, replacement in replacements.items():\n",
    "        string = string.replace(char, replacement)\n",
    "    \n",
    "    return unidecode(string.lower())\n",
    "\n",
    "\n",
    "def remove_articles(string: str) -> str:\n",
    "    articles = [\n",
    "        # German\n",
    "        r'\\bder\\b', r'\\bdie\\b', r'\\bdas\\b', r'\\bdes\\b', r'\\bdem\\b', r'\\bden\\b',\n",
    "        r'\\bdessen\\b', r'\\bderen\\b',\n",
    "        r'\\bein\\b', r'\\beine\\b', r'\\beiner\\b', r'\\beines\\b', r'\\beinem\\b', r'\\beinen\\b',\n",
    "\n",
    "        # French\n",
    "        r'\\ble\\b', r'\\bla\\b', r'\\bles\\b', r\"\\bl'\", r'\\bdu\\b', r'\\bdes\\b',\n",
    "        r'\\bau\\b', r'\\baux\\b', r'\\bun\\b', r'\\bune\\b', r\"\\bd'\", r'\\bde\\b', r'\\bde la\\b', r\"\\bde l'\",\n",
    "\n",
    "        # Italian\n",
    "        r'\\bdi\\b', r'\\bil\\b', r'\\blo\\b', r'\\bla\\b', r\"\\bl'\", r'\\bi\\b', r'\\bgli\\b', r'\\ble\\b',\n",
    "        r'\\bun\\b', r'\\buno\\b', r'\\buna\\b', r\"\\bun'\",\n",
    "        r'\\bdel\\b', r'\\bdello\\b', r'\\bdella\\b', r'\\bdei\\b', r'\\bdegli\\b', r'\\bdelle\\b',\n",
    "        r'\\bdal\\b', r'\\bdallo\\b', r'\\bdalla\\b', r'\\bdai\\b', r'\\bdagli\\b', r'\\bdalle\\b',\n",
    "        r'\\bal\\b', r'\\ballo\\b', r'\\balla\\b', r'\\bai\\b', r'\\bagli\\b', r'\\balle\\b',\n",
    "        r'\\bnel\\b', r'\\bnello\\b', r'\\bnella\\b', r'\\bnei\\b', r'\\bnegli\\b', r'\\bnelle\\b',\n",
    "        r'\\bsul\\b', r'\\bsullo\\b', r'\\bsulla\\b', r'\\bsui\\b', r'\\bsugli\\b', r'\\bsulle\\b',\n",
    "        r'\\bcol\\b', r'\\bcoi\\b'\n",
    "    ]\n",
    "    articles.sort(key=len, reverse=True)\n",
    "    pattern = re.compile('|'.join(articles), flags=re.IGNORECASE)\n",
    "    string = pattern.sub('', string)\n",
    "    return re.sub(r'\\s+', ' ', string).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_mapping = {normalize_words(k): v for k, v in country_names_to_alpha2.items()}\n",
    "nationality_mapping_norm = {remove_articles(k): v for k, v in nationality_mapping.items()}\n",
    "\n",
    "gender_mapping = {\n",
    "    'de': {normalize_words(k): v for k, v in german2gender.items()},\n",
    "    'fr': {normalize_words(k): v for k, v in french2gender.items()},\n",
    "    'it': {normalize_words(k): v for k, v in italian2gender.items()},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the hometown, place of residence, and nationality column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split multiple nationalities into individual columns\n",
    "def clean_location(language: str, string: str) -> str:\n",
    "    \"\"\"Removes artifacts at the beginning of a location such as 'à Zurich' -> 'Zurich'\"\"\"\n",
    "    mapping = {\n",
    "        'de': ['von', 'in'],\n",
    "        'fr': ['de', 'du', \"d'\", 'des', 'à'],\n",
    "        'it': ['da', \"d'\", 'in']\n",
    "    }\n",
    "    for word in mapping.get(language, []):\n",
    "        string = re.sub(rf'^{word}\\s+', ' ', string)\n",
    "    return string.strip()\n",
    "\n",
    "def split_locations(df: pd.DataFrame, orig_col: str = 'nationality'):\n",
    "    withand2noand = {\n",
    "        'Antigua und Barbuda': 'Antigua Barbuda',\n",
    "        'Bosnien und Herzegowina': 'Bosnien Herzegowina',\n",
    "        'Bonaire, Sint Eustatius und Saba': 'Bonaire, Sint Eustatius Saba',\n",
    "        'Südgeorgien und die Südlichen Sandwichinseln': 'Südgeorgien die Südlichen Sandwichinseln',\n",
    "        'Heard und McDonaldinseln': 'Heard McDonaldinseln',\n",
    "        'Saint Kitts und Nevis': 'Saint Kitts Nevis',\n",
    "        'Saint-Pierre und Miquelon': 'Saint-Pierre Miquelon',\n",
    "        'Spitzbergen und Jan Mayen': 'Spitzbergen Jan Mayen',\n",
    "        'São Tomé und Príncipe': 'São Tomé Príncipe',\n",
    "        'Turks- und Caicosinseln': 'Turks- Caicosinseln',\n",
    "        'Französische Süd- und Antarktisgebiete': 'Französische Süd- Antarktisgebiete',\n",
    "        'Trinidad und Tobago': 'Trinidad Tobago',\n",
    "        'Saint Vincent und die Grenadinen': 'Saint Vincent die Grenadinen',\n",
    "        'Wallis und Futuna': 'Wallis Futuna',\n",
    "        'Antigua et Barbuda': 'Antigua Barbuda',\n",
    "        'Géorgie du Sud et les îles Sandwich du Sud': 'Géorgie du Sud les îles Sandwich du Sud',\n",
    "        'Îles Heard et MacDonald': 'Îles Heard MacDonald',\n",
    "        'Saint-Christophe et Niévès': 'Saint-Christophe Niévès',\n",
    "        'Saint-Christophe et Nevis': 'Saint-Christophe Nevis',\n",
    "        'Saint-Pierre et Miquelon': 'Saint-Pierre Miquelon',\n",
    "        'Sao Tomé et Principe': 'Sao Tomé Principe',\n",
    "        'Îles Turques et Caïques': 'Îles Turques Caïques',\n",
    "        'Trinité et Tobago': 'Trinité Tobago',\n",
    "        'Saint-Vincent et les-Grenadines': 'Saint-Vincent les-Grenadines',\n",
    "        'Wallis et Futuna': 'Wallis et Futuna',\n",
    "        'Bosnie et Herzégovine': 'Bosnie Herzégovine',\n",
    "        'Bonaire, Saint-Eustache et Saba': 'Bonaire, Saint-Eustache Saba',\n",
    "        'Svalbard et Jan Mayen': 'Svalbard Jan Mayen',\n",
    "        'Terres australes et antarctiques françaises': 'Terres australes antarctiques françaises',\n",
    "        'Antigua e Barbuda': 'Antigua Barbuda',\n",
    "        'Bonaire, Sint Eustatius e Saba': 'Bonaire, Sint Eustatius Saba',\n",
    "        'Georgia del Sud e isole Sandwich meridionali': 'Georgia del Sud isole Sandwich meridionali',\n",
    "        'Isole Heard e McDonald': 'Isole Heard McDonald',\n",
    "        'Saint Kitts e Nevis': 'Saint Kitts Nevis',\n",
    "        'serbo e montenegrino': 'serbo montenegrino',\n",
    "        'serba e montenegrina': 'serba montenegrina',\n",
    "        'Saint-Pierre e Miquelon': 'Saint-Pierre Miquelon',\n",
    "        'Svalbard e Jan Mayen': 'Svalbard Jan Mayen',\n",
    "        'São Tomé e Príncipe': 'São Tomé Príncipe',\n",
    "        'Isole Turks e Caicos': 'Isole Turks Caicos',\n",
    "        'Trinidad e Tobago': 'Trinidad Tobago',\n",
    "        'Saint Vincent e Grenadine': 'Saint Vincent Grenadine',\n",
    "        'Wallis e Futuna': 'Wallis Futuna',\n",
    "        'Bosnia ed Erzegovina': 'Bosnia Erzegovina',\n",
    "    }\n",
    "    for original_name, replacement in withand2noand.items():\n",
    "        df[orig_col] = df[orig_col].str.replace(original_name, replacement, regex=False)\n",
    "    if orig_col == 'nationality':\n",
    "        loc_split = df[orig_col].str.split(r'\\sund\\s|\\set\\s|\\se\\s|\\sed\\s|\\s,\\s', regex=True, expand=True)  # Also split by comma\n",
    "    else:\n",
    "        loc_split = df[orig_col].str.split(r'\\sund\\s|\\set\\s|\\se\\s|\\sed\\s', regex=True, expand=True)\n",
    "    loc_split.columns = [f'{orig_col}_{i+1}' for i in range(loc_split.shape[1])]\n",
    "    loc_split.fillna('', inplace=True)\n",
    "    for col in loc_split.columns:\n",
    "        for replacement, original_name in withand2noand.items():\n",
    "            loc_split[col] = loc_split[col].str.replace(original_name, replacement, regex=False)\n",
    "    df = pd.concat([df, loc_split], axis=1)\n",
    "    return df.drop(columns=[orig_col])\n",
    "\n",
    "# Sometimes 'de et à' or 'von und in' was not correctly parsed by the LLM, so we ensure that it is the same value\n",
    "df_people_concat.loc[df_people_concat['hometown'] == 'de et', 'hometown'] = df_people_concat['place_of_residence']\n",
    "df_people_concat.loc[df_people_concat['hometown'].isin(['von', 'de', 'da']), 'hometown'] = df_people_concat['place_of_residence'].str.replace(r'in |à |au ', '')\n",
    "\n",
    "df_people_concat.loc[df_people_concat['hometown'].str.contains(r'de et à |du et au |von und in '), 'place_of_residence'] = df_people_concat['hometown'].str.replace(r'de et à |du et au |von und in ', '')\n",
    "df_people_concat.loc[df_people_concat['place_of_residence'].str.contains(r'de et à |du et au |von und in |und in '), 'hometown'] = df_people_concat['place_of_residence'].str.replace(r'de et à |du et au |von und in |und in ', '')\n",
    "\n",
    "df_people_concat['hometown'] = df_people_concat['hometown'].str.replace(r'de et à |du et au |von und in ', '')\n",
    "df_people_concat['place_of_residence'] = df_people_concat['place_of_residence'].str.replace(r'de et à |du et au |von und in ', '')\n",
    "\n",
    "# Here we want to correct entries like 'de Bagnes, à Londres , GBR' to move GBR into the place of residence col because it is not the nationality of the person\n",
    "mask = (df_people_concat['nationality'].str.isupper() & (df_people_concat['nationality'] != 'CH'))\n",
    "df_people_concat.loc[mask, 'place_of_residence'] = (\n",
    "    df_people_concat.loc[mask, 'place_of_residence'] + \n",
    "    ' ( ' + df_people_concat.loc[mask, 'nationality'] + ' )'\n",
    ")\n",
    "df_people_concat.loc[mask, 'nationality'] = ''  # remove nationality\n",
    "\n",
    "# First, split the hometown and place of residence column\n",
    "df_people_concat = split_locations(df_people_concat, 'hometown')\n",
    "df_people_concat = split_locations(df_people_concat, 'place_of_residence')\n",
    "\n",
    "# Remove artifacts from the parsing to clean the location names\n",
    "for col in [c for c in df_people_concat.columns if c.startswith('hometown_')]:\n",
    "    df_people_concat[col] = df_people_concat[col].apply(lambda x: clean_location(LANGUAGE, x))\n",
    "\n",
    "for col in [c for c in df_people_concat.columns if c.startswith('place_of_residence_')]:\n",
    "    df_people_concat[col] = df_people_concat[col].apply(lambda x: clean_location(LANGUAGE, x))\n",
    "\n",
    "df_people_concat['nationality'] = df_people_concat['nationality'].apply(lambda x: clean_location(LANGUAGE, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Move nationalities that are in the wrong column\n",
    "def contains_target_word(text):\n",
    "    pattern = r'\\bstaatsangehoerige\\b|\\bstaatsbuergerin\\b|\\bbuergerin\\b|\\bcittadina\\b|\\bressortissante\\b|\\bcitoyenne\\b|\\bstaatsangehoeriger\\b|\\bstaatsbuerger\\b|\\bbuerger\\b|\\bcittadino\\b|\\bressortissant\\b|\\bcitoyen\\b'\n",
    "    return bool(re.search(pattern, text))\n",
    "\n",
    "def move_nationalities(language: str, entries: list[str], nat: str, country_names: set[str]):\n",
    "    \"\"\"\n",
    "    Checks if any of the hometown columns contains a country name\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'de': 'und',\n",
    "        'fr': 'et',\n",
    "        'it': 'e'\n",
    "    }\n",
    "    nat_norm = normalize_words(nat)        \n",
    "    for i, entry in enumerate(entries):\n",
    "        if entry:\n",
    "            entry_norm = normalize_words(entry)\n",
    "            if (entry_norm in country_names or contains_target_word(entry_norm)) and not entry_norm in ['mex', 'sur']:\n",
    "                if not re.match(rf'\\b{entry_norm}\\b', nat_norm):\n",
    "                    nat = f\"{nat} {mapping.get(language, 'und')} {entry}\" if nat else entry\n",
    "                entries[i] = ''\n",
    "            else:\n",
    "                # If the name is not a country name and the nationality does not include Swiss yet,\n",
    "                # we want to add 'CH' to the nationalities, since the hometown is with high probability a Swiss municipality\n",
    "                if 'CH' not in nat:\n",
    "                    nat = f\"CH {mapping.get(language, 'und')} {nat}\" if nat else 'CH'\n",
    "    return entries + [nat]\n",
    "\n",
    "countries_norm = nationality_mapping.keys()\n",
    "hometown_cols = [col for col in df_people_concat.columns if 'hometown' in col]\n",
    "result_cols = hometown_cols + ['nationality']\n",
    "df_people_concat[result_cols] = df_people_concat.apply(lambda x: pd.Series(move_nationalities(LANGUAGE, [x[col] for col in hometown_cols], x['nationality'], countries_norm)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, split the nationality column\n",
    "df_people_concat = split_locations(df_people_concat, 'nationality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the authorization and shares column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_auth_and_shares(language: str, auth: str, shares: str):\n",
    "    \"\"\"\n",
    "    Checks if any of the hometown columns contains a country name\n",
    "    \"\"\"\n",
    "    keyword_mapping = {\n",
    "        'de': 'unterschrift',\n",
    "        'fr': 'signature',\n",
    "        'it': 'firma'\n",
    "    }\n",
    "    and_mapping = {\n",
    "        'de': 'und',\n",
    "        'fr': 'et',\n",
    "        'it': 'e'\n",
    "    }\n",
    "\n",
    "    # Base Case: no value in both\n",
    "    if not (auth or shares):\n",
    "        return [auth, shares]  # no switch\n",
    "    \n",
    "    match_auth = re.search(r'\\bchf\\b', auth.lower()) if auth else None\n",
    "    match_shares = re.search(keyword_mapping[language], shares.lower()) if shares else None\n",
    "\n",
    "    # Case 0: no match in both\n",
    "    if not (match_auth or match_shares):\n",
    "        return [auth, shares]  # no switch\n",
    "\n",
    "    # Case 1: match in auth and no match in shares\n",
    "    elif match_auth and not match_shares:\n",
    "        if shares:\n",
    "            return ['', f\"{auth} {and_mapping[language]} {shares}\"]  # Add auth infront of shares\n",
    "        else:\n",
    "            return ['', auth]  # switch columns: auth, share\n",
    "    \n",
    "    # Case 2: match in shares and no value in auth\n",
    "    elif match_shares and not match_auth:\n",
    "        if shares:\n",
    "            return [f\"{auth} {and_mapping[language]} {shares}\", '']  # Add shares after auth\n",
    "        else:\n",
    "            return [shares, '']  # switch columns: auth, share\n",
    "    \n",
    "    # Case 3: match in auth and shares\n",
    "    elif match_auth and not match_shares:\n",
    "        return [shares, auth]  # switch both\n",
    "    \n",
    "    else:\n",
    "        return [auth, shares]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_concat[['signing_rights', 'shares']] = df_people_concat.apply(lambda x: pd.Series(switch_auth_and_shares(LANGUAGE, x['signing_rights'], x['shares'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the nationality and add the iso-3166-1 alpha 2 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_country(nationality: str, mapping: dict, mapping_norm: dict) -> str:\n",
    "    if nationality:\n",
    "        nationality = re.sub(r'\\bstaatsangehoeriger\\b|\\bstaatsbuerger\\b|\\bbuerger\\b|\\bcittadino\\b|\\bressortissant\\b|\\bcitoyen\\b|\\bstaatsangehoerige\\b|\\bstaatsbuergerin\\b|\\bbuergerin\\b|\\bcittadina\\b|\\bressortissante\\b|\\bcitoyenne\\b', '', nationality)\n",
    "        match = mapping.get(nationality.strip())\n",
    "        if match:\n",
    "            return match\n",
    "        else:\n",
    "            return mapping_norm.get(remove_articles(nationality), '')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': ''}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped = {country: map_country(normalize_words(country), nationality_mapping, nationality_mapping_norm) for country in df_people_concat.nationality_1.unique()}\n",
    "{k: v for k, v in mapped.items() if v == ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_cols = [col for col in df_people_concat.columns if re.match(r'^nationality\\_\\d{1}$', col)]\n",
    "for nat_col in nat_cols:\n",
    "    df_people_concat[f\"{nat_col}_norm\"] = df_people_concat[nat_col].fillna('').apply(normalize_words)\n",
    "    df_people_concat[f'{nat_col}_iso_3166_1_alpha_2'] = df_people_concat[f\"{nat_col}_norm\"].apply(lambda x: map_country(x, nationality_mapping, nationality_mapping_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort(entries: list) -> list:\n",
    "    \"\"\"Sorts the unique nationality codes such that CH always comes first if present\"\"\"\n",
    "    if len(entries) > 0 and 'CH' in entries:\n",
    "        remainder = entries.remove('CH')\n",
    "        return ['CH'] + (remainder if remainder else [])\n",
    "    return entries\n",
    "\n",
    "\n",
    "nat_cols = [col for col in df_people_concat.columns if col.endswith('_iso_3166_1_alpha_2')]\n",
    "for i in range(len(nat_cols)-1):\n",
    "    for n in range(i+1,len(nat_cols)):\n",
    "        df_people_concat.loc[df_people_concat[nat_cols[i]] == '', nat_cols[i]] = df_people_concat[nat_cols[n]]\n",
    "\n",
    "unique_codes = df_people_concat[nat_cols].apply(lambda row: custom_sort(list(pd.unique(row))), axis=1)\n",
    "result_df = pd.DataFrame(unique_codes.to_list())\n",
    "result_df = result_df.replace('', None).dropna(how='all', axis=1).fillna(pd.NA)\n",
    "result_df = result_df.rename(columns={i: f'nationality_{i+1}_iso_3166_1_alpha_2' for i in range(result_df.shape[1])})\n",
    "df_people_concat = pd.concat([df_people_concat.drop(columns=nat_cols), result_df], axis=1)\n",
    "\n",
    "ht_cols = [col for col in df_people_concat.columns if col.startswith('hometown_')]\n",
    "for i in range(len(ht_cols)-1):\n",
    "    for n in range(i+1,len(ht_cols)):\n",
    "        df_people_concat.loc[df_people_concat[ht_cols[i]] == '', ht_cols[i]] = df_people_concat[ht_cols[n]]\n",
    "\n",
    "unique_codes = df_people_concat[ht_cols].apply(lambda row: list(pd.unique(row)), axis=1)\n",
    "result_df = pd.DataFrame(unique_codes.to_list())\n",
    "result_df = result_df.replace('', None).dropna(how='all', axis=1).fillna(pd.NA)\n",
    "result_df = result_df.rename(columns={i: f'hometown_{i+1}' for i in range(result_df.shape[1])})\n",
    "df_people_concat = pd.concat([df_people_concat.drop(columns=ht_cols), result_df], axis=1)\n",
    "\n",
    "pr_cols = [col for col in df_people_concat.columns if col.startswith('place_of_residence_')]\n",
    "for i in range(len(pr_cols)-1):\n",
    "    for n in range(i+1,len(pr_cols)):\n",
    "        df_people_concat.loc[df_people_concat[pr_cols[i]] == '', pr_cols[i]] = df_people_concat[pr_cols[n]]\n",
    "\n",
    "unique_codes = df_people_concat[pr_cols].apply(lambda row: list(pd.unique(row)), axis=1)\n",
    "result_df = pd.DataFrame(unique_codes.to_list())\n",
    "result_df = result_df.replace('', None).dropna(how='all', axis=1).fillna(pd.NA)\n",
    "result_df = result_df.rename(columns={i: f'place_of_residence_{i+1}' for i in range(result_df.shape[1])})\n",
    "df_people_concat = pd.concat([df_people_concat.drop(columns=pr_cols), result_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find gendered job titles and/or determine gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendered_endings = {\n",
    "    'de': {\n",
    "        'female': ['in'],\n",
    "        'male': [],  # no specific ending for male words in German\n",
    "    },\n",
    "    'fr': {\n",
    "        'female': ['euse', 'ienne', 'onne', 'ane', 'trice', 'esse'],\n",
    "        'male': ['eur', 'ien', 'on', 'an'],\n",
    "    },\n",
    "    'it': {\n",
    "        'female': ['a', 'trice', 'essa'],\n",
    "        'male': ['o', 'ore']\n",
    "    }   \n",
    "}\n",
    "\n",
    "\n",
    "def extract_nouns(text: str, language: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts nouns from a given text using spaCy for German, French, and Italian.\n",
    "    \"\"\"\n",
    "    nlp = nlp_models[language]\n",
    "    return [token.text for token in nlp(text) if token.pos_ == 'NOUN']\n",
    "\n",
    "\n",
    "def create_gendered_job_names(language: str, df: pd.DataFrame, col: str = 'job_title_norm') -> tuple[list]:\n",
    "    nouns = set([title.lower() for title in df[col].unique() for title in extract_nouns(title, language)])\n",
    "       \n",
    "    female_words = []\n",
    "    male_words = []\n",
    "    undetermined = []\n",
    "\n",
    "    for word in nouns:\n",
    "        if any(word.endswith(ending) for ending in gendered_endings[language]['female']):\n",
    "            female_words.append(word)\n",
    "        elif any(word.endswith(ending) for ending in gendered_endings[language]['male']):\n",
    "            male_words.append(word)\n",
    "        else:\n",
    "            undetermined.append(word)\n",
    "\n",
    "    if language == 'de':\n",
    "        for word in female_words:\n",
    "            male_version = word.removesuffix('in')\n",
    "            if male_version in undetermined:\n",
    "                male_words.append(male_version)\n",
    "        \n",
    "    undetermined = [w for w in undetermined if w not in male_words]\n",
    "\n",
    "    return female_words, male_words, undetermined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize job title\n",
    "df_people_concat['job_title_norm'] = df_people_concat['job_title'].apply(normalize_words)\n",
    "df_people_concat['job_title_norm'] = df_people_concat['job_title_norm'].str.replace(r'[^a-zA-Z]', ' ', regex=True).apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_GENDERED_WORDS = False\n",
    "\n",
    "if CREATE_GENDERED_WORDS:\n",
    "    female_words, male_words, undetermined = create_gendered_job_names('fr', df_people_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_names(name: str) -> str:\n",
    "    name = re.sub(r' genannt | dit | dite | detto | detta ', ' ', name)\n",
    "    name = re.sub(r'\\b\\w+\\.\\s?', '', name)  # Remove things like 'Dr.', 'Prof.', etc.\n",
    "    name = re.sub(r'\\(.*?\\)', '', name)\n",
    "    name = re.sub(r'\\[.*?\\]', '', name)\n",
    "    name = re.sub(r'[^\\w\\s\\-.]', '', name)\n",
    "    name = re.sub(r'\\b\\d+\\b', '', name)\n",
    "    name = re.sub(r'\\b\\w{1,4}\\.(?=\\s|$)', '', name)\n",
    "    return ' '.join(name.strip().split())\n",
    "\n",
    "\n",
    "def prepare_country_code(code: str) -> str:\n",
    "    code = code if code != 'XK' else 'RS'  # gender API does not support Kosovo (XK)\n",
    "    return code if len(code) <= 2 else None\n",
    "\n",
    "\n",
    "def determine_gender(\n",
    "    mapping: dict,\n",
    "    nationalities: list[str],\n",
    "    job_title: str\n",
    ") -> str|None:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Try to infer gender via nationality\n",
    "    for nationality in nationalities:\n",
    "        if re.search(r'\\bstaatsangehoeriger\\b|\\bstaatsbuerger\\b|\\bbuerger\\b|\\bcittadino\\b|\\bressortissant\\b|\\bcitoyen\\b', nationality):\n",
    "            return 'm'\n",
    "        elif re.search(r'\\bstaatsangehoerige\\b|\\bstaatsbuergerin\\b|\\bbuergerin\\b|\\bcittadina\\b|\\bressortissante\\b|\\bcitoyenne\\b', nationality):\n",
    "            return 'f'\n",
    "\n",
    "    # Try to infer gender via job title\n",
    "    genders = [mapping[w] for w in mapping.keys() if re.search(rf'(?<!\\w){w}(?!\\w)', job_title)]\n",
    "    if genders:\n",
    "        # Check if list only contains one gender\n",
    "        if genders.count(genders[0]) == len(genders):\n",
    "            return genders[0]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_norm_cols = [col for col in df_people_concat.columns if re.match(r'\\bnationality_\\d{1}_norm\\b', col, flags=re.IGNORECASE)]\n",
    "df_people_concat['gender'] = df_people_concat.apply(lambda x: determine_gender(gender_mapping[LANGUAGE], [x[col] for col in nat_norm_cols], x['job_title_norm']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zefix_gender_mapping = zefix_gender_mapping.rename(columns={'name': 'first_name_norm', 'country_of_origin': 'nationality_1_iso_3166_1_alpha_2', 'gender': 'gender_name'})\n",
    "zefix_gender_mapping['first_name_norm'] = zefix_gender_mapping['first_name_norm'].apply(lambda x: clean_names(normalize_words(x)))\n",
    "zefix_gender_mapping = zefix_gender_mapping.drop_duplicates(subset=['first_name_norm', 'nationality_1_iso_3166_1_alpha_2'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "zefix_gender_mapping['split_first_names'] = zefix_gender_mapping[(zefix_gender_mapping.probability > 0.6) & (zefix_gender_mapping.request_type == 'first_name')]['first_name_norm'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jaccard_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_names = zefix_gender_mapping.explode(column=['split_first_names']).dropna(subset=['split_first_names'])\n",
    "\n",
    "# First drop duplicate male names and duplicate female names by only keeping the first names\n",
    "individual_names = individual_names.drop_duplicates(subset=['split_first_names', 'gender_name'], keep='first')\n",
    "\n",
    "# Then drop duplicate names that are both male and female completely\n",
    "individual_names = individual_names.drop_duplicates(subset=['split_first_names'], keep=False)\n",
    "\n",
    "# Only keep names with at least 3 letters\n",
    "individual_names = individual_names[individual_names.split_first_names.str.len() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148214\n"
     ]
    }
   ],
   "source": [
    "print(len(df_people_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_concat['first_name_norm'] = df_people_concat['first_name'].apply(lambda x: clean_names(normalize_words(x)))\n",
    "df_people_concat = df_people_concat.merge(zefix_gender_mapping[['first_name_norm', 'nationality_1_iso_3166_1_alpha_2', 'gender_name']], on=['first_name_norm', 'nationality_1_iso_3166_1_alpha_2'], how='left')\n",
    "df_people_concat.loc[df_people_concat.gender.isna(), 'gender'] = df_people_concat['gender_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/zcxfhwyx0yx0sp_yprqlc61c0000gn/T/ipykernel_56769/1629553863.py:13: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask & df_people_concat['first_name_norm'].str.contains(male_pattern) &\n",
      "/var/folders/lm/zcxfhwyx0yx0sp_yprqlc61c0000gn/T/ipykernel_56769/1629553863.py:14: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ~df_people_concat['first_name_norm'].str.contains(female_pattern),\n",
      "/var/folders/lm/zcxfhwyx0yx0sp_yprqlc61c0000gn/T/ipykernel_56769/1629553863.py:19: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask & df_people_concat['first_name_norm'].str.contains(female_pattern) &\n",
      "/var/folders/lm/zcxfhwyx0yx0sp_yprqlc61c0000gn/T/ipykernel_56769/1629553863.py:20: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  ~df_people_concat['first_name_norm'].str.contains(male_pattern),\n"
     ]
    }
   ],
   "source": [
    "male_names = individual_names[individual_names.gender_name == 'm']['split_first_names']\n",
    "female_names = individual_names[individual_names.gender_name == 'f']['split_first_names']\n",
    "\n",
    "# Compile regex patterns once (efficient)\n",
    "male_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, male_names)) + r')\\b')\n",
    "female_pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, female_names)) + r')\\b')\n",
    "\n",
    "# Mask for rows with gender NaN\n",
    "mask = df_people_concat['gender'].isna()\n",
    "\n",
    "# Apply patterns efficiently using pandas string methods\n",
    "df_people_concat.loc[\n",
    "    mask & df_people_concat['first_name_norm'].str.contains(male_pattern) &\n",
    "    ~df_people_concat['first_name_norm'].str.contains(female_pattern),\n",
    "    'gender'\n",
    "] = 'm'\n",
    "\n",
    "df_people_concat.loc[\n",
    "    mask & df_people_concat['first_name_norm'].str.contains(female_pattern) &\n",
    "    ~df_people_concat['first_name_norm'].str.contains(male_pattern),\n",
    "    'gender'\n",
    "] = 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Gender: 23781\n"
     ]
    }
   ],
   "source": [
    "print(f'Missing Gender: {len(df_people_concat[df_people_concat.gender.isna()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_concat['founders'] = [int('status.neu' in codes) for codes in df_people_concat['codes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dataframe_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    required_columns = [\n",
    "        \"ehraid\", \"shab_date\", \"shab_id\", \"keyword\", \"first_name\", \"last_name\",\n",
    "        \"job_title\", \"signing_rights\", \"shares\",\n",
    "        \"hometown_1\", \"hometown_2\", \"hometown_3\", \"hometown_4\", \"hometown_5\",\n",
    "        \"place_of_residence_1\", \"place_of_residence_2\",\n",
    "        \"nationality_1_iso_3166_1_alpha_2\", \"nationality_2_iso_3166_1_alpha_2\", \"nationality_3_iso_3166_1_alpha_2\",\n",
    "        \"gender\", \"founders\"\n",
    "    ]\n",
    "\n",
    "    # Add missing columns with NaN values\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    # Ensure correct column order\n",
    "    df = df[required_columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people_concat = ensure_dataframe_columns(df_people_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_firms_concat = df_firms_concat.drop(columns=['message_raw', 'codes'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>job_title</th>\n",
       "      <th>signing_rights</th>\n",
       "      <th>shares</th>\n",
       "      <th>hometown_1</th>\n",
       "      <th>...</th>\n",
       "      <th>hometown_3</th>\n",
       "      <th>hometown_4</th>\n",
       "      <th>hometown_5</th>\n",
       "      <th>place_of_residence_1</th>\n",
       "      <th>place_of_residence_2</th>\n",
       "      <th>nationality_1_iso_3166_1_alpha_2</th>\n",
       "      <th>nationality_2_iso_3166_1_alpha_2</th>\n",
       "      <th>nationality_3_iso_3166_1_alpha_2</th>\n",
       "      <th>gender</th>\n",
       "      <th>founders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>1004711015</td>\n",
       "      <td>ausgeschiedene personen und erloschene untersc...</td>\n",
       "      <td>Philippe</td>\n",
       "      <td>Nappez</td>\n",
       "      <td>Mitglied des Verwaltungsrates</td>\n",
       "      <td>mit Kollektivunterschrift zu zweien</td>\n",
       "      <td></td>\n",
       "      <td>Grandfontaine</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Binningen</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CH</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>1004711015</td>\n",
       "      <td>eingetragene personen neu oder mutierend</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>Ebneter</td>\n",
       "      <td>Mitglied des Verwaltungsrates</td>\n",
       "      <td>mit Kollektivunterschrift zu zweien</td>\n",
       "      <td></td>\n",
       "      <td>Häggenschwil</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Rheinfelden</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CH</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>1004825123</td>\n",
       "      <td>eingetragene personen neu oder mutierend</td>\n",
       "      <td>Gabriella</td>\n",
       "      <td>Karger Travella</td>\n",
       "      <td>Mitglied des Verwaltungsrates</td>\n",
       "      <td>mit Kollektivunterschrift zu zweien</td>\n",
       "      <td></td>\n",
       "      <td>Basel</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Basel</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CH</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>1004825123</td>\n",
       "      <td>eingetragene personen neu oder mutierend</td>\n",
       "      <td>Julien</td>\n",
       "      <td>Orsini</td>\n",
       "      <td>Liquidator</td>\n",
       "      <td>mit Einzelunterschrift</td>\n",
       "      <td></td>\n",
       "      <td>Basel</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Reinach ( BL )</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CH</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>2022-04-19</td>\n",
       "      <td>1005453034</td>\n",
       "      <td>eingetragene personen neu oder mutierend</td>\n",
       "      <td>Urs</td>\n",
       "      <td>Antener</td>\n",
       "      <td>Präsident des Verwaltungsrates</td>\n",
       "      <td>mit Einzelunterschrift</td>\n",
       "      <td></td>\n",
       "      <td>Eggiwil</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Sarnen</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CH</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ehraid   shab_date     shab_id  \\\n",
       "0       2  2019-09-06  1004711015   \n",
       "1       2  2019-09-06  1004711015   \n",
       "2       2  2020-02-07  1004825123   \n",
       "3       2  2020-02-07  1004825123   \n",
       "4      15  2022-04-19  1005453034   \n",
       "\n",
       "                                             keyword first_name  \\\n",
       "0  ausgeschiedene personen und erloschene untersc...   Philippe   \n",
       "1           eingetragene personen neu oder mutierend     Daniel   \n",
       "2           eingetragene personen neu oder mutierend  Gabriella   \n",
       "3           eingetragene personen neu oder mutierend     Julien   \n",
       "4           eingetragene personen neu oder mutierend        Urs   \n",
       "\n",
       "         last_name                       job_title  \\\n",
       "0           Nappez   Mitglied des Verwaltungsrates   \n",
       "1          Ebneter   Mitglied des Verwaltungsrates   \n",
       "2  Karger Travella   Mitglied des Verwaltungsrates   \n",
       "3           Orsini                      Liquidator   \n",
       "4          Antener  Präsident des Verwaltungsrates   \n",
       "\n",
       "                        signing_rights shares     hometown_1  ... hometown_3  \\\n",
       "0  mit Kollektivunterschrift zu zweien         Grandfontaine  ...       <NA>   \n",
       "1  mit Kollektivunterschrift zu zweien          Häggenschwil  ...       <NA>   \n",
       "2  mit Kollektivunterschrift zu zweien                 Basel  ...       <NA>   \n",
       "3               mit Einzelunterschrift                 Basel  ...       <NA>   \n",
       "4               mit Einzelunterschrift               Eggiwil  ...       <NA>   \n",
       "\n",
       "  hometown_4 hometown_5 place_of_residence_1 place_of_residence_2  \\\n",
       "0       <NA>       <NA>            Binningen                 <NA>   \n",
       "1       <NA>       <NA>          Rheinfelden                 <NA>   \n",
       "2       <NA>       <NA>                Basel                 <NA>   \n",
       "3       <NA>       <NA>       Reinach ( BL )                 <NA>   \n",
       "4       <NA>       <NA>               Sarnen                 <NA>   \n",
       "\n",
       "  nationality_1_iso_3166_1_alpha_2 nationality_2_iso_3166_1_alpha_2  \\\n",
       "0                               CH                             <NA>   \n",
       "1                               CH                             <NA>   \n",
       "2                               CH                             <NA>   \n",
       "3                               CH                             <NA>   \n",
       "4                               CH                             <NA>   \n",
       "\n",
       "  nationality_3_iso_3166_1_alpha_2 gender founders  \n",
       "0                             <NA>      m        0  \n",
       "1                             <NA>      m        0  \n",
       "2                             <NA>      f        0  \n",
       "3                             <NA>      m        0  \n",
       "4                             <NA>      m        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_INSCRIBED_PEOPLE = f\"\"\"CREATE TABLE IF NOT EXISTS zefix.history_inscribed_people (\n",
    "                ehraid INT,\n",
    "                shab_date DATE,\n",
    "                shab_id INT,\n",
    "                keyword TEXT,\n",
    "                first_name TEXT,\n",
    "                last_name TEXT,\n",
    "                job_title TEXT,\n",
    "                signing_rights TEXT,\n",
    "                shares TEXT,\n",
    "                hometown_1 TEXT,\n",
    "                hometown_2 TEXT,\n",
    "                hometown_3 TEXT,\n",
    "                hometown_4 TEXT,\n",
    "                hometown_5 TEXT,\n",
    "                place_of_residence_1 TEXT,\n",
    "                place_of_residence_2 TEXT,\n",
    "                nationality_1_iso_3166_1_alpha_2 TEXT,\n",
    "                nationality_2_iso_3166_1_alpha_2 TEXT,\n",
    "                nationality_3_iso_3166_1_alpha_2 TEXT,\n",
    "                gender TEXT,\n",
    "                founders BOOLEAN\n",
    "            )\"\"\"\n",
    "\n",
    "INIT_HISTORY_INSCRIBED_FIRMS = f\"\"\"CREATE TABLE IF NOT EXISTS zefix.history_inscribed_firms (\n",
    "                ehraid INT,\n",
    "                shab_date DATE,\n",
    "                shab_id INT,\n",
    "                keyword TEXT,\n",
    "                firm_name TEXT,\n",
    "                firm_uid TEXT,\n",
    "                firm_seat TEXT,\n",
    "                firm_type TEXT,\n",
    "                firm_shares TEXT\n",
    "            )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "name2table = {\n",
    "    'history_inscribed_people': (INIT_HISTORY_INSCRIBED_PEOPLE, df_people_concat),\n",
    "    'history_inscribed_firms': (INIT_HISTORY_INSCRIBED_FIRMS, df_firms_concat),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_people_concat.to_csv(PROCESSED_DATA_DIR / f'people_and_firms_{LANGUAGE}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE HISTORICAL PURPOSE FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_df = shab_merged[shab_merged.category == 'purpose'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_lists = [l for l in purpose_df.text_slices if l and len(l) > 1]\n",
    "assert len(purpose_lists) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_df['purpose_raw'] = [' . '.join(purpose) if purpose else '' for purpose in purpose_df['text_slices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>registry_office_canton</th>\n",
       "      <th>message_raw</th>\n",
       "      <th>codes</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_slices</th>\n",
       "      <th>parsed_variables</th>\n",
       "      <th>category</th>\n",
       "      <th>purpose_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>54</td>\n",
       "      <td>1005740573</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>SO</td>\n",
       "      <td>Aare Finanz- und Holding-AG, in Olten, CHE-102...</td>\n",
       "      <td>[zweckaenderung, kapitalaenderung, kapitalaend...</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Übernahme und Verwaltung von Beteiligungen so...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Übernahme und Verwaltung von Beteiligungen sow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>55</td>\n",
       "      <td>1005420418</td>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>AG</td>\n",
       "      <td>Aaraucar AG, in Aarau, CHE-101.892.069, Aktien...</td>\n",
       "      <td>[zweckaenderung]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Zweck der Gesellschaft ist der Erwerb , die V...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Zweck der Gesellschaft ist der Erwerb , die Ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>57</td>\n",
       "      <td>1005770142</td>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>BE</td>\n",
       "      <td>GASSER CERAMIC AG, in Rapperswil (BE), CHE-102...</td>\n",
       "      <td>[zweckaenderung, aenderungkapitalband]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Die Gesellschaft bezweckt das Halten und Verw...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Die Gesellschaft bezweckt das Halten und Verwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>72</td>\n",
       "      <td>1004882905</td>\n",
       "      <td>2020-05-05</td>\n",
       "      <td>BE</td>\n",
       "      <td>Aare-Kies AG, in Kirchdorf (BE), CHE-104.079.6...</td>\n",
       "      <td>[zweckaenderung, aenderungorgane]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Die Gesellschaft bezweckt die Gewinnung , Auf...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Die Gesellschaft bezweckt die Gewinnung , Aufb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>74</td>\n",
       "      <td>1004675877</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>AG</td>\n",
       "      <td>Aare-Taxi AG, in Brugg, CHE-107.123.578, Aktie...</td>\n",
       "      <td>[zweckaenderung, aenderungorgane]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Führen eines Taxigeschäftes mit Festangestell...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Führen eines Taxigeschäftes mit Festangestellt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274311</th>\n",
       "      <td>1682040</td>\n",
       "      <td>1006272424</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>LU</td>\n",
       "      <td>Hooshyarsangari, in Luzern, CHE-455.021.858, L...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Durchführung von Umzügen und Reinigungsarbeit...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Durchführung von Umzügen und Reinigungsarbeite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274319</th>\n",
       "      <td>1682041</td>\n",
       "      <td>1006272425</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>LU</td>\n",
       "      <td>LA Capital AG, in Beromünster, CHE-343.494.505...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Die Gesellschaft bezweckt den Erwerb , das Ha...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Die Gesellschaft bezweckt den Erwerb , das Hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274325</th>\n",
       "      <td>1682042</td>\n",
       "      <td>1006272426</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>LU</td>\n",
       "      <td>Mentor Gerüst GmbH, in Wauwil, CHE-483.874.731...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Die Gesellschaft bezweckt die Vermietung , Mo...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Die Gesellschaft bezweckt die Vermietung , Mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274328</th>\n",
       "      <td>1682043</td>\n",
       "      <td>1006272427</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>LU</td>\n",
       "      <td>REINIGUNG - SARACENO, in Rickenbach (LU), CHE-...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Reinigung von Privatwohnungen ; gewerbliche B...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Reinigung von Privatwohnungen ; gewerbliche Bü...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274334</th>\n",
       "      <td>1682044</td>\n",
       "      <td>1006272428</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>LU</td>\n",
       "      <td>swiss-immo point gmbh, in Beromünster, CHE-306...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>zweck neu</td>\n",
       "      <td>[Die Gesellschaft bezweckt das Projektmanageme...</td>\n",
       "      <td>{}</td>\n",
       "      <td>purpose</td>\n",
       "      <td>Die Gesellschaft bezweckt das Projektmanagemen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>389162 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ehraid     shab_id   shab_date registry_office_canton  \\\n",
       "107           54  1005740573  2023-05-08                     SO   \n",
       "110           55  1005420418  2022-03-04                     AG   \n",
       "117           57  1005770142  2023-06-16                     BE   \n",
       "161           72  1004882905  2020-05-05                     BE   \n",
       "169           74  1004675877  2019-07-15                     AG   \n",
       "...          ...         ...         ...                    ...   \n",
       "5274311  1682040  1006272424  2025-03-04                     LU   \n",
       "5274319  1682041  1006272425  2025-03-04                     LU   \n",
       "5274325  1682042  1006272426  2025-03-04                     LU   \n",
       "5274328  1682043  1006272427  2025-03-04                     LU   \n",
       "5274334  1682044  1006272428  2025-03-04                     LU   \n",
       "\n",
       "                                               message_raw  \\\n",
       "107      Aare Finanz- und Holding-AG, in Olten, CHE-102...   \n",
       "110      Aaraucar AG, in Aarau, CHE-101.892.069, Aktien...   \n",
       "117      GASSER CERAMIC AG, in Rapperswil (BE), CHE-102...   \n",
       "161      Aare-Kies AG, in Kirchdorf (BE), CHE-104.079.6...   \n",
       "169      Aare-Taxi AG, in Brugg, CHE-107.123.578, Aktie...   \n",
       "...                                                    ...   \n",
       "5274311  Hooshyarsangari, in Luzern, CHE-455.021.858, L...   \n",
       "5274319  LA Capital AG, in Beromünster, CHE-343.494.505...   \n",
       "5274325  Mentor Gerüst GmbH, in Wauwil, CHE-483.874.731...   \n",
       "5274328  REINIGUNG - SARACENO, in Rickenbach (LU), CHE-...   \n",
       "5274334  swiss-immo point gmbh, in Beromünster, CHE-306...   \n",
       "\n",
       "                                                     codes    keyword  \\\n",
       "107      [zweckaenderung, kapitalaenderung, kapitalaend...  zweck neu   \n",
       "110                                       [zweckaenderung]  zweck neu   \n",
       "117                 [zweckaenderung, aenderungkapitalband]  zweck neu   \n",
       "161                      [zweckaenderung, aenderungorgane]  zweck neu   \n",
       "169                      [zweckaenderung, aenderungorgane]  zweck neu   \n",
       "...                                                    ...        ...   \n",
       "5274311                               [status, status.neu]  zweck neu   \n",
       "5274319                               [status, status.neu]  zweck neu   \n",
       "5274325                               [status, status.neu]  zweck neu   \n",
       "5274328                               [status, status.neu]  zweck neu   \n",
       "5274334                               [status, status.neu]  zweck neu   \n",
       "\n",
       "                                               text_slices parsed_variables  \\\n",
       "107      [Übernahme und Verwaltung von Beteiligungen so...               {}   \n",
       "110      [Zweck der Gesellschaft ist der Erwerb , die V...               {}   \n",
       "117      [Die Gesellschaft bezweckt das Halten und Verw...               {}   \n",
       "161      [Die Gesellschaft bezweckt die Gewinnung , Auf...               {}   \n",
       "169      [Führen eines Taxigeschäftes mit Festangestell...               {}   \n",
       "...                                                    ...              ...   \n",
       "5274311  [Durchführung von Umzügen und Reinigungsarbeit...               {}   \n",
       "5274319  [Die Gesellschaft bezweckt den Erwerb , das Ha...               {}   \n",
       "5274325  [Die Gesellschaft bezweckt die Vermietung , Mo...               {}   \n",
       "5274328  [Reinigung von Privatwohnungen ; gewerbliche B...               {}   \n",
       "5274334  [Die Gesellschaft bezweckt das Projektmanageme...               {}   \n",
       "\n",
       "        category                                        purpose_raw  \n",
       "107      purpose  Übernahme und Verwaltung von Beteiligungen sow...  \n",
       "110      purpose  Zweck der Gesellschaft ist der Erwerb , die Ve...  \n",
       "117      purpose  Die Gesellschaft bezweckt das Halten und Verwa...  \n",
       "161      purpose  Die Gesellschaft bezweckt die Gewinnung , Aufb...  \n",
       "169      purpose  Führen eines Taxigeschäftes mit Festangestellt...  \n",
       "...          ...                                                ...  \n",
       "5274311  purpose  Durchführung von Umzügen und Reinigungsarbeite...  \n",
       "5274319  purpose  Die Gesellschaft bezweckt den Erwerb , das Hal...  \n",
       "5274325  purpose  Die Gesellschaft bezweckt die Vermietung , Mon...  \n",
       "5274328  purpose  Reinigung von Privatwohnungen ; gewerbliche Bü...  \n",
       "5274334  purpose  Die Gesellschaft bezweckt das Projektmanagemen...  \n",
       "\n",
       "[389162 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purpose_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_df['branch'] = [int(any(w in keyword for w in ['succursale', 'zweigniederlassung'])) for keyword in purpose_df['keyword']]\n",
    "purpose_df['main_seat'] = [int(any(w in keyword for w in ['principal', 'hauptsitz'])) for keyword in purpose_df['keyword']]\n",
    "purpose_df['founding_purpose'] = [int('status.neu' in codes) if isinstance(codes, list) else 0 for codes in purpose_df['codes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['zweck neu', 'zweck hauptsitz neu', 'zweck zweigniederlassung neu'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purpose_df.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose_df = purpose_df.drop(columns=['registry_office_canton', 'message_raw', 'codes', 'parsed_variables', 'category', 'text_slices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_PURPOSE = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_purpose (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    purpose_raw TEXT,\n",
    "    branch BOOLEAN,\n",
    "    main_seat BOOLEAN,\n",
    "    founding_purpose BOOLEAN\n",
    ");\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_purpose': (INIT_HISTORY_PURPOSE, purpose_df),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS HISTORICAL FIRM CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_changes_df = shab_merged[shab_merged.category == 'firm and address changes'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = [codes for codes in firm_changes_df.codes if isinstance(codes, list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adressaenderung',\n",
       " 'aenderungkapitalband',\n",
       " 'aenderungorgane',\n",
       " 'aenderunguid',\n",
       " 'firmenaenderung',\n",
       " 'fusion',\n",
       " 'kapitalaenderung',\n",
       " 'kapitalaenderung.libriert',\n",
       " 'kapitalaenderung.nominell',\n",
       " 'kapitalaenderung.stueckelung',\n",
       " 'rechtsformaenderung',\n",
       " 'spaltung',\n",
       " 'status',\n",
       " 'status.aufl',\n",
       " 'status.aufl.konk',\n",
       " 'status.aufl.liq',\n",
       " 'status.loeschung',\n",
       " 'status.neu',\n",
       " 'status.wiedereintrag',\n",
       " 'status.wiederrufliq',\n",
       " 'vermoegenstransfer',\n",
       " 'zweckaenderung'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{c for code in codes for c in code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'de': [\n",
    "        ['firma neu'],\n",
    "        ['adresse neu', 'weitere adressen', 'weitere adressen gestrichen', 'liquidationsadresse', 'postadresse neu'],\n",
    "        ['zweigniederlassung neu', 'zweigniederlassung gestrichen'],\n",
    "    ],\n",
    "    'fr': [\n",
    "        ['nouvelle raison sociale'],\n",
    "        ['nouvelle adresse', 'autres adresses', 'autres adresses radiées', 'adresse de liquidation', 'nouvelle adresse postale'],\n",
    "        ['nouvelle succursale', 'succursale radiée'],\n",
    "    ],\n",
    "    'it': [\n",
    "        ['nuova ditta'],\n",
    "        ['nuovo recapito', 'altri indirizzi', 'indirizzo della liquidazione', 'nuovo indirizzo postale'],\n",
    "        ['nuova succursale', 'succursale radiata'],\n",
    "    ]\n",
    "}\n",
    "firm_name_df = firm_changes_df[firm_changes_df['keyword'].isin(mapping[LANGUAGE][0])].copy()\n",
    "firm_address_df = firm_changes_df[firm_changes_df['keyword'].isin(mapping[LANGUAGE][1])].copy()\n",
    "branches_df = firm_changes_df[firm_changes_df['keyword'].isin(mapping[LANGUAGE][2])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dissolution table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>reason_for_dissolution</th>\n",
       "      <th>liquidation</th>\n",
       "      <th>bankruptcy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>1004825123</td>\n",
       "      <td>uebersetzungen der firma neu</td>\n",
       "      <td>{Die Gesellschaft ist mit Beschluss der Genera...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>2020-08-10</td>\n",
       "      <td>1004954893</td>\n",
       "      <td>firma neu</td>\n",
       "      <td>{Die Gesellschaft ist mit Beschluss der Genera...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>3873587</td>\n",
       "      <td>adresse neu</td>\n",
       "      <td>{Die Genossenschaft ist mit Beschluss der Gene...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140</td>\n",
       "      <td>2024-04-12</td>\n",
       "      <td>1006007698</td>\n",
       "      <td>liquidationsadresse</td>\n",
       "      <td>{Die Gesellschaft ist mit Beschluss der Genera...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>2672921</td>\n",
       "      <td>firma neu</td>\n",
       "      <td>{Mit Verfügung des Gerichtspräsidiums Rheinfel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ehraid   shab_date     shab_id                       keyword  \\\n",
       "0       2  2020-02-07  1004825123  uebersetzungen der firma neu   \n",
       "1      38  2020-08-10  1004954893                     firma neu   \n",
       "2      93  2017-11-16     3873587                   adresse neu   \n",
       "3     140  2024-04-12  1006007698           liquidationsadresse   \n",
       "4     185  2016-02-17     2672921                     firma neu   \n",
       "\n",
       "                              reason_for_dissolution  liquidation  bankruptcy  \n",
       "0  {Die Gesellschaft ist mit Beschluss der Genera...            1           0  \n",
       "1  {Die Gesellschaft ist mit Beschluss der Genera...            1           0  \n",
       "2  {Die Genossenschaft ist mit Beschluss der Gene...            1           0  \n",
       "3  {Die Gesellschaft ist mit Beschluss der Genera...            1           0  \n",
       "4  {Mit Verfügung des Gerichtspräsidiums Rheinfel...            0           1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dissolution_df = shab_merged[shab_merged.codes.apply(lambda x: ('status.aufl' in x) if isinstance(x, list) else False)]\n",
    "\n",
    "ehraids = []\n",
    "shab_dates = []\n",
    "shab_ids = []\n",
    "keywords = []\n",
    "codes_list = []\n",
    "reasons_for_dissolution_list = []\n",
    "\n",
    "lang2dissolution = {\n",
    "    'de': ['gerichtspräsidium', 'einzelgericht', 'zivilgericht', 'kreisgericht', 'bezirksgericht', 'kantonsgericht', 'gemäss verfügung', 'einzelrichter', 'konkursrichter', 'konkursverfahren', 'auflösung', 'aufgelöst', 'aufgehoben', 'konkurs eröffnet', 'gestorben', 'verstorben'],\n",
    "    'fr': ['tribunal', 'assemblée générale', 'assemblée des associés', 'juge unique compétent', 'cour de justice', 'selon décision des associés', 'déclarée dissoute', 'par décision', 'selon décision', 'est dissoute', 'déclarée dissoute', 'la dissolution', 'mort', 'décédé'],\n",
    "    'it': ['tribunale', 'pretura del distretto', 'assemblea generale', 'assemblea dei soci', 'assemblea sociale', 'autorità federale', 'ordinata la liquidazione', 'è sciolta', 'è dichiarata sciolta', 'con decisione', 'con decreto', 'morto', 'deceduto']\n",
    "}\n",
    "\n",
    "# 'è sciolta', 'con decreto', 'con decisione', \"sciolta d'ufficio\", \n",
    "for i, row in dissolution_df.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    ehraid = row['ehraid']\n",
    "    shab_date = row['shab_date']\n",
    "    keyword = row['keyword']\n",
    "    codes = row['codes']\n",
    "\n",
    "    text_slices = row['text_slices']\n",
    "    reasons_for_dissolution = []\n",
    "    for slice in text_slices:\n",
    "        for s in slice.split(' . '):\n",
    "            words = lang2dissolution[LANGUAGE]\n",
    "            if any(word in s.lower() for word in words):\n",
    "                reasons_for_dissolution.append(s)\n",
    "    if len(reasons_for_dissolution) > 0:\n",
    "        reasons_for_dissolution_list.append(reasons_for_dissolution)\n",
    "        shab_ids.append(shab_id)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n",
    "        keywords.append(keyword)\n",
    "        codes_list.append(codes)\n",
    "\n",
    "return_df = pd.DataFrame({\n",
    "    'ehraid': ehraids,\n",
    "    'shab_date': shab_dates,\n",
    "    'shab_id': shab_ids,\n",
    "    'keyword': keywords,\n",
    "    'codes': codes_list,\n",
    "    'reason_for_dissolution': reasons_for_dissolution_list\n",
    "})\n",
    "missing_df = dissolution_df[~dissolution_df.shab_id.isin(return_df.shab_id)][['ehraid', 'shab_date', 'shab_id', 'codes']].drop_duplicates(subset=['shab_id'], keep='first')\n",
    "missing_df['keyword'] = ''\n",
    "missing_df['reason_for_dissolution'] = '[]'\n",
    "missing_df['reason_for_dissolution'] = missing_df['reason_for_dissolution'].apply(ast.literal_eval)\n",
    "\n",
    "return_df = pd.concat([return_df, missing_df])\n",
    "return_df['reason_for_dissolution'] = return_df['reason_for_dissolution'].apply(lambda x: '{' + ','.join(map(str, x)) + '}' if x else '{}')\n",
    "return_df['liquidation'] = [int('status.aufl.liq' in codes) if isinstance(codes, list) else False for codes in return_df['codes']]\n",
    "return_df['bankruptcy'] = [int('status.aufl.konk' in codes) if isinstance(codes, list) else False for codes in return_df['codes']]\n",
    "return_df = return_df.drop(columns=['codes'])\n",
    "return_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_DISSOLUTIONS = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_dissolutions (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    reason_for_dissolution TEXT[],\n",
    "    liquidation BOOLEAN,\n",
    "    bankruptcy BOOLEAN\n",
    ");\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_dissolutions': (INIT_HISTORY_DISSOLUTIONS, return_df),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare firm name changes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>keyword</th>\n",
       "      <th>firm_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1004825123</td>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>firma neu</td>\n",
       "      <td>AA-Annoncen Agentur AG in Liquidation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>38</td>\n",
       "      <td>1004954893</td>\n",
       "      <td>2020-08-10</td>\n",
       "      <td>firma neu</td>\n",
       "      <td>AAFC Financial Consult Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>77</td>\n",
       "      <td>1005402464</td>\n",
       "      <td>2022-02-10</td>\n",
       "      <td>firma neu</td>\n",
       "      <td>ATG Aare Touring Garage AG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>83</td>\n",
       "      <td>1006046071</td>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>firma neu</td>\n",
       "      <td>Aareschlucht AG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>93</td>\n",
       "      <td>3873587</td>\n",
       "      <td>2017-11-16</td>\n",
       "      <td>firma neu</td>\n",
       "      <td>Genossenschaft PRO BON AARGAU in Liq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ehraid     shab_id   shab_date    keyword  \\\n",
       "8         2  1004825123  2020-02-07  firma neu   \n",
       "79       38  1004954893  2020-08-10  firma neu   \n",
       "178      77  1005402464  2022-02-10  firma neu   \n",
       "193      83  1006046071  2024-06-03  firma neu   \n",
       "214      93     3873587  2017-11-16  firma neu   \n",
       "\n",
       "                                 firm_name  \n",
       "8    AA-Annoncen Agentur AG in Liquidation  \n",
       "79              AAFC Financial Consult Ltd  \n",
       "178             ATG Aare Touring Garage AG  \n",
       "193                        Aareschlucht AG  \n",
       "214   Genossenschaft PRO BON AARGAU in Liq  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firm_name_df['firm_name'] = firm_name_df['parsed_variables'].apply(lambda x: x.get('firm_name', [''])[0])\n",
    "firm_name_df = firm_name_df.drop(columns=['registry_office_canton', 'message_raw', 'codes', 'parsed_variables', 'category', 'text_slices'])\n",
    "firm_name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_FIRM_NAMES = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_firm_names (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    firm_name TEXT\n",
    ");\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_firm_names': (INIT_HISTORY_FIRM_NAMES, firm_name_df),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare firm address changes table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>keyword</th>\n",
       "      <th>care_of</th>\n",
       "      <th>street</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>town</th>\n",
       "      <th>new</th>\n",
       "      <th>until_now</th>\n",
       "      <th>deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1681790</td>\n",
       "      <td>1005867319</td>\n",
       "      <td>2023-10-24</td>\n",
       "      <td>weitere adressen</td>\n",
       "      <td>None</td>\n",
       "      <td>Postfach 1589</td>\n",
       "      <td>8027</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1681790</td>\n",
       "      <td>1005249079</td>\n",
       "      <td>2021-07-14</td>\n",
       "      <td>adresse neu</td>\n",
       "      <td></td>\n",
       "      <td>Tellistrasse 114</td>\n",
       "      <td>5000</td>\n",
       "      <td>Aarau</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1681790</td>\n",
       "      <td>1005249078</td>\n",
       "      <td>2021-07-14</td>\n",
       "      <td>adresse neu</td>\n",
       "      <td></td>\n",
       "      <td>Tellistrasse 114</td>\n",
       "      <td>5000</td>\n",
       "      <td>Aarau</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1681790</td>\n",
       "      <td>3533349</td>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>adresse neu</td>\n",
       "      <td></td>\n",
       "      <td>Hafenstrasse 50 D</td>\n",
       "      <td>8280</td>\n",
       "      <td>Kreuzlingen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1681790</td>\n",
       "      <td>3540613</td>\n",
       "      <td>2017-05-24</td>\n",
       "      <td>adresse neu</td>\n",
       "      <td></td>\n",
       "      <td>Sigelwiesstrasse 21</td>\n",
       "      <td>8451</td>\n",
       "      <td>Kleinandelfingen</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ehraid     shab_id   shab_date           keyword care_of  \\\n",
       "0  1681790  1005867319  2023-10-24  weitere adressen    None   \n",
       "1  1681790  1005249079  2021-07-14       adresse neu           \n",
       "2  1681790  1005249078  2021-07-14       adresse neu           \n",
       "3  1681790     3533349  2017-05-19       adresse neu           \n",
       "4  1681790     3540613  2017-05-24       adresse neu           \n",
       "\n",
       "                street  postal_code              town  new  until_now  deleted  \n",
       "0        Postfach 1589         8027            Zürich    0          0        1  \n",
       "1     Tellistrasse 114         5000             Aarau    1          0        0  \n",
       "2     Tellistrasse 114         5000             Aarau    1          0        0  \n",
       "3    Hafenstrasse 50 D         8280       Kreuzlingen    1          0        0  \n",
       "4  Sigelwiesstrasse 21         8451  Kleinandelfingen    1          0        0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shab_ids = []\n",
    "keywords = []\n",
    "ehraids = []\n",
    "shab_dates = []\n",
    "\n",
    "care_ofs = []\n",
    "streets = []\n",
    "postal_codes = []\n",
    "towns = []\n",
    "\n",
    "new = []\n",
    "until_now = []\n",
    "deleted = []\n",
    "\n",
    "for i, row in firm_address_df.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    keyword = row['keyword']\n",
    "    ehraid = row['ehraid']\n",
    "    shab_date = row['shab_date']\n",
    "    \n",
    "    parsed_variables = row['parsed_variables']\n",
    "    address_new = parsed_variables.get('addresses_new', [])\n",
    "    address_until = parsed_variables.get('addresses_until_now', [])\n",
    "    address_deleted = parsed_variables.get('addresses_deleted', [])\n",
    "\n",
    "    for address in address_new:\n",
    "\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n",
    "\n",
    "        care_ofs.append(address.get('care_of', ''))\n",
    "        streets.append(address.get('street', ''))\n",
    "        postal_codes.append(address.get('postal_code', -1))\n",
    "        towns.append(address.get('town', ''))\n",
    "        new.append(1)\n",
    "        until_now.append(0)\n",
    "        deleted.append(0)\n",
    "\n",
    "    for address in address_until:\n",
    "\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n",
    "\n",
    "        care_ofs.append(address.get('care_of', ''))\n",
    "        streets.append(address.get('street', ''))\n",
    "        postal_codes.append(address.get('postal_code', -1))\n",
    "        towns.append(address.get('town', ''))\n",
    "        new.append(0)\n",
    "        until_now.append(1)\n",
    "        deleted.append(0)\n",
    "\n",
    "    for address in address_deleted:\n",
    "\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n",
    "\n",
    "        care_ofs.append(address.get('care_of', ''))\n",
    "        streets.append(address.get('street', ''))\n",
    "        postal_codes.append(address.get('postal_code', -1))\n",
    "        towns.append(address.get('town', ''))\n",
    "        new.append(0)\n",
    "        until_now.append(0)\n",
    "        deleted.append(1)\n",
    "\n",
    "firm_address_df = pd.DataFrame({\n",
    "    'ehraid': ehraid,\n",
    "    'shab_id': shab_ids,\n",
    "    'shab_date': shab_dates,\n",
    "    'keyword': keywords,\n",
    "    'care_of': care_ofs,\n",
    "    'street': streets,\n",
    "    'postal_code': postal_codes,\n",
    "    'town': towns,\n",
    "    'new': new,\n",
    "    'until_now': until_now,\n",
    "    'deleted': deleted\n",
    "})\n",
    "firm_address_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_FIRM_ADDRESSES = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_firm_addresses (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    care_of TEXT,\n",
    "    street TEXT,\n",
    "    postal_code INT,\n",
    "    town TEXT,\n",
    "    new BOOLEAN,\n",
    "    until_now BOOLEAN,\n",
    "    deleted BOOLEAN\n",
    ");\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_firm_addresses': (INIT_HISTORY_FIRM_ADDRESSES, firm_address_df),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare firm branches table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>id</th>\n",
       "      <th>new</th>\n",
       "      <th>until_now</th>\n",
       "      <th>deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1680346</td>\n",
       "      <td>1004515888</td>\n",
       "      <td>2018-12-10</td>\n",
       "      <td>zweigniederlassung neu</td>\n",
       "      <td>Frauenfeld</td>\n",
       "      <td>CHE-443.103.739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1680346</td>\n",
       "      <td>1004745179</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>zweigniederlassung neu</td>\n",
       "      <td>Schwyz</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1680346</td>\n",
       "      <td>1004745179</td>\n",
       "      <td>2019-10-25</td>\n",
       "      <td>zweigniederlassung neu</td>\n",
       "      <td>Zürich</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1680346</td>\n",
       "      <td>1005473054</td>\n",
       "      <td>2022-05-13</td>\n",
       "      <td>zweigniederlassung neu</td>\n",
       "      <td>Affoltern am Albis</td>\n",
       "      <td>CHE-413.995.453</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1680346</td>\n",
       "      <td>1006068778</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>zweigniederlassung neu</td>\n",
       "      <td>Affoltern am Albis</td>\n",
       "      <td>CHE-413.995.453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ehraid     shab_id   shab_date                 keyword  \\\n",
       "0  1680346  1004515888  2018-12-10  zweigniederlassung neu   \n",
       "1  1680346  1004745179  2019-10-25  zweigniederlassung neu   \n",
       "2  1680346  1004745179  2019-10-25  zweigniederlassung neu   \n",
       "3  1680346  1005473054  2022-05-13  zweigniederlassung neu   \n",
       "4  1680346  1006068778  2024-06-27  zweigniederlassung neu   \n",
       "\n",
       "             location               id  new  until_now  deleted  \n",
       "0          Frauenfeld  CHE-443.103.739    0          0        1  \n",
       "1              Schwyz                     0          0        1  \n",
       "2              Zürich                     0          0        1  \n",
       "3  Affoltern am Albis  CHE-413.995.453    1          0        0  \n",
       "4  Affoltern am Albis  CHE-413.995.453    0          0        1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shab_ids = []\n",
    "keywords = []\n",
    "ehraids = []\n",
    "shab_dates = []\n",
    "\n",
    "locations = []\n",
    "ids = []\n",
    "\n",
    "new = []\n",
    "until_now = []\n",
    "deleted = []\n",
    "\n",
    "for i, row in branches_df.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    keyword = row['keyword']\n",
    "    ehraid = row['ehraid']\n",
    "    shab_date = row['shab_date']\n",
    "    \n",
    "    parsed_variables = row['parsed_variables']\n",
    "    branches_new = parsed_variables.get('branches_new', [])\n",
    "    branches_until = parsed_variables.get('branches_until_now', [])\n",
    "    branches_deleted = parsed_variables.get('branches_deleted', [])\n",
    "\n",
    "    for branch in branches_new:\n",
    "\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n",
    "\n",
    "        locations.append(branch.get('location', ''))\n",
    "        ids.append(branch.get('id', ''))\n",
    "\n",
    "        new.append(1)\n",
    "        until_now.append(0)\n",
    "        deleted.append(0)\n",
    "\n",
    "    for branch in branches_until:\n",
    "\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n",
    "\n",
    "        locations.append(branch.get('location', ''))\n",
    "        ids.append(branch.get('id', ''))\n",
    "\n",
    "        new.append(0)\n",
    "        until_now.append(1)\n",
    "        deleted.append(0)\n",
    "\n",
    "    for branch in branches_deleted:\n",
    "\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n",
    "\n",
    "        locations.append(branch.get('location', ''))\n",
    "        ids.append(branch.get('id', ''))\n",
    "\n",
    "        new.append(0)\n",
    "        until_now.append(0)\n",
    "        deleted.append(1)\n",
    "\n",
    "branches_df = pd.DataFrame({\n",
    "    'ehraid': ehraid,\n",
    "    'shab_id': shab_ids,\n",
    "    'shab_date': shab_dates,\n",
    "    'keyword': keywords,\n",
    "    'location': locations,\n",
    "    'id': ids,\n",
    "    'new': new,\n",
    "    'until_now': until_now,\n",
    "    'deleted': deleted\n",
    "})\n",
    "branches_df.loc[branches_df.id.str.contains(r'radiat|radié|gestrichen'), 'id'] = ''\n",
    "branches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_BRANCHES = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_branches (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    location TEXT,\n",
    "    id TEXT,\n",
    "    new BOOLEAN,\n",
    "    until_now BOOLEAN,\n",
    "    deleted BOOLEAN\n",
    ");\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_branches': (INIT_HISTORY_BRANCHES, branches_df),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS CAPITAL CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_changes = shab_merged[shab_merged.category == 'capital and legal changes'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct wrong values like '446.001.000.00'\n",
    "def correct_number(number_str: str) -> str:\n",
    "    parts = number_str.rsplit('.', 1)\n",
    "    return parts[0].replace('.', '') + '.' + parts[1] if len(parts) > 1 else parts[0].replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "shab_ids = [[], []]\n",
    "keywords = [[], []]\n",
    "ehraids = [[], []]\n",
    "shab_dates = [[], []]\n",
    "\n",
    "capital_new = []\n",
    "capital_until_now = []\n",
    "num_shares_new = []\n",
    "val_shares_new = []\n",
    "typ_shares_new = []\n",
    "\n",
    "for i, row in capital_changes.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    keyword = row['keyword']\n",
    "    ehraid = row['ehraid']\n",
    "    shab_date = row['shab_date']\n",
    "\n",
    "    parsed_variables = row['parsed_variables']\n",
    "    cap_new = parsed_variables.get('capital_new', [])\n",
    "    cap_unt = parsed_variables.get('capital_until_now', [])\n",
    "    srs_new = parsed_variables.get('shares_new', [])\n",
    "\n",
    "    if cap_new or cap_unt:\n",
    "        capital_new.append(cap_new[0] if len(cap_new) > 0 else None)\n",
    "        capital_until_now.append(cap_unt[0] if len(cap_unt) > 0 else None)\n",
    "        shab_ids[0].append(shab_id)\n",
    "        keywords[0].append(keyword)\n",
    "        ehraids[0].append(ehraid)\n",
    "        shab_dates[0].append(shab_date)\n",
    "    if srs_new:\n",
    "        for s in srs_new:\n",
    "            num_shares_new.append(s.get('number'))\n",
    "            val_shares_new.append(s.get('value'))\n",
    "            typ_shares_new.append(s.get('type'))\n",
    "            shab_ids[1].append(shab_id)\n",
    "            keywords[1].append(keyword)\n",
    "            ehraids[1].append(ehraid)\n",
    "            shab_dates[1].append(shab_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_new = pd.DataFrame({\n",
    "    'ehraid': ehraids[0],\n",
    "    'shab_date': shab_dates[0],\n",
    "    'shab_id': shab_ids[0],\n",
    "    'keyword': keywords[0],\n",
    "    'capital_new': capital_new,\n",
    "    'capital_until_now': capital_until_now})\n",
    "\n",
    "cap_new['capital_new'] = cap_new['capital_new'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Extract Währung\n",
    "cap_new['currency_new'] = cap_new['capital_new'].str.extract(r'^([^\\d\\s]+)')\n",
    "cap_new['currency_new'] = cap_new['currency_new'].fillna('')\n",
    "\n",
    "cap_new['currency_until_now'] = cap_new['capital_until_now'].str.extract(r'^([^\\d\\s]+)')\n",
    "cap_new['currency_until_now'] = cap_new['currency_until_now'].fillna('')\n",
    "\n",
    "# Extract Kapital\n",
    "cap_new['capital_new'] = cap_new['capital_new'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "# Apply correction to remove unneccessary punctuations\n",
    "cap_new['capital_new'] = cap_new['capital_new'].apply(correct_number)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].apply(correct_number)\n",
    "\n",
    "cap_new['capital_new'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in cap_new['capital_new']]\n",
    "cap_new.loc[cap_new['capital_new'] == '.', 'capital_new'] = np.nan\n",
    "\n",
    "cap_new['capital_until_now'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in cap_new['capital_until_now']]\n",
    "cap_new.loc[cap_new['capital_until_now'] == '.', 'capital_until_now'] = np.nan\n",
    "\n",
    "# Ensure correct types\n",
    "cap_new['capital_new'] = cap_new['capital_new'].astype(float)\n",
    "cap_new['capital_until_now'] = cap_new['capital_until_now'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_new = pd.DataFrame({\n",
    "    'ehraid': ehraids[1],\n",
    "    'shab_date': shab_dates[1],\n",
    "    'shab_id': shab_ids[1],\n",
    "    'keyword': keywords[1],\n",
    "    'num_shares_new': num_shares_new,\n",
    "    'val_shares_new': val_shares_new,\n",
    "    'typ_shares_new': typ_shares_new,})\n",
    "\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Extract Währung\n",
    "stocks_new['currency_shares_new'] = stocks_new['val_shares_new'].str.extract(r'^([^\\d\\s]+)')\n",
    "stocks_new['currency_shares_new'] = stocks_new['currency_shares_new'].fillna('')\n",
    "\n",
    "# Extract number of Stocks, etc.\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "# Apply correction to remove unneccessary punctuations\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].apply(correct_number)\n",
    "stocks_new['val_shares_new'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in stocks_new['val_shares_new']]\n",
    "stocks_new.loc[stocks_new['val_shares_new'] == '.', 'val_shares_new'] = np.nan\n",
    "\n",
    "# Ensure correct types\n",
    "stocks_new['val_shares_new'] = stocks_new['val_shares_new'].astype(float)\n",
    "stocks_new['num_shares_new'] = stocks_new['num_shares_new'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total value of the capital by multiplying the number of shares with their individual value\n",
    "stocks_new['value_total'] = stocks_new['num_shares_new'] * stocks_new['val_shares_new']\n",
    "\n",
    "# Calculate new capital for shab ids where kapital_neu variable is not given, but scheine_neu is\n",
    "missing_ids = set(stocks_new.shab_id).difference(set(cap_new.shab_id))\n",
    "stocks_new_missing = stocks_new[stocks_new.shab_id.isin(missing_ids)]\n",
    "\n",
    "cap_new_missing = stocks_new_missing.groupby(['ehraid', 'shab_date', 'shab_id', 'keyword']).agg(\n",
    "    capital_new=pd.NamedAgg(column='value_total', aggfunc='sum'),\n",
    "    currency_new=pd.NamedAgg(column='currency_shares_new', aggfunc=lambda x: list(set([currency for currency in x if currency != ''])))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are mixed currencies\n",
    "assert len([cur_set for cur_set in cap_new_missing['currency_new'] if len(cur_set) > 1]) == 0\n",
    "\n",
    "cap_new_missing['currency_new'] = [v[0] if len(v) > 0 else '' for v in cap_new_missing['currency_new']]\n",
    "cap_new_missing['currency_until_now'] = ''\n",
    "cap_new_missing['capital_until_now'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes to get all capital changes\n",
    "cap_new_concat = pd.concat([cap_new, cap_new_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>capital_new</th>\n",
       "      <th>capital_until_now</th>\n",
       "      <th>currency_new</th>\n",
       "      <th>currency_until_now</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1005740573</td>\n",
       "      <td>aktienkapital neu</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHF</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1005740573</td>\n",
       "      <td>liberierung aktienkapital neu</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHF</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>1004976547</td>\n",
       "      <td>aktienkapital neu</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHF</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>2020-09-11</td>\n",
       "      <td>1004976547</td>\n",
       "      <td>liberierung aktienkapital neu</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHF</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>158</td>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>1004698965</td>\n",
       "      <td>aktienkapital neu</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHF</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ehraid   shab_date     shab_id                        keyword  capital_new  \\\n",
       "0      54  2023-05-08  1005740573              aktienkapital neu    1350000.0   \n",
       "1      54  2023-05-08  1005740573  liberierung aktienkapital neu    1350000.0   \n",
       "2     149  2020-09-11  1004976547              aktienkapital neu     100000.0   \n",
       "3     149  2020-09-11  1004976547  liberierung aktienkapital neu     100000.0   \n",
       "4     158  2019-08-20  1004698965              aktienkapital neu     150000.0   \n",
       "\n",
       "   capital_until_now currency_new currency_until_now  \n",
       "0                NaN          CHF                     \n",
       "1                NaN          CHF                     \n",
       "2                NaN          CHF                     \n",
       "3                NaN          CHF                     \n",
       "4                NaN          CHF                     "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_new_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_REGISTERED_CAPITAL = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_registered_capital (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    capital_new NUMERIC,\n",
    "    capital_until_now NUMERIC,\n",
    "    currency_new TEXT,\n",
    "    currency_until_now TEXT\n",
    ")\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_registered_capital': (INIT_HISTORY_REGISTERED_CAPITAL, cap_new_concat),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap_new_concat.to_csv(EXTERNAL_DATA_DIR / 'capital_changes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS MERGERS AND ACQUISIITONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergers_and_acquisitions = shab_merged[shab_merged.category == 'mergers and separations'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>registry_office_canton</th>\n",
       "      <th>message_raw</th>\n",
       "      <th>codes</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_slices</th>\n",
       "      <th>parsed_variables</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11</td>\n",
       "      <td>1004788634</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>VS</td>\n",
       "      <td>AA'S. AG, in Zermatt, CHE-106.369.697, Aktieng...</td>\n",
       "      <td>[aenderungorgane, fusion]</td>\n",
       "      <td>fusion</td>\n",
       "      <td>[Übernahme der Aktiven und Passiven der AA 'S....</td>\n",
       "      <td>{'balance_sheet_date': ['30.06.2019'], 'firms_...</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>54</td>\n",
       "      <td>1005740573</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>SO</td>\n",
       "      <td>Aare Finanz- und Holding-AG, in Olten, CHE-102...</td>\n",
       "      <td>[zweckaenderung, kapitalaenderung, kapitalaend...</td>\n",
       "      <td>abspaltung</td>\n",
       "      <td>[Ein Teil der Aktiven und Passiven geht gemäss...</td>\n",
       "      <td>{}</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>87</td>\n",
       "      <td>2693405</td>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>AG</td>\n",
       "      <td>Aarfim AG, in Aarburg, CHE-102.588.119, Aktien...</td>\n",
       "      <td>[adressaenderung, aenderungorgane, fusion]</td>\n",
       "      <td>fusion</td>\n",
       "      <td>[Übernahme der Aktiven und Passiven der Mininv...</td>\n",
       "      <td>{'balance_sheet_date': ['31.12.2015'], 'firms_...</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>133</td>\n",
       "      <td>1004662690</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>AG</td>\n",
       "      <td>Aarolac AG Lack- und Farbenfabrik, in Oberentf...</td>\n",
       "      <td>[fusion]</td>\n",
       "      <td>fusion</td>\n",
       "      <td>[Übernahme der Aktiven und Passiven der MMA Ho...</td>\n",
       "      <td>{'balance_sheet_date': ['31.12.2018'], 'firms_...</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>261</td>\n",
       "      <td>1005484711</td>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>ZH</td>\n",
       "      <td>ABB Asea Brown Boveri Ltd, in Zürich, CHE-106....</td>\n",
       "      <td>[fusion]</td>\n",
       "      <td>fusion</td>\n",
       "      <td>[Übernahme der Aktiven und Passiven der ABB In...</td>\n",
       "      <td>{'balance_sheet_date': ['31.12.2021'], 'firms_...</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238855</th>\n",
       "      <td>1672008</td>\n",
       "      <td>1006216881</td>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>SG</td>\n",
       "      <td>mirame AG, in Wil (SG), CHE-395.572.325, c/o R...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>abspaltung</td>\n",
       "      <td>[Die Gesellschaft entsteht aus der Abspaltung ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238950</th>\n",
       "      <td>1672023</td>\n",
       "      <td>1006216892</td>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>SG</td>\n",
       "      <td>Stephan Wenger AG, in Wil (SG), CHE-496.436.41...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>abspaltung</td>\n",
       "      <td>[Die Gesellschaft entsteht aus der Abspaltung ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240219</th>\n",
       "      <td>1672396</td>\n",
       "      <td>1006218724</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>GR</td>\n",
       "      <td>Krol-Active Company GmbH, in Trun, CHE-471.878...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>abspaltung</td>\n",
       "      <td>[Die Gesellschaft entsteht aus der Abspaltung ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248376</th>\n",
       "      <td>1674791</td>\n",
       "      <td>1006234147</td>\n",
       "      <td>2025-01-21</td>\n",
       "      <td>AG</td>\n",
       "      <td>PARU Finanz Holding AG, in Neuenhof, CHE-147.4...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>abspaltung</td>\n",
       "      <td>[Die Gesellschaft entsteht aus der Abspaltung ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5253902</th>\n",
       "      <td>1676439</td>\n",
       "      <td>1006242842</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>TG</td>\n",
       "      <td>MAALRUPA Finanz Holding AG, in Altnau, CHE-255...</td>\n",
       "      <td>[status, status.neu]</td>\n",
       "      <td>abspaltung</td>\n",
       "      <td>[Die Gesellschaft entsteht aus der Abspaltung ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>mergers and separations</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12490 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ehraid     shab_id   shab_date registry_office_canton  \\\n",
       "19            11  1004788634  2019-12-19                     VS   \n",
       "98            54  1005740573  2023-05-08                     SO   \n",
       "203           87     2693405  2016-02-29                     AG   \n",
       "281          133  1004662690  2019-06-28                     AG   \n",
       "650          261  1005484711  2022-05-31                     ZH   \n",
       "...          ...         ...         ...                    ...   \n",
       "5238855  1672008  1006216881  2024-12-27                     SG   \n",
       "5238950  1672023  1006216892  2024-12-27                     SG   \n",
       "5240219  1672396  1006218724  2024-12-30                     GR   \n",
       "5248376  1674791  1006234147  2025-01-21                     AG   \n",
       "5253902  1676439  1006242842  2025-01-30                     TG   \n",
       "\n",
       "                                               message_raw  \\\n",
       "19       AA'S. AG, in Zermatt, CHE-106.369.697, Aktieng...   \n",
       "98       Aare Finanz- und Holding-AG, in Olten, CHE-102...   \n",
       "203      Aarfim AG, in Aarburg, CHE-102.588.119, Aktien...   \n",
       "281      Aarolac AG Lack- und Farbenfabrik, in Oberentf...   \n",
       "650      ABB Asea Brown Boveri Ltd, in Zürich, CHE-106....   \n",
       "...                                                    ...   \n",
       "5238855  mirame AG, in Wil (SG), CHE-395.572.325, c/o R...   \n",
       "5238950  Stephan Wenger AG, in Wil (SG), CHE-496.436.41...   \n",
       "5240219  Krol-Active Company GmbH, in Trun, CHE-471.878...   \n",
       "5248376  PARU Finanz Holding AG, in Neuenhof, CHE-147.4...   \n",
       "5253902  MAALRUPA Finanz Holding AG, in Altnau, CHE-255...   \n",
       "\n",
       "                                                     codes     keyword  \\\n",
       "19                               [aenderungorgane, fusion]      fusion   \n",
       "98       [zweckaenderung, kapitalaenderung, kapitalaend...  abspaltung   \n",
       "203             [adressaenderung, aenderungorgane, fusion]      fusion   \n",
       "281                                               [fusion]      fusion   \n",
       "650                                               [fusion]      fusion   \n",
       "...                                                    ...         ...   \n",
       "5238855                               [status, status.neu]  abspaltung   \n",
       "5238950                               [status, status.neu]  abspaltung   \n",
       "5240219                               [status, status.neu]  abspaltung   \n",
       "5248376                               [status, status.neu]  abspaltung   \n",
       "5253902                               [status, status.neu]  abspaltung   \n",
       "\n",
       "                                               text_slices  \\\n",
       "19       [Übernahme der Aktiven und Passiven der AA 'S....   \n",
       "98       [Ein Teil der Aktiven und Passiven geht gemäss...   \n",
       "203      [Übernahme der Aktiven und Passiven der Mininv...   \n",
       "281      [Übernahme der Aktiven und Passiven der MMA Ho...   \n",
       "650      [Übernahme der Aktiven und Passiven der ABB In...   \n",
       "...                                                    ...   \n",
       "5238855  [Die Gesellschaft entsteht aus der Abspaltung ...   \n",
       "5238950  [Die Gesellschaft entsteht aus der Abspaltung ...   \n",
       "5240219  [Die Gesellschaft entsteht aus der Abspaltung ...   \n",
       "5248376  [Die Gesellschaft entsteht aus der Abspaltung ...   \n",
       "5253902  [Die Gesellschaft entsteht aus der Abspaltung ...   \n",
       "\n",
       "                                          parsed_variables  \\\n",
       "19       {'balance_sheet_date': ['30.06.2019'], 'firms_...   \n",
       "98                                                      {}   \n",
       "203      {'balance_sheet_date': ['31.12.2015'], 'firms_...   \n",
       "281      {'balance_sheet_date': ['31.12.2018'], 'firms_...   \n",
       "650      {'balance_sheet_date': ['31.12.2021'], 'firms_...   \n",
       "...                                                    ...   \n",
       "5238855                                                 {}   \n",
       "5238950                                                 {}   \n",
       "5240219                                                 {}   \n",
       "5248376                                                 {}   \n",
       "5253902                                                 {}   \n",
       "\n",
       "                        category  \n",
       "19       mergers and separations  \n",
       "98       mergers and separations  \n",
       "203      mergers and separations  \n",
       "281      mergers and separations  \n",
       "650      mergers and separations  \n",
       "...                          ...  \n",
       "5238855  mergers and separations  \n",
       "5238950  mergers and separations  \n",
       "5240219  mergers and separations  \n",
       "5248376  mergers and separations  \n",
       "5253902  mergers and separations  \n",
       "\n",
       "[12490 rows x 10 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergers_and_acquisitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "shab_ids = []\n",
    "keywords = []\n",
    "ehraids = []\n",
    "shab_dates = []\n",
    "\n",
    "contract_dates = []\n",
    "balance_sheet_dates = []\n",
    "\n",
    "firm_taken_over = []\n",
    "location_taken_over = []\n",
    "id_taken_over = []\n",
    "assets_taken_over = []\n",
    "liabilities_taken_over = []\n",
    "\n",
    "for i, row in mergers_and_acquisitions.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    keyword = row['keyword']\n",
    "    shab_date = row['shab_date']\n",
    "    ehraid = row['ehraid']\n",
    "\n",
    "    parsed_variables = row['parsed_variables']\n",
    "\n",
    "    if parsed_variables:\n",
    "        contract_date = parsed_variables.get('contract_date', [])\n",
    "        balance_sheet_date = parsed_variables.get('balance_sheet_date', [])\n",
    "        firms_taken_over = parsed_variables.get('firms_taken_over', [])\n",
    "        for firm in firms_taken_over:\n",
    "            firm_name = firm.get('firm_name', '')\n",
    "            location = firm.get('location', '')\n",
    "            id = firm.get('id', '')\n",
    "            capital_taken_over = firm.get('capital_taken_over', {})\n",
    "            assets = capital_taken_over.get('assets', '') if capital_taken_over else ''\n",
    "            liabilities = capital_taken_over.get('liabilities', '') if capital_taken_over else ''\n",
    "\n",
    "            contract_dates.append(contract_date[0] if len(contract_date) > 0 else '')\n",
    "            balance_sheet_dates.append(balance_sheet_date[0] if len(balance_sheet_date) > 0 else '')\n",
    "            firm_taken_over.append(firm_name)\n",
    "            location_taken_over.append(location)\n",
    "            id_taken_over.append(id)\n",
    "            assets_taken_over.append(assets)\n",
    "            liabilities_taken_over.append(liabilities)\n",
    "            shab_ids.append(shab_id)\n",
    "            keywords.append(keyword)\n",
    "            ehraids.append(ehraid)\n",
    "            shab_dates.append(shab_date)\n",
    "    else:\n",
    "        firm_taken_over.append('')\n",
    "        location_taken_over.append('')\n",
    "        id_taken_over.append('')\n",
    "        assets_taken_over.append('')\n",
    "        liabilities_taken_over.append('')\n",
    "        shab_ids.append(shab_id)\n",
    "        keywords.append(keyword)\n",
    "        ehraids.append(ehraid)\n",
    "        shab_dates.append(shab_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(shab_ids) == len(keywords) == len(firm_taken_over) == len(location_taken_over) == len(id_taken_over) == len(assets_taken_over) == len(liabilities_taken_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mergers = pd.DataFrame({\n",
    "    'ehraid': ehraids,\n",
    "    'shab_date': shab_dates,\n",
    "    'shab_id': shab_ids,\n",
    "    'keyword': keywords,\n",
    "    'firm_taken_over': firm_taken_over,\n",
    "    'location_taken_over': location_taken_over,\n",
    "    'id_taken_over': id_taken_over,\n",
    "    'assets_taken_over': assets_taken_over,\n",
    "    'liabilities_taken_over': liabilities_taken_over\n",
    "})\n",
    "\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Extract Währung of Aktiven/Passiven\n",
    "processed_mergers['currency_assets_taken_over'] = processed_mergers['assets_taken_over'].str.extract(r'^([^\\d\\s]+)')\n",
    "processed_mergers['currency_assets_taken_over'] = processed_mergers['currency_assets_taken_over'].fillna('')\n",
    "\n",
    "processed_mergers['currency_liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].str.extract(r'^([^\\d\\s]+)')\n",
    "processed_mergers['currency_liabilities_taken_over'] = processed_mergers['currency_liabilities_taken_over'].fillna('')\n",
    "\n",
    "# Extract value of Aktiven/Passiven\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "# Apply correction to remove unneccessary punctuations\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].apply(correct_number)\n",
    "processed_mergers['assets_taken_over'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in processed_mergers['assets_taken_over']]\n",
    "\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].apply(correct_number)\n",
    "processed_mergers['liabilities_taken_over'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in processed_mergers['liabilities_taken_over']]\n",
    "\n",
    "# Ensure correct types\n",
    "processed_mergers['assets_taken_over'] = processed_mergers['assets_taken_over'].astype(float)\n",
    "processed_mergers['liabilities_taken_over'] = processed_mergers['liabilities_taken_over'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>firm_taken_over</th>\n",
       "      <th>location_taken_over</th>\n",
       "      <th>id_taken_over</th>\n",
       "      <th>assets_taken_over</th>\n",
       "      <th>liabilities_taken_over</th>\n",
       "      <th>currency_assets_taken_over</th>\n",
       "      <th>currency_liabilities_taken_over</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>1004788634</td>\n",
       "      <td>fusion</td>\n",
       "      <td>AA 'S. Immobilien AG</td>\n",
       "      <td>Grächen</td>\n",
       "      <td>CHE-101.430.817</td>\n",
       "      <td>2.847237e+06</td>\n",
       "      <td>2269431.7</td>\n",
       "      <td>CHF</td>\n",
       "      <td>CHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>1005740573</td>\n",
       "      <td>abspaltung</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87</td>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>2693405</td>\n",
       "      <td>fusion</td>\n",
       "      <td>Mininvest AG</td>\n",
       "      <td>Aarburg</td>\n",
       "      <td>CHE-103.927.388</td>\n",
       "      <td>2.510449e+06</td>\n",
       "      <td>106923.0</td>\n",
       "      <td>CHF</td>\n",
       "      <td>CHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>1004662690</td>\n",
       "      <td>fusion</td>\n",
       "      <td>MMA Holding AG</td>\n",
       "      <td>Oberentfelden</td>\n",
       "      <td>CHE-114.610.317</td>\n",
       "      <td>3.495312e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CHF</td>\n",
       "      <td>CHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261</td>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>1005484711</td>\n",
       "      <td>fusion</td>\n",
       "      <td>ABB Investment Holding 2 GmbH</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>CHE-203.422.841</td>\n",
       "      <td>9.719461e+08</td>\n",
       "      <td>43080.0</td>\n",
       "      <td>CHF</td>\n",
       "      <td>CHF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ehraid   shab_date     shab_id     keyword                firm_taken_over  \\\n",
       "0      11  2019-12-19  1004788634      fusion           AA 'S. Immobilien AG   \n",
       "1      54  2023-05-08  1005740573  abspaltung                                  \n",
       "2      87  2016-02-29     2693405      fusion                   Mininvest AG   \n",
       "3     133  2019-06-28  1004662690      fusion                 MMA Holding AG   \n",
       "4     261  2022-05-31  1005484711      fusion  ABB Investment Holding 2 GmbH   \n",
       "\n",
       "  location_taken_over    id_taken_over  assets_taken_over  \\\n",
       "0             Grächen  CHE-101.430.817       2.847237e+06   \n",
       "1                                                     NaN   \n",
       "2             Aarburg  CHE-103.927.388       2.510449e+06   \n",
       "3       Oberentfelden  CHE-114.610.317       3.495312e+06   \n",
       "4              Zürich  CHE-203.422.841       9.719461e+08   \n",
       "\n",
       "   liabilities_taken_over currency_assets_taken_over  \\\n",
       "0               2269431.7                        CHF   \n",
       "1                     NaN                              \n",
       "2                106923.0                        CHF   \n",
       "3                     0.0                        CHF   \n",
       "4                 43080.0                        CHF   \n",
       "\n",
       "  currency_liabilities_taken_over  \n",
       "0                             CHF  \n",
       "1                                  \n",
       "2                             CHF  \n",
       "3                             CHF  \n",
       "4                             CHF  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_mergers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_MERGER = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_merger (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    firm_taken_over TEXT,\n",
    "    location_taken_over TEXT,\n",
    "    id_taken_over TEXT,\n",
    "    assets_taken_over NUMERIC,\n",
    "    liabilities_taken_over NUMERIC,\n",
    "    currency_assets_taken_over TEXT,\n",
    "    currency_liabilities_taken_over TEXT\n",
    ");\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_merger': (INIT_HISTORY_MERGER, processed_mergers),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_mergers.to_csv(EXTERNAL_DATA_DIR / 'merger_sizes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS LEGAL FORM CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_form_changes = shab_merged[shab_merged.category == 'capital and legal changes'].copy()\n",
    "\n",
    "shab_ids = []\n",
    "keywords = []\n",
    "ehraids = []\n",
    "shab_dates = []\n",
    "\n",
    "legal_forms_new = []\n",
    "legal_forms_until_now = []\n",
    "legal_forms_deleted = []\n",
    "conversion_assets = []\n",
    "conversion_liabilities = []\n",
    "\n",
    "for i, row in legal_form_changes.iterrows():\n",
    "    shab_id = row['shab_id']\n",
    "    keyword = row['keyword']\n",
    "    shab_date = row['shab_date']\n",
    "    ehraid = row['ehraid']\n",
    "\n",
    "    parsed_variables = row['parsed_variables']\n",
    "\n",
    "    if parsed_variables:\n",
    "        legal_form_new = parsed_variables.get('legal_form_new', [])\n",
    "        legal_form_until_now = parsed_variables.get('legal_form_until_now', [])\n",
    "        legal_form_deleted = parsed_variables.get('legal_form_deleted', [])\n",
    "        assets = parsed_variables.get('assets', [])\n",
    "        liabilities = parsed_variables.get('liabilities', [])\n",
    "        if legal_form_new or legal_form_until_now or legal_form_deleted:\n",
    "            legal_forms_new.append(legal_form_new[0] if len(legal_form_new) > 0 else '')\n",
    "            legal_forms_until_now.append(legal_form_until_now[0] if len(legal_form_until_now) > 0 else '')\n",
    "            legal_forms_deleted.append(legal_form_deleted[0] if len(legal_form_deleted) > 0 else '')\n",
    "            conversion_assets.append(assets[0] if len(assets) > 0 else '')\n",
    "            conversion_liabilities.append(liabilities[0] if len(liabilities) > 0 else '')\n",
    "            shab_ids.append(shab_id)\n",
    "            keywords.append(keyword)\n",
    "            ehraids.append(ehraid)\n",
    "            shab_dates.append(shab_date)\n",
    "\n",
    "return_df = pd.DataFrame({\n",
    "    'ehraid': ehraids,\n",
    "    'shab_date': shab_dates,\n",
    "    'shab_id': shab_ids,\n",
    "    'keyword': keywords,\n",
    "    'legal_form_new': legal_forms_new,\n",
    "    'legal_form_until_now': legal_forms_until_now,\n",
    "    'legal_form_deleted': legal_forms_deleted,\n",
    "    'assets': conversion_assets,\n",
    "    'liabilities': conversion_liabilities\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>legal_form_new</th>\n",
       "      <th>legal_form_until_now</th>\n",
       "      <th>legal_form_deleted</th>\n",
       "      <th>assets</th>\n",
       "      <th>liabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>2706811</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>1004781481</td>\n",
       "      <td>rechtsform neu</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>308</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>1004781481</td>\n",
       "      <td>umwandlung</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CHF 499'207'508.00</td>\n",
       "      <td>CHF 169'631'206.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7073</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>1004505486</td>\n",
       "      <td>rechtsform neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7073</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>1004505486</td>\n",
       "      <td>umwandlung</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td></td>\n",
       "      <td>CHF 3'692'400</td>\n",
       "      <td>CHF 1'569'412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18331</th>\n",
       "      <td>1681739</td>\n",
       "      <td>2025-03-03</td>\n",
       "      <td>1006271202</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18332</th>\n",
       "      <td>1681782</td>\n",
       "      <td>2025-03-03</td>\n",
       "      <td>1006271600</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18333</th>\n",
       "      <td>1681808</td>\n",
       "      <td>2025-03-03</td>\n",
       "      <td>1006271284</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18334</th>\n",
       "      <td>1681915</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006272100</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18335</th>\n",
       "      <td>1681972</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006272516</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>private company limited by shares</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18336 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ehraid   shab_date     shab_id                   keyword  \\\n",
       "0          181  2016-03-07     2706811  rechtsform hauptsitz neu   \n",
       "1          308  2019-12-12  1004781481            rechtsform neu   \n",
       "2          308  2019-12-12  1004781481                umwandlung   \n",
       "3         7073  2018-11-26  1004505486            rechtsform neu   \n",
       "4         7073  2018-11-26  1004505486                umwandlung   \n",
       "...        ...         ...         ...                       ...   \n",
       "18331  1681739  2025-03-03  1006271202  rechtsform hauptsitz neu   \n",
       "18332  1681782  2025-03-03  1006271600  rechtsform hauptsitz neu   \n",
       "18333  1681808  2025-03-03  1006271284  rechtsform hauptsitz neu   \n",
       "18334  1681915  2025-03-04  1006272100  rechtsform hauptsitz neu   \n",
       "18335  1681972  2025-03-04  1006272516  rechtsform hauptsitz neu   \n",
       "\n",
       "                              legal_form_new  \\\n",
       "0                         Aktiengesellschaft   \n",
       "1      Gesellschaft mit beschränkter Haftung   \n",
       "2      Gesellschaft mit beschränkter Haftung   \n",
       "3                         Aktiengesellschaft   \n",
       "4                         Aktiengesellschaft   \n",
       "...                                      ...   \n",
       "18331                     Aktiengesellschaft   \n",
       "18332                     Aktiengesellschaft   \n",
       "18333                     Aktiengesellschaft   \n",
       "18334                     Aktiengesellschaft   \n",
       "18335      private company limited by shares   \n",
       "\n",
       "                        legal_form_until_now legal_form_deleted  \\\n",
       "0                                                                 \n",
       "1                                                                 \n",
       "2                                                                 \n",
       "3                                                                 \n",
       "4      Gesellschaft mit beschränkter Haftung                      \n",
       "...                                      ...                ...   \n",
       "18331                                                             \n",
       "18332                                                             \n",
       "18333                                                             \n",
       "18334                                                             \n",
       "18335                                                             \n",
       "\n",
       "                   assets         liabilities  \n",
       "0                                              \n",
       "1                                              \n",
       "2      CHF 499'207'508.00  CHF 169'631'206.00  \n",
       "3                                              \n",
       "4           CHF 3'692'400       CHF 1'569'412  \n",
       "...                   ...                 ...  \n",
       "18331                                          \n",
       "18332                                          \n",
       "18333                                          \n",
       "18334                                          \n",
       "18335                                          \n",
       "\n",
       "[18336 rows x 9 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df['assets'] = return_df['assets'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "return_df['liabilities'] = return_df['liabilities'].fillna('').str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Extract Währung of Aktiven/Passiven\n",
    "return_df['currency_assets'] = return_df['assets'].str.extract(r'^([^\\d\\s]+)')\n",
    "return_df['currency_assets'] = return_df['currency_assets'].fillna('')\n",
    "\n",
    "return_df['currency_liabilities'] = return_df['liabilities'].str.extract(r'^([^\\d\\s]+)')\n",
    "return_df['currency_liabilities'] = return_df['currency_liabilities'].fillna('')\n",
    "\n",
    "# Extract value of Aktiven/Passiven\n",
    "return_df['assets'] = return_df['assets'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "return_df['liabilities'] = return_df['liabilities'].str.extract(r'([\\d.,]+)').astype(str)\n",
    "\n",
    "# Apply correction to remove unneccessary punctuations\n",
    "return_df['assets'] = return_df['assets'].apply(correct_number)\n",
    "return_df['assets'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in return_df['assets']]\n",
    "\n",
    "return_df['liabilities'] = return_df['liabilities'].apply(correct_number)\n",
    "return_df['liabilities'] = [v.replace(',', '.') if not '.' in v else v.replace(',', '') for v in return_df['liabilities']]\n",
    "\n",
    "# Ensure correct types\n",
    "return_df['assets'] = return_df['assets'].astype(float)\n",
    "return_df['liabilities'] = return_df['liabilities'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ehraid</th>\n",
       "      <th>shab_date</th>\n",
       "      <th>shab_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>legal_form_new</th>\n",
       "      <th>legal_form_until_now</th>\n",
       "      <th>legal_form_deleted</th>\n",
       "      <th>assets</th>\n",
       "      <th>liabilities</th>\n",
       "      <th>currency_assets</th>\n",
       "      <th>currency_liabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>2706811</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>1004781481</td>\n",
       "      <td>rechtsform neu</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>308</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>1004781481</td>\n",
       "      <td>umwandlung</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>499207508.0</td>\n",
       "      <td>169631206.0</td>\n",
       "      <td>CHF</td>\n",
       "      <td>CHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7073</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>1004505486</td>\n",
       "      <td>rechtsform neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7073</td>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>1004505486</td>\n",
       "      <td>umwandlung</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td>Gesellschaft mit beschränkter Haftung</td>\n",
       "      <td></td>\n",
       "      <td>3692400.0</td>\n",
       "      <td>1569412.0</td>\n",
       "      <td>CHF</td>\n",
       "      <td>CHF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18331</th>\n",
       "      <td>1681739</td>\n",
       "      <td>2025-03-03</td>\n",
       "      <td>1006271202</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18332</th>\n",
       "      <td>1681782</td>\n",
       "      <td>2025-03-03</td>\n",
       "      <td>1006271600</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18333</th>\n",
       "      <td>1681808</td>\n",
       "      <td>2025-03-03</td>\n",
       "      <td>1006271284</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18334</th>\n",
       "      <td>1681915</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006272100</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>Aktiengesellschaft</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18335</th>\n",
       "      <td>1681972</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>1006272516</td>\n",
       "      <td>rechtsform hauptsitz neu</td>\n",
       "      <td>private company limited by shares</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18336 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ehraid   shab_date     shab_id                   keyword  \\\n",
       "0          181  2016-03-07     2706811  rechtsform hauptsitz neu   \n",
       "1          308  2019-12-12  1004781481            rechtsform neu   \n",
       "2          308  2019-12-12  1004781481                umwandlung   \n",
       "3         7073  2018-11-26  1004505486            rechtsform neu   \n",
       "4         7073  2018-11-26  1004505486                umwandlung   \n",
       "...        ...         ...         ...                       ...   \n",
       "18331  1681739  2025-03-03  1006271202  rechtsform hauptsitz neu   \n",
       "18332  1681782  2025-03-03  1006271600  rechtsform hauptsitz neu   \n",
       "18333  1681808  2025-03-03  1006271284  rechtsform hauptsitz neu   \n",
       "18334  1681915  2025-03-04  1006272100  rechtsform hauptsitz neu   \n",
       "18335  1681972  2025-03-04  1006272516  rechtsform hauptsitz neu   \n",
       "\n",
       "                              legal_form_new  \\\n",
       "0                         Aktiengesellschaft   \n",
       "1      Gesellschaft mit beschränkter Haftung   \n",
       "2      Gesellschaft mit beschränkter Haftung   \n",
       "3                         Aktiengesellschaft   \n",
       "4                         Aktiengesellschaft   \n",
       "...                                      ...   \n",
       "18331                     Aktiengesellschaft   \n",
       "18332                     Aktiengesellschaft   \n",
       "18333                     Aktiengesellschaft   \n",
       "18334                     Aktiengesellschaft   \n",
       "18335      private company limited by shares   \n",
       "\n",
       "                        legal_form_until_now legal_form_deleted       assets  \\\n",
       "0                                                                        NaN   \n",
       "1                                                                        NaN   \n",
       "2                                                                499207508.0   \n",
       "3                                                                        NaN   \n",
       "4      Gesellschaft mit beschränkter Haftung                       3692400.0   \n",
       "...                                      ...                ...          ...   \n",
       "18331                                                                    NaN   \n",
       "18332                                                                    NaN   \n",
       "18333                                                                    NaN   \n",
       "18334                                                                    NaN   \n",
       "18335                                                                    NaN   \n",
       "\n",
       "       liabilities currency_assets currency_liabilities  \n",
       "0              NaN                                       \n",
       "1              NaN                                       \n",
       "2      169631206.0             CHF                  CHF  \n",
       "3              NaN                                       \n",
       "4        1569412.0             CHF                  CHF  \n",
       "...            ...             ...                  ...  \n",
       "18331          NaN                                       \n",
       "18332          NaN                                       \n",
       "18333          NaN                                       \n",
       "18334          NaN                                       \n",
       "18335          NaN                                       \n",
       "\n",
       "[18336 rows x 11 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_HISTORY_LEGAL_FORMS= f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zefix.history_legal_forms (\n",
    "    ehraid INT,\n",
    "    shab_date DATE,\n",
    "    shab_id INT,\n",
    "    keyword TEXT,\n",
    "    legal_form_new TEXT,\n",
    "    legal_form_until_now TEXT,\n",
    "    legal_form_deleted TEXT,\n",
    "    assets NUMERIC,\n",
    "    liabilities NUMERIC,\n",
    "    currency_assets TEXT,\n",
    "    currency_liabilities TEXT\n",
    ");\"\"\"\n",
    "\n",
    "name2table = {\n",
    "    'history_legal_forms': (INIT_HISTORY_LEGAL_FORMS, return_df),\n",
    "}\n",
    "for table_name, (query, df) in name2table.items():\n",
    "    with connect_database() as con:\n",
    "        con.execute(query)\n",
    "        save_to_database(con, df, table_name, 'zefix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
