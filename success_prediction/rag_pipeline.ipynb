{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import argparse\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from ftlangdetect import detect\n",
    "from pymilvus import MilvusClient, CollectionSchema, FieldSchema, DataType\n",
    "from pymilvus.client.types import ExtraList\n",
    "from success_prediction.rag_components.embeddings import EmbeddingHandler\n",
    "from success_prediction.rag_components.cleanup import MarkdownCleaner\n",
    "\n",
    "from success_prediction.config import DATA_DIR, RAW_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Clients:\n",
    "    md_cleaner: MarkdownCleaner\n",
    "    embedding_creator: EmbeddingHandler\n",
    "    db_client: MilvusClient\n",
    "\n",
    "\n",
    "def load_raw_file(file_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Loads a gzipped JSON file and returns its content as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (Path): Path to the gzipped JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed JSON data.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def store_links(file_path: Path, data: dict) -> None:\n",
    "    \"\"\"\n",
    "    Stores a dictionary as a formatted JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (Path): Destination file path.\n",
    "        data (dict): Dictionary to save.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        return json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def structure_links(\n",
    "    ehraid: int,\n",
    "    links: list[dict],\n",
    "    email_addresses: set,\n",
    "    social_media: dict\n",
    ") -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Organizes links by identifying emails and social media handles and storing them per company ID.\n",
    "\n",
    "    Args:\n",
    "        ehraid (int): Unique company identifier.\n",
    "        links (List[dict]): List of extracted link dictionaries.\n",
    "        email_addresses (Dict[int, Dict[str, Set[str]]]): Storage for emails.\n",
    "        social_media (Dict[int, Dict[str, Set[str]]]): Storage for social links.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing updated email_addresses and social_media.\n",
    "    \"\"\"\n",
    "    for link in links:\n",
    "        base_domain = link.get('base_domain')\n",
    "        if '@' in link.get('text'):\n",
    "            email_addresses[ehraid]['emails'].add(link['text'])\n",
    "        elif base_domain == \"linkedin.com\":\n",
    "            social_media[ehraid]['linkedin'].add(link['href'])\n",
    "        elif base_domain == \"instagram.com\":\n",
    "            social_media[ehraid]['instagram'].add(link['href'])\n",
    "        elif base_domain == \"facebook.com\":\n",
    "            social_media[ehraid]['facebook'].add(link['href'])\n",
    "        elif base_domain == \"tiktok.com\":\n",
    "            social_media[ehraid]['tiktok'].add(link['href'])\n",
    "        elif base_domain == \"youtube.com\":\n",
    "            social_media[ehraid]['youtube'].add(link['href'])\n",
    "        elif base_domain == \"x.com\" or base_domain == \"twitter.com\":\n",
    "            social_media[ehraid]['x'].add(link['href'])\n",
    "    return email_addresses, social_media\n",
    "\n",
    "\n",
    "def run_pipeline(clients: Clients, idx: int, file_path: Path, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Processes raw company website data:\n",
    "    - Cleans and chunks content\n",
    "    - Embeds it\n",
    "    - Extracts contact and social media links\n",
    "    - Stores results in a Milvus database and contact info files\n",
    "\n",
    "    Args:\n",
    "        clients (Clients): Wrapper containing the database, embedding, and cleaning tools.\n",
    "        idx (int): Index of the file being processed.\n",
    "        file_path (Path): Path to the raw JSON file.\n",
    "        **kwargs: Additional options, expects 'collection_name'.\n",
    "    \"\"\"\n",
    "    raw_json = load_raw_file(file_path)\n",
    "    processed_files = []\n",
    "    email_addresses, social_media = {}, {}\n",
    "\n",
    "    for ehraid, urls2attributes in tqdm(raw_json.items()):\n",
    "        email_addresses[ehraid] = {'emails': set()}\n",
    "        social_media[ehraid] = {k: set() for k in ['linkedin', 'instagram', 'facebook', 'tiktok', 'youtube', 'x']}\n",
    "\n",
    "        for url, attributes in urls2attributes.items():\n",
    "            markdown = attributes.get('markdown')\n",
    "            if not markdown:\n",
    "                continue\n",
    "\n",
    "            date = attributes['date']\n",
    "            internal_links = [link['href'] for link in attributes['links']['internal']]\n",
    "            external_links = [link['href'] for link in attributes['links']['external']]\n",
    "\n",
    "            email_addresses, social_media = structure_links(\n",
    "                ehraid, attributes['links']['external'], email_addresses, social_media)\n",
    "\n",
    "            markdown_clean = clients.md_cleaner.clean(markdown, internal_links, external_links)\n",
    "            markdown_no_links = clients.md_cleaner.remove_nested_brackets(markdown_clean).replace('\\n', ' ')\n",
    "            if len(markdown_no_links) <= 300:\n",
    "                continue\n",
    "\n",
    "            # Detect language using the text without bracket content, since it includes\n",
    "            # English tokens such as INTERNAL_LINKS that might confuse the model\n",
    "            language = detect(text=markdown_no_links)\n",
    "\n",
    "            # Split the text into smaller chunks to fit into the model context + normalize whitespace per chunk\n",
    "            markdown_chunks = clients.embedding_creator.chunk(markdown_no_links)\n",
    "            markdown_chunks_clean = [\n",
    "                clients.md_cleaner.normalize_whitespace(doc.page_content)\n",
    "                for doc in markdown_chunks\n",
    "            ]\n",
    "\n",
    "            passage_embeddings = clients.embedding_creator.embed(\n",
    "                markdown_chunks_clean, prefix='passage:')\n",
    "\n",
    "            query_embeddings = clients.embedding_creator.embed(\n",
    "                markdown_chunks_clean, prefix='query:')\n",
    "\n",
    "            processed_files.extend([\n",
    "                {\n",
    "                    'ehraid': int(ehraid),\n",
    "                    'url': str(url),\n",
    "                    'date': date,\n",
    "                    'language': language.get('lang'),\n",
    "                    'text': md,\n",
    "                    'embedding_passage': p_emb,\n",
    "                    'embedding_query': q_emb\n",
    "                }\n",
    "                for md, p_emb, q_emb in zip(markdown_chunks_clean, passage_embeddings, query_embeddings)\n",
    "            ])\n",
    "\n",
    "        email_addresses[ehraid] = {k: list(v) for k, v in email_addresses[ehraid].items()}\n",
    "        social_media[ehraid] = {k: list(v) for k, v in social_media[ehraid].items()}\n",
    "\n",
    "    clients.db_client.insert(collection_name=kwargs.get('collection_name'), data=processed_files)\n",
    "\n",
    "    store_links(RAW_DATA_DIR / 'company_websites' / 'current' / 'contact_info' / f'emails_{idx}.json', email_addresses)\n",
    "    store_links(RAW_DATA_DIR / 'company_websites' / 'current' / 'contact_info' /  f'social_media_{idx}.json', social_media)\n",
    "\n",
    "\n",
    "def setup_database(client: MilvusClient, collection_name: str, schema: CollectionSchema, replace: bool) -> None:\n",
    "    \"\"\"\n",
    "    Sets up a Milvus collection for storing embedded documents.\n",
    "\n",
    "    Args:\n",
    "        client (MilvusClient): Initialized Milvus client.\n",
    "        collection_name (str): Name of the collection to use/create.\n",
    "        schema (CollectionSchema): Schema of the collection.\n",
    "        replace (bool): Whether to drop and recreate the collection if it exists.\n",
    "    \"\"\"\n",
    "    if replace and client.has_collection(collection_name):\n",
    "        client.drop_collection(collection_name)\n",
    "\n",
    "    if not client.has_collection(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            schema=schema)\n",
    "    else:\n",
    "        print(f\"{collection_name} already exists!\")\n",
    "\n",
    "\n",
    "def main(args: argparse.Namespace):\n",
    "\n",
    "    clients = Clients(\n",
    "        md_cleaner=MarkdownCleaner(),\n",
    "        embedding_creator=EmbeddingHandler(model_name='intfloat/multilingual-e5-base'),\n",
    "        db_client=MilvusClient(uri=DATA_DIR / 'database' / 'websites.db')\n",
    "    )\n",
    "\n",
    "    website_schema = CollectionSchema(fields=[\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"ehraid\", dtype=DataType.INT64),\n",
    "        FieldSchema(name=\"url\", dtype=DataType.VARCHAR, max_length=512),\n",
    "        FieldSchema(name=\"date\", dtype=DataType.VARCHAR, max_length=10),\n",
    "        FieldSchema(name=\"language\", dtype=DataType.VARCHAR, max_length=5),\n",
    "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=64_000),\n",
    "        FieldSchema(name=\"embedding_passage\", dtype=DataType.FLOAT_VECTOR, dim=768),\n",
    "        FieldSchema(name=\"embedding_query\", dtype=DataType.FLOAT_VECTOR, dim=768),\n",
    "    ])\n",
    "    setup_database(clients.db_client, collection_name=args.collection_name, schema=website_schema, replace=args.replace or False)\n",
    "\n",
    "    raw_files = [file for file in Path(RAW_DATA_DIR / 'company_websites' / 'current').iterdir() if str(file).endswith('.json.gz')]\n",
    "    # raw_files = [RAW_DATA_DIR / 'company_websites' / 'current' / '0_websites.json.gz']\n",
    "\n",
    "    for i, file in enumerate(raw_files):\n",
    "        run_pipeline(clients, idx=i, file_path=file, collection_name=args.collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EmbeddingHandler] Using model on `mps`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [05:06<00:00,  1.63it/s]\n",
      "100%|██████████| 496/496 [04:57<00:00,  1.67it/s]\n",
      "100%|██████████| 500/500 [04:22<00:00,  1.90it/s]\n",
      "100%|██████████| 499/499 [03:18<00:00,  2.52it/s]\n",
      "100%|██████████| 499/499 [03:38<00:00,  2.28it/s]\n",
      "100%|██████████| 498/498 [04:12<00:00,  1.98it/s]\n",
      "100%|██████████| 449/449 [03:28<00:00,  2.16it/s]\n",
      "100%|██████████| 500/500 [02:57<00:00,  2.82it/s]\n",
      "100%|██████████| 448/448 [03:12<00:00,  2.32it/s]\n",
      "100%|██████████| 450/450 [03:07<00:00,  2.41it/s]\n",
      "100%|██████████| 499/499 [03:38<00:00,  2.28it/s]\n",
      "100%|██████████| 499/499 [04:02<00:00,  2.06it/s]\n",
      "100%|██████████| 500/500 [03:44<00:00,  2.23it/s]\n",
      "100%|██████████| 499/499 [03:15<00:00,  2.55it/s]\n",
      "100%|██████████| 499/499 [03:19<00:00,  2.50it/s]\n",
      "100%|██████████| 499/499 [03:52<00:00,  2.15it/s]\n",
      "100%|██████████| 498/498 [03:52<00:00,  2.14it/s]\n",
      "100%|██████████| 500/500 [03:55<00:00,  2.12it/s]\n",
      "100%|██████████| 499/499 [02:57<00:00,  2.81it/s]\n",
      "100%|██████████| 450/450 [02:46<00:00,  2.70it/s]\n",
      "100%|██████████| 499/499 [04:02<00:00,  2.05it/s]\n",
      "100%|██████████| 448/448 [03:05<00:00,  2.41it/s]\n",
      "100%|██████████| 497/497 [02:46<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [03:45<00:00,  2.22it/s]\n",
      "100%|██████████| 499/499 [03:59<00:00,  2.08it/s]\n",
      "100%|██████████| 498/498 [04:04<00:00,  2.04it/s]\n",
      "100%|██████████| 500/500 [03:33<00:00,  2.34it/s]\n",
      "100%|██████████| 180/180 [01:09<00:00,  2.61it/s]\n",
      "100%|██████████| 499/499 [04:55<00:00,  1.69it/s]\n",
      "100%|██████████| 499/499 [04:14<00:00,  1.96it/s]\n",
      "100%|██████████| 449/449 [03:57<00:00,  1.89it/s]\n",
      "100%|██████████| 500/500 [03:27<00:00,  2.41it/s]\n",
      "100%|██████████| 499/499 [03:06<00:00,  2.68it/s]\n",
      "100%|██████████| 500/500 [03:40<00:00,  2.27it/s]\n",
      "100%|██████████| 499/499 [04:03<00:00,  2.05it/s]\n",
      "100%|██████████| 498/498 [03:35<00:00,  2.31it/s]\n",
      "100%|██████████| 499/499 [05:01<00:00,  1.65it/s]\n",
      "100%|██████████| 500/500 [04:20<00:00,  1.92it/s]\n",
      "100%|██████████| 500/500 [03:00<00:00,  2.77it/s]\n",
      "100%|██████████| 448/448 [03:26<00:00,  2.17it/s]\n",
      "100%|██████████| 500/500 [03:24<00:00,  2.44it/s]\n",
      "100%|██████████| 498/498 [06:12<00:00,  1.34it/s]  \n",
      "100%|██████████| 499/499 [03:53<00:00,  2.14it/s]\n",
      "100%|██████████| 499/499 [03:53<00:00,  2.14it/s]\n",
      "100%|██████████| 498/498 [04:11<00:00,  1.98it/s]\n",
      "100%|██████████| 500/500 [03:43<00:00,  2.24it/s]\n",
      "100%|██████████| 499/499 [03:38<00:00,  2.28it/s]\n",
      "100%|██████████| 499/499 [03:47<00:00,  2.19it/s]\n",
      "100%|██████████| 450/450 [03:37<00:00,  2.07it/s]\n",
      "100%|██████████| 500/500 [05:23<00:00,  1.54it/s]\n",
      "100%|██████████| 498/498 [03:46<00:00,  2.20it/s]\n",
      "100%|██████████| 497/497 [04:01<00:00,  2.06it/s]\n",
      "100%|██████████| 500/500 [03:29<00:00,  2.39it/s]\n",
      "100%|██████████| 450/450 [03:40<00:00,  2.04it/s]\n",
      "100%|██████████| 499/499 [03:25<00:00,  2.43it/s]\n",
      "100%|██████████| 498/498 [03:58<00:00,  2.09it/s]\n",
      "100%|██████████| 499/499 [03:22<00:00,  2.46it/s]\n",
      "100%|██████████| 449/449 [03:49<00:00,  1.96it/s]\n",
      "100%|██████████| 500/500 [03:37<00:00,  2.30it/s]\n",
      "100%|██████████| 499/499 [03:33<00:00,  2.33it/s]\n",
      "100%|██████████| 500/500 [03:48<00:00,  2.19it/s]\n",
      "100%|██████████| 499/499 [03:55<00:00,  2.12it/s]\n",
      "100%|██████████| 500/500 [04:44<00:00,  1.76it/s]\n",
      "100%|██████████| 450/450 [03:29<00:00,  2.15it/s]\n",
      "100%|██████████| 498/498 [04:20<00:00,  1.91it/s]\n",
      "100%|██████████| 499/499 [03:53<00:00,  2.14it/s]\n",
      "100%|██████████| 499/499 [03:20<00:00,  2.49it/s]\n",
      "100%|██████████| 499/499 [03:34<00:00,  2.32it/s]\n",
      "100%|██████████| 500/500 [03:29<00:00,  2.39it/s]\n",
      "100%|██████████| 498/498 [03:11<00:00,  2.61it/s]\n",
      "100%|██████████| 499/499 [04:13<00:00,  1.97it/s]\n",
      "100%|██████████| 500/500 [04:06<00:00,  2.03it/s]\n",
      "100%|██████████| 500/500 [04:03<00:00,  2.06it/s]\n",
      "100%|██████████| 499/499 [03:26<00:00,  2.42it/s]\n",
      "100%|██████████| 500/500 [04:18<00:00,  1.93it/s]\n",
      "100%|██████████| 499/499 [03:35<00:00,  2.31it/s]\n",
      "100%|██████████| 449/449 [03:45<00:00,  1.99it/s]\n",
      "100%|██████████| 500/500 [03:54<00:00,  2.13it/s]\n",
      "100%|██████████| 499/499 [03:14<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog='RAGPipeline',\n",
    "        description='Processes the markdown and handles retrieval from the Milvus DB',\n",
    "    )\n",
    "    parser.add_argument('--collection_name', default='current_websites')\n",
    "    parser.add_argument('--replace', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "    \"\"\"\n",
    "    main('current_websites', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper.crawler_config import value_keywords, esg_keywords, team_keywords, product_keywords\n",
    "\n",
    "VALUE_PROPOSITION_KEYWORDS = list({k for k in value_keywords + esg_keywords})\n",
    "LEADERSHIP_KEYWORDS = list({k for k in value_keywords + esg_keywords})\n",
    "RESPONSIBILITY_KEYWORDS = list({k for k in value_keywords + esg_keywords})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EmbeddingHandler] Using model on `mps`.\n"
     ]
    }
   ],
   "source": [
    "db_client = MilvusClient(uri=str(DATA_DIR / 'database' / 'websites.db'))\n",
    "embedding_creator = EmbeddingHandler()\n",
    "\n",
    "dim2query = {\n",
    "    \"Value Proposition & Innovation\": [\n",
    "        \"What solutions, services, or products does the company provide to customers?\",\n",
    "        \"What products and services does the company advertise on its website?\",\n",
    "        \"Which innovative features or technologies are highlighted in the company's offerings?\",\n",
    "        \"What benefits or outcomes does the company promise or deliver through its solutions, services, products, or platforms?\",\n",
    "        \"How does the company differentiate its products or services from competitors, and what specific customer needs are addressed?\"\n",
    "    ],\n",
    "    \"Purpose & Responsibility\": [\n",
    "        \"What is the stated mission, purpose, or long-term vision of the company?\",\n",
    "        \"Which ethical, social, or environmental commitments does the company emphasize?\",\n",
    "        \"Does the company value sustainability, diversity, inclusion, or in general ESG-related goals?\",\n",
    "        \"What values or principles guide the company's operations and decisions?\",\n",
    "        \"Does the company participate in any charitable initiatives, community outreach, or global impact programs?\"\n",
    "    ],\n",
    "    \"Leadership & People\": [\n",
    "        \"Who are the founders or key leaders of the company, and what roles do they hold?\",\n",
    "        \"What are the professional backgrounds or credentials of the company's executive team?\",\n",
    "        \"Who makes up the leadership team, and how is the company structured in terms of people and roles?\",\n",
    "        \"What experience or expertise does the management bring to the company?\",\n",
    "        \"Are there biographies or personal stories of team members or executives available on the website?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to list\n",
    "dim2embedding = {dimension: [embedding_creator.embed([q], prefix='query:') for q in queries] for dimension, queries in dim2query.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = [{\n",
    "    \"field_name\": \"embedding\",\n",
    "    \"metric_type\": \"IP\",  # Use inner product because E5 embeddings are normalized (||v|| = 1)\n",
    "    \"index_type\": \"FLAT\",\n",
    "}]\n",
    "\n",
    "db_client.create_index(\n",
    "    collection_name=\"current_websites\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "db_client.load_collection(collection_name=\"current_websites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_top_passages(company_data, query_embeddings, top_k_per_query=15, final_top_k=15):\n",
    "    \"\"\"\n",
    "    Returns the passages that appear most frequently in the top-k across query ensemble.\n",
    "\n",
    "    Parameters:\n",
    "        company_data: List of dicts with fields 'id', 'embedding', 'text', 'url'\n",
    "        query_embeddings: List of embedded queries for the dimension\n",
    "        top_k_per_query: Number of top results to keep for each query\n",
    "        final_top_k: Final number of consensus passages to return\n",
    "\n",
    "    Returns:\n",
    "        List of dicts: top passages by consensus frequency\n",
    "    \"\"\"\n",
    "    passage_scores = defaultdict(list)\n",
    "\n",
    "    for query_vec in query_embeddings:\n",
    "        query_vec = np.array(query_vec[0])\n",
    "        scored_entries = []\n",
    "\n",
    "        for entry in company_data:\n",
    "            score = np.dot(query_vec, entry['embedding'])\n",
    "            scored_entries.append({**entry, 'score': score})\n",
    "\n",
    "        top_k = sorted(scored_entries, key=lambda x: x['score'], reverse=True)[:top_k_per_query]\n",
    "\n",
    "        for passage in top_k:\n",
    "            passage_scores[passage['id']].append((passage['score'], passage))\n",
    "\n",
    "    frequency_counter = Counter({pid: len(scores) for pid, scores in passage_scores.items()})\n",
    "\n",
    "    # Sort by frequency (how often in top 15), then by best score (descending)\n",
    "    sorted_passages = sorted(\n",
    "        passage_scores.items(),\n",
    "        key=lambda x: (frequency_counter[x[0]], max(s[0] for s in x[1])),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    final_passages = [x[1][0][1] for x in sorted_passages[:final_top_k]]\n",
    "    return final_passages\n",
    "\n",
    "\n",
    "def ensemble_rerank(top_n_entries, query_texts):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for entry in top_n_entries:\n",
    "        pairs = [(query, entry['text']) for query in query_texts]\n",
    "        relevancy_score = embedding_creator.calculate_relevancy_scores(sentence_pairs=pairs).median()\n",
    "        entry.update({'attention_score': relevancy_score})\n",
    "\n",
    "    sorted_entries = sorted(\n",
    "        top_n_entries,\n",
    "        key=lambda entry: (float(entry['attention_score'])),\n",
    "        reverse=True\n",
    "    )\n",
    "    scores = np.array([entry['attention_score'] for entry in sorted_entries])\n",
    "    z_scores = (scores - np.mean(scores)) / np.std(scores)\n",
    "    return [entry for z_score, entry in zip(z_scores, sorted_entries) if z_score >= 0]\n",
    "\n",
    "def get_dimension_vec(dimension: str, company_data: ExtraList, dim2embedding: dict, dim2query: dict):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get top 15 based on cosine / IP similarity\n",
    "    top_15 = ensemble_top_passages(\n",
    "        company_data=company_data,\n",
    "        query_embeddings=dim2embedding[dimension]\n",
    "    )\n",
    "\n",
    "    # Get the most relevant by reranking them via cross encoder\n",
    "    most_relevant = ensemble_rerank(\n",
    "        top_n_entries=top_15,\n",
    "        query_texts=dim2query[dimension]\n",
    "    )\n",
    "    \n",
    "    # combine the remaining into one vector by using the quasi attention score from the ensemble rerank\n",
    "    dim_vec = embedding_creator.waggregate_embeddings([torch.tensor(entry['embedding']) for entry in most_relevant], [entry['attention_score'] for entry in most_relevant])\n",
    "    return most_relevant, dim_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_results = []\n",
    "for ehraid in [1251382, 1433629]:\n",
    "    company_data = db_client.query(collection_name='current_websites', filter=f\"ehraid == {ehraid}\")\n",
    "    dim_vectors = {}, {}\n",
    "    for dim in dim2query.keys():\n",
    "        most_relevant, dim_vec = get_dimension_vec(dim, company_data, dim2embedding, dim2query)\n",
    "        dim_vectors[dim]['entries'] = most_relevant\n",
    "        dim_vectors[dim]['vectors'] = dim_vec\n",
    "    vec_results.append({ehraid: dim_vectors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
